{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1ZQPHuDnf-Io"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Natural Language Processing (PDS3102)\n",
        "\n",
        "Submitted By : Subrat Ku Nanda"
      ],
      "metadata": {
        "id": "_cysfKLjf_Uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1.1\n"
      ],
      "metadata": {
        "id": "2QzEX2Cdgwo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP Practical Assignment: Text Preprocessing\n",
        "\n",
        "\n",
        "In this assignment, you will practice various text preprocessing techniques using Python and the NLTK library. You'll work on tokenization, chunking, stemming, lemmatization, and stop word removal.\n"
      ],
      "metadata": {
        "id": "cOKYer80g5pL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1: Tokenization and Sentence Splitting\n",
        "1. Read the text from the provided \"nlp_text.txt\" file.\n",
        "2. Tokenize the text into words using the word_tokenize function from NLTK.\n",
        "3. Split the text into sentences using the sent_tokenize function from NLTK.\n",
        "4. Print out the list of tokens and sentences.\n",
        "5. Try the same task using re (regular expression)"
      ],
      "metadata": {
        "id": "EepeZfyGg_C3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries & download all nltk packages\n",
        "import nltk\n",
        "\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QZtILHVjhQO",
        "outputId": "46893211-e396-4687-f906-5466429b4d19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dctG8kAVcdjP"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the stopwords from nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YuRra8LFiYJV",
        "outputId": "67a7c6ff-fdac-4698-fb19-bc97958a3c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1- Read the text from the provided \"nlp_text.txt\" file.\n"
      ],
      "metadata": {
        "id": "EpDFHXqllANN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "data = pd.read_csv('/content/reviews.csv')\n",
        "#print(data.head())"
      ],
      "metadata": {
        "id": "etAFYxsEkDks",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "P5UCTgfgkreM",
        "outputId": "27308567-3e57-460d-b01d-fbd08a086578",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Index          Name Overall_Rating  \\\n",
              "0      0  Oliver Brown            3.9   \n",
              "1      1  Oliver Brown            3.9   \n",
              "2      2  Crush Coffee              3   \n",
              "3      3   The Mohalla            3.8   \n",
              "4      4   The Mohalla            3.8   \n",
              "\n",
              "                                             Cuisine Rate for two       City  \\\n",
              "0  Cafe, Coffee, Shake, Juices, Beverages, Waffle...          500  ahmedabad   \n",
              "1  Cafe, Coffee, Shake, Juices, Beverages, Waffle...          500  ahmedabad   \n",
              "2                   Cafe, Shake, Beverages, Desserts          600  ahmedabad   \n",
              "3                                               Cafe          550  ahmedabad   \n",
              "4                                               Cafe          550  ahmedabad   \n",
              "\n",
              "                                              Review  \n",
              "0  Been to this place 3-4 times. Prakash is alway...  \n",
              "1  I recently visited Oliver Brown on a weekend f...  \n",
              "2                         Very watery ans thin shake  \n",
              "3  it was not cheese burst pizza.. only cheeze wa...  \n",
              "4  Yammi.,....test burger is best I love ðŸ’— this B...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f9dcc848-7e58-41bd-95d7-7bc2da598816\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Index</th>\n",
              "      <th>Name</th>\n",
              "      <th>Overall_Rating</th>\n",
              "      <th>Cuisine</th>\n",
              "      <th>Rate for two</th>\n",
              "      <th>City</th>\n",
              "      <th>Review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Oliver Brown</td>\n",
              "      <td>3.9</td>\n",
              "      <td>Cafe, Coffee, Shake, Juices, Beverages, Waffle...</td>\n",
              "      <td>500</td>\n",
              "      <td>ahmedabad</td>\n",
              "      <td>Been to this place 3-4 times. Prakash is alway...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Oliver Brown</td>\n",
              "      <td>3.9</td>\n",
              "      <td>Cafe, Coffee, Shake, Juices, Beverages, Waffle...</td>\n",
              "      <td>500</td>\n",
              "      <td>ahmedabad</td>\n",
              "      <td>I recently visited Oliver Brown on a weekend f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Crush Coffee</td>\n",
              "      <td>3</td>\n",
              "      <td>Cafe, Shake, Beverages, Desserts</td>\n",
              "      <td>600</td>\n",
              "      <td>ahmedabad</td>\n",
              "      <td>Very watery ans thin shake</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>The Mohalla</td>\n",
              "      <td>3.8</td>\n",
              "      <td>Cafe</td>\n",
              "      <td>550</td>\n",
              "      <td>ahmedabad</td>\n",
              "      <td>it was not cheese burst pizza.. only cheeze wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>The Mohalla</td>\n",
              "      <td>3.8</td>\n",
              "      <td>Cafe</td>\n",
              "      <td>550</td>\n",
              "      <td>ahmedabad</td>\n",
              "      <td>Yammi.,....test burger is best I love ðŸ’— this B...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9dcc848-7e58-41bd-95d7-7bc2da598816')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f9dcc848-7e58-41bd-95d7-7bc2da598816 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f9dcc848-7e58-41bd-95d7-7bc2da598816');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-797e9fbe-5461-468e-88f0-b3ae4b5aab4b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-797e9fbe-5461-468e-88f0-b3ae4b5aab4b')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-797e9fbe-5461-468e-88f0-b3ae4b5aab4b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#open text file in read mode\n",
        "text_file = open(\"/content/nlp_text.txt\", \"r\")\n",
        "\n",
        "#read whole file to a string\n",
        "data = text_file.read()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "gXvLxOXTlHXg",
        "outputId": "239b8f3d-9bd1-4c2e-e244-e7fc342cd5c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# An Introduction to Natural Language Processing (NLP)\\n\\nNatural Language Processing (NLP) is a field of study that focuses on the interaction between computers and humans using natural language. It involves developing algorithms and computational models that can analyze, understand, and generate human language.\\n\\nNLP has become increasingly important in recent years due to the vast amounts of unstructured data that are generated every day through social media, emails, and other forms of communication. By using NLP techniques, it is possible to extract valuable insights from this data, which can be used for a wide range of applications including sentiment analysis, chatbots, and machine translation.\\n\\nOne of the key challenges in NLP is the ambiguity of human language. Words can have multiple meanings depending on the context in which they are used, and grammar rules can be broken without affecting the meaning of a sentence. To address these challenges, NLP researchers have developed a range of techniques including statistical models, rule-based systems, and deep learning algorithms.\\n\\nOne popular application of NLP is sentiment analysis, which involves analyzing text to determine the emotional tone of the writer. This can be useful for businesses who want to understand how their customers feel about their products or services. Another application is chatbots, which use NLP to understand and respond to user queries in a natural way.\\n\\nDespite its many benefits, NLP also raises ethical concerns around issues such as privacy and bias. For example, if NLP is used to analyze social media data, there is a risk that personal information could be exposed. Additionally, if the algorithms used in NLP are biased, they could perpetuate discrimination against certain groups of people.\\n\\nIn conclusion, NLP is a rapidly growing field with many exciting applications. As we continue to generate vast amounts of unstructured data, NLP techniques will become increasingly important for extracting valuable insights from this data. However, it is important to be aware of the ethical implications of using NLP and to ensure that these technologies are developed in a responsible and equitable way.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2- Tokenize the text into words using the word_tokenize function from NLTK.\n"
      ],
      "metadata": {
        "id": "xdGyvn_vlEbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenization\n",
        "tokens = word_tokenize(data)"
      ],
      "metadata": {
        "id": "FRN9MFcwmhI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3- Split the text into sentences using the sent_tokenize function from NLTK."
      ],
      "metadata": {
        "id": "-sPXzYXMnZS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "sentences = sent_tokenize(data)"
      ],
      "metadata": {
        "id": "NSyko-ZRnQsU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4- Print out the list of tokens and sentences.\n"
      ],
      "metadata": {
        "id": "VC9CynR8oIn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying Tokens\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Vl-Xy94nKkp",
        "outputId": "7780dabc-2d0f-4d42-ce4b-8c606c41d8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#', 'An', 'Introduction', 'to', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'study', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language', '.', 'It', 'involves', 'developing', 'algorithms', 'and', 'computational', 'models', 'that', 'can', 'analyze', ',', 'understand', ',', 'and', 'generate', 'human', 'language', '.', 'NLP', 'has', 'become', 'increasingly', 'important', 'in', 'recent', 'years', 'due', 'to', 'the', 'vast', 'amounts', 'of', 'unstructured', 'data', 'that', 'are', 'generated', 'every', 'day', 'through', 'social', 'media', ',', 'emails', ',', 'and', 'other', 'forms', 'of', 'communication', '.', 'By', 'using', 'NLP', 'techniques', ',', 'it', 'is', 'possible', 'to', 'extract', 'valuable', 'insights', 'from', 'this', 'data', ',', 'which', 'can', 'be', 'used', 'for', 'a', 'wide', 'range', 'of', 'applications', 'including', 'sentiment', 'analysis', ',', 'chatbots', ',', 'and', 'machine', 'translation', '.', 'One', 'of', 'the', 'key', 'challenges', 'in', 'NLP', 'is', 'the', 'ambiguity', 'of', 'human', 'language', '.', 'Words', 'can', 'have', 'multiple', 'meanings', 'depending', 'on', 'the', 'context', 'in', 'which', 'they', 'are', 'used', ',', 'and', 'grammar', 'rules', 'can', 'be', 'broken', 'without', 'affecting', 'the', 'meaning', 'of', 'a', 'sentence', '.', 'To', 'address', 'these', 'challenges', ',', 'NLP', 'researchers', 'have', 'developed', 'a', 'range', 'of', 'techniques', 'including', 'statistical', 'models', ',', 'rule-based', 'systems', ',', 'and', 'deep', 'learning', 'algorithms', '.', 'One', 'popular', 'application', 'of', 'NLP', 'is', 'sentiment', 'analysis', ',', 'which', 'involves', 'analyzing', 'text', 'to', 'determine', 'the', 'emotional', 'tone', 'of', 'the', 'writer', '.', 'This', 'can', 'be', 'useful', 'for', 'businesses', 'who', 'want', 'to', 'understand', 'how', 'their', 'customers', 'feel', 'about', 'their', 'products', 'or', 'services', '.', 'Another', 'application', 'is', 'chatbots', ',', 'which', 'use', 'NLP', 'to', 'understand', 'and', 'respond', 'to', 'user', 'queries', 'in', 'a', 'natural', 'way', '.', 'Despite', 'its', 'many', 'benefits', ',', 'NLP', 'also', 'raises', 'ethical', 'concerns', 'around', 'issues', 'such', 'as', 'privacy', 'and', 'bias', '.', 'For', 'example', ',', 'if', 'NLP', 'is', 'used', 'to', 'analyze', 'social', 'media', 'data', ',', 'there', 'is', 'a', 'risk', 'that', 'personal', 'information', 'could', 'be', 'exposed', '.', 'Additionally', ',', 'if', 'the', 'algorithms', 'used', 'in', 'NLP', 'are', 'biased', ',', 'they', 'could', 'perpetuate', 'discrimination', 'against', 'certain', 'groups', 'of', 'people', '.', 'In', 'conclusion', ',', 'NLP', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'many', 'exciting', 'applications', '.', 'As', 'we', 'continue', 'to', 'generate', 'vast', 'amounts', 'of', 'unstructured', 'data', ',', 'NLP', 'techniques', 'will', 'become', 'increasingly', 'important', 'for', 'extracting', 'valuable', 'insights', 'from', 'this', 'data', '.', 'However', ',', 'it', 'is', 'important', 'to', 'be', 'aware', 'of', 'the', 'ethical', 'implications', 'of', 'using', 'NLP', 'and', 'to', 'ensure', 'that', 'these', 'technologies', 'are', 'developed', 'in', 'a', 'responsible', 'and', 'equitable', 'way', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying Sentences\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YM7c1MrRn5Cs",
        "outputId": "baf92902-e576-4415-ae25-a88bf3ffa7a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['# An Introduction to Natural Language Processing (NLP)\\n\\nNatural Language Processing (NLP) is a field of study that focuses on the interaction between computers and humans using natural language.', 'It involves developing algorithms and computational models that can analyze, understand, and generate human language.', 'NLP has become increasingly important in recent years due to the vast amounts of unstructured data that are generated every day through social media, emails, and other forms of communication.', 'By using NLP techniques, it is possible to extract valuable insights from this data, which can be used for a wide range of applications including sentiment analysis, chatbots, and machine translation.', 'One of the key challenges in NLP is the ambiguity of human language.', 'Words can have multiple meanings depending on the context in which they are used, and grammar rules can be broken without affecting the meaning of a sentence.', 'To address these challenges, NLP researchers have developed a range of techniques including statistical models, rule-based systems, and deep learning algorithms.', 'One popular application of NLP is sentiment analysis, which involves analyzing text to determine the emotional tone of the writer.', 'This can be useful for businesses who want to understand how their customers feel about their products or services.', 'Another application is chatbots, which use NLP to understand and respond to user queries in a natural way.', 'Despite its many benefits, NLP also raises ethical concerns around issues such as privacy and bias.', 'For example, if NLP is used to analyze social media data, there is a risk that personal information could be exposed.', 'Additionally, if the algorithms used in NLP are biased, they could perpetuate discrimination against certain groups of people.', 'In conclusion, NLP is a rapidly growing field with many exciting applications.', 'As we continue to generate vast amounts of unstructured data, NLP techniques will become increasingly important for extracting valuable insights from this data.', 'However, it is important to be aware of the ethical implications of using NLP and to ensure that these technologies are developed in a responsible and equitable way.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Displaying Sentences line by line\n",
        "for item in sentences :\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsuTKZgCnSr8",
        "outputId": "0ceec339-0b9a-4336-ea55-f42beb62fe6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# An Introduction to Natural Language Processing (NLP)\n",
            "\n",
            "Natural Language Processing (NLP) is a field of study that focuses on the interaction between computers and humans using natural language.\n",
            "It involves developing algorithms and computational models that can analyze, understand, and generate human language.\n",
            "NLP has become increasingly important in recent years due to the vast amounts of unstructured data that are generated every day through social media, emails, and other forms of communication.\n",
            "By using NLP techniques, it is possible to extract valuable insights from this data, which can be used for a wide range of applications including sentiment analysis, chatbots, and machine translation.\n",
            "One of the key challenges in NLP is the ambiguity of human language.\n",
            "Words can have multiple meanings depending on the context in which they are used, and grammar rules can be broken without affecting the meaning of a sentence.\n",
            "To address these challenges, NLP researchers have developed a range of techniques including statistical models, rule-based systems, and deep learning algorithms.\n",
            "One popular application of NLP is sentiment analysis, which involves analyzing text to determine the emotional tone of the writer.\n",
            "This can be useful for businesses who want to understand how their customers feel about their products or services.\n",
            "Another application is chatbots, which use NLP to understand and respond to user queries in a natural way.\n",
            "Despite its many benefits, NLP also raises ethical concerns around issues such as privacy and bias.\n",
            "For example, if NLP is used to analyze social media data, there is a risk that personal information could be exposed.\n",
            "Additionally, if the algorithms used in NLP are biased, they could perpetuate discrimination against certain groups of people.\n",
            "In conclusion, NLP is a rapidly growing field with many exciting applications.\n",
            "As we continue to generate vast amounts of unstructured data, NLP techniques will become increasingly important for extracting valuable insights from this data.\n",
            "However, it is important to be aware of the ethical implications of using NLP and to ensure that these technologies are developed in a responsible and equitable way.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5- Try the same task using re (regular expression)"
      ],
      "metadata": {
        "id": "C7SPZFLUoA5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#importing regular exp package\n",
        "import re\n",
        "sentence_endings = r\"[.?!]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "#sentence_token = re.split(sentence_endings, data)\n",
        "sentence_token = re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', data)\n",
        "print(sentence_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-06drcAjoNlM",
        "outputId": "66e41728-b048-442d-cf55-0733288ac0da"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['# An Introduction to Natural Language Processing (NLP)\\n\\nNatural Language Processing (NLP) is a field of study that focuses on the interaction between computers and humans using natural language', 'It involves developing algorithms and computational models that can analyze, understand, and generate human language', '\\n\\nNLP has become increasingly important in recent years due to the vast amounts of unstructured data that are generated every day through social media, emails, and other forms of communication', 'By using NLP techniques, it is possible to extract valuable insights from this data, which can be used for a wide range of applications including sentiment analysis, chatbots, and machine translation', '\\n\\nOne of the key challenges in NLP is the ambiguity of human language', 'Words can have multiple meanings depending on the context in which they are used, and grammar rules can be broken without affecting the meaning of a sentence', 'To address these challenges, NLP researchers have developed a range of techniques including statistical models, rule-based systems, and deep learning algorithms', '\\n\\nOne popular application of NLP is sentiment analysis, which involves analyzing text to determine the emotional tone of the writer', 'This can be useful for businesses who want to understand how their customers feel about their products or services', 'Another application is chatbots, which use NLP to understand and respond to user queries in a natural way', '\\n\\nDespite its many benefits, NLP also raises ethical concerns around issues such as privacy and bias', 'For example, if NLP is used to analyze social media data, there is a risk that personal information could be exposed', 'Additionally, if the algorithms used in NLP are biased, they could perpetuate discrimination against certain groups of people', '\\n\\nIn conclusion, NLP is a rapidly growing field with many exciting applications', 'As we continue to generate vast amounts of unstructured data, NLP techniques will become increasingly important for extracting valuable insights from this data', 'However, it is important to be aware of the ethical implications of using NLP and to ensure that these technologies are developed in a responsible and equitable way', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation :\n",
        "```\n",
        "re.split(r' *[\\.\\?!][\\'\"\\)\\]]* *', data)\n",
        "1.  *: Matches zero or more whitespace characters.\n",
        "2.  [\\.\\?!]: Matches any of the specified punctuation marks - period, question mark, or exclamation mark.\n",
        "3.  [\\'\"\\)\\]]*: Matches zero or more occurrences of single quotes, double quotes, closing parentheses, or closing square brackets.\n",
        "4.  *: Matches zero or more whitespace characters.\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r0nFgIeOs5eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display sentence token line by line\n",
        "for item in sentence_token :\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "7FuYtcM1ouW1",
        "outputId": "321e31e7-39e0-4232-e875-6adb67026380",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# An Introduction to Natural Language Processing (NLP)\n",
            "\n",
            "Natural Language Processing (NLP) is a field of study that focuses on the interaction between computers and humans using natural language\n",
            "It involves developing algorithms and computational models that can analyze, understand, and generate human language\n",
            "\n",
            "\n",
            "NLP has become increasingly important in recent years due to the vast amounts of unstructured data that are generated every day through social media, emails, and other forms of communication\n",
            "By using NLP techniques, it is possible to extract valuable insights from this data, which can be used for a wide range of applications including sentiment analysis, chatbots, and machine translation\n",
            "\n",
            "\n",
            "One of the key challenges in NLP is the ambiguity of human language\n",
            "Words can have multiple meanings depending on the context in which they are used, and grammar rules can be broken without affecting the meaning of a sentence\n",
            "To address these challenges, NLP researchers have developed a range of techniques including statistical models, rule-based systems, and deep learning algorithms\n",
            "\n",
            "\n",
            "One popular application of NLP is sentiment analysis, which involves analyzing text to determine the emotional tone of the writer\n",
            "This can be useful for businesses who want to understand how their customers feel about their products or services\n",
            "Another application is chatbots, which use NLP to understand and respond to user queries in a natural way\n",
            "\n",
            "\n",
            "Despite its many benefits, NLP also raises ethical concerns around issues such as privacy and bias\n",
            "For example, if NLP is used to analyze social media data, there is a risk that personal information could be exposed\n",
            "Additionally, if the algorithms used in NLP are biased, they could perpetuate discrimination against certain groups of people\n",
            "\n",
            "\n",
            "In conclusion, NLP is a rapidly growing field with many exciting applications\n",
            "As we continue to generate vast amounts of unstructured data, NLP techniques will become increasingly important for extracting valuable insights from this data\n",
            "However, it is important to be aware of the ethical implications of using NLP and to ensure that these technologies are developed in a responsible and equitable way\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split my_string on spaces and print the result\n",
        "spaces = r\"\\s+\"\n",
        "word_token = re.split(spaces, data)\n",
        "print( word_token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K7FB6DLfq4U8",
        "outputId": "ba120663-6c3b-42c5-ee11-dfd6b2fb60e4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#', 'An', 'Introduction', 'to', 'Natural', 'Language', 'Processing', '(NLP)', 'Natural', 'Language', 'Processing', '(NLP)', 'is', 'a', 'field', 'of', 'study', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'using', 'natural', 'language.', 'It', 'involves', 'developing', 'algorithms', 'and', 'computational', 'models', 'that', 'can', 'analyze,', 'understand,', 'and', 'generate', 'human', 'language.', 'NLP', 'has', 'become', 'increasingly', 'important', 'in', 'recent', 'years', 'due', 'to', 'the', 'vast', 'amounts', 'of', 'unstructured', 'data', 'that', 'are', 'generated', 'every', 'day', 'through', 'social', 'media,', 'emails,', 'and', 'other', 'forms', 'of', 'communication.', 'By', 'using', 'NLP', 'techniques,', 'it', 'is', 'possible', 'to', 'extract', 'valuable', 'insights', 'from', 'this', 'data,', 'which', 'can', 'be', 'used', 'for', 'a', 'wide', 'range', 'of', 'applications', 'including', 'sentiment', 'analysis,', 'chatbots,', 'and', 'machine', 'translation.', 'One', 'of', 'the', 'key', 'challenges', 'in', 'NLP', 'is', 'the', 'ambiguity', 'of', 'human', 'language.', 'Words', 'can', 'have', 'multiple', 'meanings', 'depending', 'on', 'the', 'context', 'in', 'which', 'they', 'are', 'used,', 'and', 'grammar', 'rules', 'can', 'be', 'broken', 'without', 'affecting', 'the', 'meaning', 'of', 'a', 'sentence.', 'To', 'address', 'these', 'challenges,', 'NLP', 'researchers', 'have', 'developed', 'a', 'range', 'of', 'techniques', 'including', 'statistical', 'models,', 'rule-based', 'systems,', 'and', 'deep', 'learning', 'algorithms.', 'One', 'popular', 'application', 'of', 'NLP', 'is', 'sentiment', 'analysis,', 'which', 'involves', 'analyzing', 'text', 'to', 'determine', 'the', 'emotional', 'tone', 'of', 'the', 'writer.', 'This', 'can', 'be', 'useful', 'for', 'businesses', 'who', 'want', 'to', 'understand', 'how', 'their', 'customers', 'feel', 'about', 'their', 'products', 'or', 'services.', 'Another', 'application', 'is', 'chatbots,', 'which', 'use', 'NLP', 'to', 'understand', 'and', 'respond', 'to', 'user', 'queries', 'in', 'a', 'natural', 'way.', 'Despite', 'its', 'many', 'benefits,', 'NLP', 'also', 'raises', 'ethical', 'concerns', 'around', 'issues', 'such', 'as', 'privacy', 'and', 'bias.', 'For', 'example,', 'if', 'NLP', 'is', 'used', 'to', 'analyze', 'social', 'media', 'data,', 'there', 'is', 'a', 'risk', 'that', 'personal', 'information', 'could', 'be', 'exposed.', 'Additionally,', 'if', 'the', 'algorithms', 'used', 'in', 'NLP', 'are', 'biased,', 'they', 'could', 'perpetuate', 'discrimination', 'against', 'certain', 'groups', 'of', 'people.', 'In', 'conclusion,', 'NLP', 'is', 'a', 'rapidly', 'growing', 'field', 'with', 'many', 'exciting', 'applications.', 'As', 'we', 'continue', 'to', 'generate', 'vast', 'amounts', 'of', 'unstructured', 'data,', 'NLP', 'techniques', 'will', 'become', 'increasingly', 'important', 'for', 'extracting', 'valuable', 'insights', 'from', 'this', 'data.', 'However,', 'it', 'is', 'important', 'to', 'be', 'aware', 'of', 'the', 'ethical', 'implications', 'of', 'using', 'NLP', 'and', 'to', 'ensure', 'that', 'these', 'technologies', 'are', 'developed', 'in', 'a', 'responsible', 'and', 'equitable', 'way.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explnation :\n",
        "\n",
        "```\n",
        "pattern \\s+, which matches one or more whitespace characters\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "t2l8yXb5uYWP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2: Stop Word Removal\n",
        "1. Using the list of English stopwords from NLTK, remove the stopwords from the\n",
        "list of tokens generated in Task 1.\n",
        "2. Print out the list of tokens after stop word removal.\n"
      ],
      "metadata": {
        "id": "BR8xnhOmvXEG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 - Using the list of English stopwords from NLTK, remove the stopwords from the\n",
        "list of tokens generated in Task 1.\n",
        "\n"
      ],
      "metadata": {
        "id": "QOJMPWzzvZRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n"
      ],
      "metadata": {
        "id": "dl2Fch-uvgNo"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Print out the list of tokens after stop word removal."
      ],
      "metadata": {
        "id": "1GZyI7JIve_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens after stop word removal:\")\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt58D6mkv1bt",
        "outputId": "433af2da-00da-43f4-9b20-0189b7d48394"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens after stop word removal:\n",
            "['#', 'Introduction', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'field', 'study', 'focuses', 'interaction', 'computers', 'humans', 'using', 'natural', 'language', '.', 'involves', 'developing', 'algorithms', 'computational', 'models', 'analyze', ',', 'understand', ',', 'generate', 'human', 'language', '.', 'NLP', 'become', 'increasingly', 'important', 'recent', 'years', 'due', 'vast', 'amounts', 'unstructured', 'data', 'generated', 'every', 'day', 'social', 'media', ',', 'emails', ',', 'forms', 'communication', '.', 'using', 'NLP', 'techniques', ',', 'possible', 'extract', 'valuable', 'insights', 'data', ',', 'used', 'wide', 'range', 'applications', 'including', 'sentiment', 'analysis', ',', 'chatbots', ',', 'machine', 'translation', '.', 'One', 'key', 'challenges', 'NLP', 'ambiguity', 'human', 'language', '.', 'Words', 'multiple', 'meanings', 'depending', 'context', 'used', ',', 'grammar', 'rules', 'broken', 'without', 'affecting', 'meaning', 'sentence', '.', 'address', 'challenges', ',', 'NLP', 'researchers', 'developed', 'range', 'techniques', 'including', 'statistical', 'models', ',', 'rule-based', 'systems', ',', 'deep', 'learning', 'algorithms', '.', 'One', 'popular', 'application', 'NLP', 'sentiment', 'analysis', ',', 'involves', 'analyzing', 'text', 'determine', 'emotional', 'tone', 'writer', '.', 'useful', 'businesses', 'want', 'understand', 'customers', 'feel', 'products', 'services', '.', 'Another', 'application', 'chatbots', ',', 'use', 'NLP', 'understand', 'respond', 'user', 'queries', 'natural', 'way', '.', 'Despite', 'many', 'benefits', ',', 'NLP', 'also', 'raises', 'ethical', 'concerns', 'around', 'issues', 'privacy', 'bias', '.', 'example', ',', 'NLP', 'used', 'analyze', 'social', 'media', 'data', ',', 'risk', 'personal', 'information', 'could', 'exposed', '.', 'Additionally', ',', 'algorithms', 'used', 'NLP', 'biased', ',', 'could', 'perpetuate', 'discrimination', 'certain', 'groups', 'people', '.', 'conclusion', ',', 'NLP', 'rapidly', 'growing', 'field', 'many', 'exciting', 'applications', '.', 'continue', 'generate', 'vast', 'amounts', 'unstructured', 'data', ',', 'NLP', 'techniques', 'become', 'increasingly', 'important', 'extracting', 'valuable', 'insights', 'data', '.', 'However', ',', 'important', 'aware', 'ethical', 'implications', 'using', 'NLP', 'ensure', 'technologies', 'developed', 'responsible', 'equitable', 'way', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3: Stemming\n",
        "1. Apply stemming using the Porter Stemmer to the list of tokens after stop word\n",
        "removal.\n",
        "2. Print out the list of tokens after stemming."
      ],
      "metadata": {
        "id": "3RkrQU9Ev4XE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 - Apply stemming using the Porter Stemmer to the list of tokens after stop word\n",
        "removal.\n"
      ],
      "metadata": {
        "id": "TLSU3T21v6Sm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]"
      ],
      "metadata": {
        "id": "eZIOy1Q-v9sw"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Print out the list of tokens after stemming."
      ],
      "metadata": {
        "id": "EQ2BK2dev-nq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens after stemming:\")\n",
        "print(stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Lf67yY1wGR5",
        "outputId": "25b10180-72eb-491c-e000-e76654a1eea9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens after stemming:\n",
            "['#', 'introduct', 'natur', 'languag', 'process', '(', 'nlp', ')', 'natur', 'languag', 'process', '(', 'nlp', ')', 'field', 'studi', 'focus', 'interact', 'comput', 'human', 'use', 'natur', 'languag', '.', 'involv', 'develop', 'algorithm', 'comput', 'model', 'analyz', ',', 'understand', ',', 'gener', 'human', 'languag', '.', 'nlp', 'becom', 'increasingli', 'import', 'recent', 'year', 'due', 'vast', 'amount', 'unstructur', 'data', 'gener', 'everi', 'day', 'social', 'media', ',', 'email', ',', 'form', 'commun', '.', 'use', 'nlp', 'techniqu', ',', 'possibl', 'extract', 'valuabl', 'insight', 'data', ',', 'use', 'wide', 'rang', 'applic', 'includ', 'sentiment', 'analysi', ',', 'chatbot', ',', 'machin', 'translat', '.', 'one', 'key', 'challeng', 'nlp', 'ambigu', 'human', 'languag', '.', 'word', 'multipl', 'mean', 'depend', 'context', 'use', ',', 'grammar', 'rule', 'broken', 'without', 'affect', 'mean', 'sentenc', '.', 'address', 'challeng', ',', 'nlp', 'research', 'develop', 'rang', 'techniqu', 'includ', 'statist', 'model', ',', 'rule-bas', 'system', ',', 'deep', 'learn', 'algorithm', '.', 'one', 'popular', 'applic', 'nlp', 'sentiment', 'analysi', ',', 'involv', 'analyz', 'text', 'determin', 'emot', 'tone', 'writer', '.', 'use', 'busi', 'want', 'understand', 'custom', 'feel', 'product', 'servic', '.', 'anoth', 'applic', 'chatbot', ',', 'use', 'nlp', 'understand', 'respond', 'user', 'queri', 'natur', 'way', '.', 'despit', 'mani', 'benefit', ',', 'nlp', 'also', 'rais', 'ethic', 'concern', 'around', 'issu', 'privaci', 'bia', '.', 'exampl', ',', 'nlp', 'use', 'analyz', 'social', 'media', 'data', ',', 'risk', 'person', 'inform', 'could', 'expos', '.', 'addit', ',', 'algorithm', 'use', 'nlp', 'bias', ',', 'could', 'perpetu', 'discrimin', 'certain', 'group', 'peopl', '.', 'conclus', ',', 'nlp', 'rapidli', 'grow', 'field', 'mani', 'excit', 'applic', '.', 'continu', 'gener', 'vast', 'amount', 'unstructur', 'data', ',', 'nlp', 'techniqu', 'becom', 'increasingli', 'import', 'extract', 'valuabl', 'insight', 'data', '.', 'howev', ',', 'import', 'awar', 'ethic', 'implic', 'use', 'nlp', 'ensur', 'technolog', 'develop', 'respons', 'equit', 'way', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4: Lemmatization\n",
        "1. Apply lemmatization using the WordNet Lemmatizer to the list of tokens after\n",
        "stop word removal.\n",
        "2. Print out the list of tokens after lemmatization.\n",
        "3. Perform the same task using spaCy and find out the differences(if any)\n"
      ],
      "metadata": {
        "id": "Keqa3W-9wJ2Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 - Apply lemmatization using the WordNet Lemmatizer to the list of tokens after\n",
        "stop word removal.\n",
        "\n"
      ],
      "metadata": {
        "id": "zP_oPkN_wLTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]"
      ],
      "metadata": {
        "id": "P4yFHDDRwVFe"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Print out the list of tokens after lemmatization.\n"
      ],
      "metadata": {
        "id": "jqjmhXGQwN60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens after lemmatization:\")\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEHd6w_AwX-v",
        "outputId": "5c259d42-e51d-4352-f512-d6257770a498"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens after lemmatization:\n",
            "['#', 'Introduction', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'field', 'study', 'focus', 'interaction', 'computer', 'human', 'using', 'natural', 'language', '.', 'involves', 'developing', 'algorithm', 'computational', 'model', 'analyze', ',', 'understand', ',', 'generate', 'human', 'language', '.', 'NLP', 'become', 'increasingly', 'important', 'recent', 'year', 'due', 'vast', 'amount', 'unstructured', 'data', 'generated', 'every', 'day', 'social', 'medium', ',', 'email', ',', 'form', 'communication', '.', 'using', 'NLP', 'technique', ',', 'possible', 'extract', 'valuable', 'insight', 'data', ',', 'used', 'wide', 'range', 'application', 'including', 'sentiment', 'analysis', ',', 'chatbots', ',', 'machine', 'translation', '.', 'One', 'key', 'challenge', 'NLP', 'ambiguity', 'human', 'language', '.', 'Words', 'multiple', 'meaning', 'depending', 'context', 'used', ',', 'grammar', 'rule', 'broken', 'without', 'affecting', 'meaning', 'sentence', '.', 'address', 'challenge', ',', 'NLP', 'researcher', 'developed', 'range', 'technique', 'including', 'statistical', 'model', ',', 'rule-based', 'system', ',', 'deep', 'learning', 'algorithm', '.', 'One', 'popular', 'application', 'NLP', 'sentiment', 'analysis', ',', 'involves', 'analyzing', 'text', 'determine', 'emotional', 'tone', 'writer', '.', 'useful', 'business', 'want', 'understand', 'customer', 'feel', 'product', 'service', '.', 'Another', 'application', 'chatbots', ',', 'use', 'NLP', 'understand', 'respond', 'user', 'query', 'natural', 'way', '.', 'Despite', 'many', 'benefit', ',', 'NLP', 'also', 'raise', 'ethical', 'concern', 'around', 'issue', 'privacy', 'bias', '.', 'example', ',', 'NLP', 'used', 'analyze', 'social', 'medium', 'data', ',', 'risk', 'personal', 'information', 'could', 'exposed', '.', 'Additionally', ',', 'algorithm', 'used', 'NLP', 'biased', ',', 'could', 'perpetuate', 'discrimination', 'certain', 'group', 'people', '.', 'conclusion', ',', 'NLP', 'rapidly', 'growing', 'field', 'many', 'exciting', 'application', '.', 'continue', 'generate', 'vast', 'amount', 'unstructured', 'data', ',', 'NLP', 'technique', 'become', 'increasingly', 'important', 'extracting', 'valuable', 'insight', 'data', '.', 'However', ',', 'important', 'aware', 'ethical', 'implication', 'using', 'NLP', 'ensure', 'technology', 'developed', 'responsible', 'equitable', 'way', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3 - Perform the same task using spaCy and find out the differences(if any)"
      ],
      "metadata": {
        "id": "ZDRf73KywQfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrmvVXADwcH1",
        "outputId": "e63015a1-46a8-41ec-ac3e-21512cafb718"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.2)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# Input text\n",
        "# Reading from previous \"data\"\n",
        "'''\n",
        "text_file = open(\"/content/nlp_text.txt\", \"r\")\n",
        "data = text_file.read()\n",
        "'''\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(data)\n",
        "# Perform lemmatization on each token\n",
        "lemmatized_words = [token.lemma_ for token in doc]\n",
        "# Join the lemmatized words back into a sentence\n",
        "lemmatized_text = ' '.join(lemmatized_words)\n",
        "\n"
      ],
      "metadata": {
        "id": "SoWHNgSFwfna"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"*\" * 25 + \" Original Text: \" + \"*\" * 25)\n",
        "print(data)\n",
        "#print(\"Lemmatized Text:\", lemmatized_text)\n",
        "print(\"\\n\"+\"*\" * 25 + \" Lemmatized Text: \" + \"*\" * 25)\n",
        "print(lemmatized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSldvCxsxNJs",
        "outputId": "d0744333-5f53-4aef-ac16-acb60694b95a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "************************* Original Text: *************************\n",
            "# An Introduction to Natural Language Processing (NLP)\n",
            "\n",
            "Natural Language Processing (NLP) is a field of study that focuses on the interaction between computers and humans using natural language. It involves developing algorithms and computational models that can analyze, understand, and generate human language.\n",
            "\n",
            "NLP has become increasingly important in recent years due to the vast amounts of unstructured data that are generated every day through social media, emails, and other forms of communication. By using NLP techniques, it is possible to extract valuable insights from this data, which can be used for a wide range of applications including sentiment analysis, chatbots, and machine translation.\n",
            "\n",
            "One of the key challenges in NLP is the ambiguity of human language. Words can have multiple meanings depending on the context in which they are used, and grammar rules can be broken without affecting the meaning of a sentence. To address these challenges, NLP researchers have developed a range of techniques including statistical models, rule-based systems, and deep learning algorithms.\n",
            "\n",
            "One popular application of NLP is sentiment analysis, which involves analyzing text to determine the emotional tone of the writer. This can be useful for businesses who want to understand how their customers feel about their products or services. Another application is chatbots, which use NLP to understand and respond to user queries in a natural way.\n",
            "\n",
            "Despite its many benefits, NLP also raises ethical concerns around issues such as privacy and bias. For example, if NLP is used to analyze social media data, there is a risk that personal information could be exposed. Additionally, if the algorithms used in NLP are biased, they could perpetuate discrimination against certain groups of people.\n",
            "\n",
            "In conclusion, NLP is a rapidly growing field with many exciting applications. As we continue to generate vast amounts of unstructured data, NLP techniques will become increasingly important for extracting valuable insights from this data. However, it is important to be aware of the ethical implications of using NLP and to ensure that these technologies are developed in a responsible and equitable way.\n",
            "\n",
            "************************* Lemmatized Text: *************************\n",
            "# an introduction to Natural Language Processing ( NLP ) \n",
            "\n",
            " Natural Language Processing ( NLP ) be a field of study that focus on the interaction between computer and human use natural language . it involve develop algorithm and computational model that can analyze , understand , and generate human language . \n",
            "\n",
            " NLP have become increasingly important in recent year due to the vast amount of unstructured datum that be generate every day through social medium , email , and other form of communication . by use NLP technique , it be possible to extract valuable insight from this datum , which can be use for a wide range of application include sentiment analysis , chatbot , and machine translation . \n",
            "\n",
            " one of the key challenge in NLP be the ambiguity of human language . word can have multiple meaning depend on the context in which they be use , and grammar rule can be break without affect the meaning of a sentence . to address these challenge , NLP researcher have develop a range of technique include statistical model , rule - base system , and deep learning algorithm . \n",
            "\n",
            " one popular application of NLP be sentiment analysis , which involve analyze text to determine the emotional tone of the writer . this can be useful for business who want to understand how their customer feel about their product or service . another application be chatbot , which use NLP to understand and respond to user query in a natural way . \n",
            "\n",
            " despite its many benefit , NLP also raise ethical concern around issue such as privacy and bias . for example , if NLP be use to analyze social medium datum , there be a risk that personal information could be expose . additionally , if the algorithm use in NLP be bias , they could perpetuate discrimination against certain group of people . \n",
            "\n",
            " in conclusion , NLP be a rapidly grow field with many exciting application . as we continue to generate vast amount of unstructured datum , NLP technique will become increasingly important for extract valuable insight from this datum . however , it be important to be aware of the ethical implication of use NLP and to ensure that these technology be develop in a responsible and equitable way .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5: Chunking\n",
        "1. Perform chunking on the list of tokens after stop word removal and before\n",
        "stemming.\n",
        "2. Create a chunk grammar that chunks the tokens into noun phrases.\n",
        "Print out the resulting chunks."
      ],
      "metadata": {
        "id": "m2ued5b3xxtI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1 - Perform chunking on the list of tokens after stop word removal and before stemming.\n"
      ],
      "metadata": {
        "id": "MrNYhnAnxzGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text for POS tagging\n",
        "'''\n",
        "text_file = open(\"/content/nlp_text.txt\", \"r\")\n",
        "data = text_file.read()\n",
        "'''\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(data)\n",
        "\n",
        "# Iterate over the tokens and print their text and POS tags\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, POS: {token.pos_}, POS Tag: {token.tag_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4HjG6Oax4Rp",
        "outputId": "9acbc42c-9166-4a75-93af-b59ab5793fc2"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: #, POS: PUNCT, POS Tag: NFP\n",
            "Token: An, POS: DET, POS Tag: DT\n",
            "Token: Introduction, POS: NOUN, POS Tag: NN\n",
            "Token: to, POS: ADP, POS Tag: IN\n",
            "Token: Natural, POS: PROPN, POS Tag: NNP\n",
            "Token: Language, POS: PROPN, POS Tag: NNP\n",
            "Token: Processing, POS: PROPN, POS Tag: NNP\n",
            "Token: (, POS: PUNCT, POS Tag: -LRB-\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: ), POS: PUNCT, POS Tag: -RRB-\n",
            "Token: \n",
            "\n",
            ", POS: SPACE, POS Tag: _SP\n",
            "Token: Natural, POS: PROPN, POS Tag: NNP\n",
            "Token: Language, POS: PROPN, POS Tag: NNP\n",
            "Token: Processing, POS: PROPN, POS Tag: NNP\n",
            "Token: (, POS: PUNCT, POS Tag: -LRB-\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: ), POS: PUNCT, POS Tag: -RRB-\n",
            "Token: is, POS: AUX, POS Tag: VBZ\n",
            "Token: a, POS: DET, POS Tag: DT\n",
            "Token: field, POS: NOUN, POS Tag: NN\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: study, POS: NOUN, POS Tag: NN\n",
            "Token: that, POS: PRON, POS Tag: WDT\n",
            "Token: focuses, POS: VERB, POS Tag: VBZ\n",
            "Token: on, POS: ADP, POS Tag: IN\n",
            "Token: the, POS: DET, POS Tag: DT\n",
            "Token: interaction, POS: NOUN, POS Tag: NN\n",
            "Token: between, POS: ADP, POS Tag: IN\n",
            "Token: computers, POS: NOUN, POS Tag: NNS\n",
            "Token: and, POS: CCONJ, POS Tag: CC\n",
            "Token: humans, POS: NOUN, POS Tag: NNS\n",
            "Token: using, POS: VERB, POS Tag: VBG\n",
            "Token: natural, POS: ADJ, POS Tag: JJ\n",
            "Token: language, POS: NOUN, POS Tag: NN\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: It, POS: PRON, POS Tag: PRP\n",
            "Token: involves, POS: VERB, POS Tag: VBZ\n",
            "Token: developing, POS: VERB, POS Tag: VBG\n",
            "Token: algorithms, POS: NOUN, POS Tag: NNS\n",
            "Token: and, POS: CCONJ, POS Tag: CC\n",
            "Token: computational, POS: ADJ, POS Tag: JJ\n",
            "Token: models, POS: NOUN, POS Tag: NNS\n",
            "Token: that, POS: PRON, POS Tag: WDT\n",
            "Token: can, POS: AUX, POS Tag: MD\n",
            "Token: analyze, POS: VERB, POS Tag: VB\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: understand, POS: VERB, POS Tag: VB\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: and, POS: CCONJ, POS Tag: CC\n",
            "Token: generate, POS: VERB, POS Tag: VB\n",
            "Token: human, POS: ADJ, POS Tag: JJ\n",
            "Token: language, POS: NOUN, POS Tag: NN\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: \n",
            "\n",
            ", POS: SPACE, POS Tag: _SP\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: has, POS: AUX, POS Tag: VBZ\n",
            "Token: become, POS: VERB, POS Tag: VBN\n",
            "Token: increasingly, POS: ADV, POS Tag: RB\n",
            "Token: important, POS: ADJ, POS Tag: JJ\n",
            "Token: in, POS: ADP, POS Tag: IN\n",
            "Token: recent, POS: ADJ, POS Tag: JJ\n",
            "Token: years, POS: NOUN, POS Tag: NNS\n",
            "Token: due, POS: ADP, POS Tag: IN\n",
            "Token: to, POS: ADP, POS Tag: IN\n",
            "Token: the, POS: DET, POS Tag: DT\n",
            "Token: vast, POS: ADJ, POS Tag: JJ\n",
            "Token: amounts, POS: NOUN, POS Tag: NNS\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: unstructured, POS: ADJ, POS Tag: JJ\n",
            "Token: data, POS: NOUN, POS Tag: NNS\n",
            "Token: that, POS: PRON, POS Tag: WDT\n",
            "Token: are, POS: AUX, POS Tag: VBP\n",
            "Token: generated, POS: VERB, POS Tag: VBN\n",
            "Token: every, POS: DET, POS Tag: DT\n",
            "Token: day, POS: NOUN, POS Tag: NN\n",
            "Token: through, POS: ADP, POS Tag: IN\n",
            "Token: social, POS: ADJ, POS Tag: JJ\n",
            "Token: media, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: emails, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: and, POS: CCONJ, POS Tag: CC\n",
            "Token: other, POS: ADJ, POS Tag: JJ\n",
            "Token: forms, POS: NOUN, POS Tag: NNS\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: communication, POS: NOUN, POS Tag: NN\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: By, POS: ADP, POS Tag: IN\n",
            "Token: using, POS: VERB, POS Tag: VBG\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: techniques, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: it, POS: PRON, POS Tag: PRP\n",
            "Token: is, POS: AUX, POS Tag: VBZ\n",
            "Token: possible, POS: ADJ, POS Tag: JJ\n",
            "Token: to, POS: PART, POS Tag: TO\n",
            "Token: extract, POS: VERB, POS Tag: VB\n",
            "Token: valuable, POS: ADJ, POS Tag: JJ\n",
            "Token: insights, POS: NOUN, POS Tag: NNS\n",
            "Token: from, POS: ADP, POS Tag: IN\n",
            "Token: this, POS: DET, POS Tag: DT\n",
            "Token: data, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: which, POS: PRON, POS Tag: WDT\n",
            "Token: can, POS: AUX, POS Tag: MD\n",
            "Token: be, POS: AUX, POS Tag: VB\n",
            "Token: used, POS: VERB, POS Tag: VBN\n",
            "Token: for, POS: ADP, POS Tag: IN\n",
            "Token: a, POS: DET, POS Tag: DT\n",
            "Token: wide, POS: ADJ, POS Tag: JJ\n",
            "Token: range, POS: NOUN, POS Tag: NN\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: applications, POS: NOUN, POS Tag: NNS\n",
            "Token: including, POS: VERB, POS Tag: VBG\n",
            "Token: sentiment, POS: NOUN, POS Tag: NN\n",
            "Token: analysis, POS: NOUN, POS Tag: NN\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: chatbots, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: and, POS: CCONJ, POS Tag: CC\n",
            "Token: machine, POS: NOUN, POS Tag: NN\n",
            "Token: translation, POS: NOUN, POS Tag: NN\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: \n",
            "\n",
            ", POS: SPACE, POS Tag: _SP\n",
            "Token: One, POS: NUM, POS Tag: CD\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: the, POS: DET, POS Tag: DT\n",
            "Token: key, POS: ADJ, POS Tag: JJ\n",
            "Token: challenges, POS: NOUN, POS Tag: NNS\n",
            "Token: in, POS: ADP, POS Tag: IN\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: is, POS: AUX, POS Tag: VBZ\n",
            "Token: the, POS: DET, POS Tag: DT\n",
            "Token: ambiguity, POS: NOUN, POS Tag: NN\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: human, POS: ADJ, POS Tag: JJ\n",
            "Token: language, POS: NOUN, POS Tag: NN\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: Words, POS: NOUN, POS Tag: NNS\n",
            "Token: can, POS: AUX, POS Tag: MD\n",
            "Token: have, POS: VERB, POS Tag: VB\n",
            "Token: multiple, POS: ADJ, POS Tag: JJ\n",
            "Token: meanings, POS: NOUN, POS Tag: NNS\n",
            "Token: depending, POS: VERB, POS Tag: VBG\n",
            "Token: on, POS: ADP, POS Tag: IN\n",
            "Token: the, POS: DET, POS Tag: DT\n",
            "Token: context, POS: NOUN, POS Tag: NN\n",
            "Token: in, POS: ADP, POS Tag: IN\n",
            "Token: which, POS: PRON, POS Tag: WDT\n",
            "Token: they, POS: PRON, POS Tag: PRP\n",
            "Token: are, POS: AUX, POS Tag: VBP\n",
            "Token: used, POS: VERB, POS Tag: VBN\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: and, POS: CCONJ, POS Tag: CC\n",
            "Token: grammar, POS: NOUN, POS Tag: NN\n",
            "Token: rules, POS: NOUN, POS Tag: NNS\n",
            "Token: can, POS: AUX, POS Tag: MD\n",
            "Token: be, POS: AUX, POS Tag: VB\n",
            "Token: broken, POS: VERB, POS Tag: VBN\n",
            "Token: without, POS: ADP, POS Tag: IN\n",
            "Token: affecting, POS: VERB, POS Tag: VBG\n",
            "Token: the, POS: DET, POS Tag: DT\n",
            "Token: meaning, POS: NOUN, POS Tag: NN\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: a, POS: DET, POS Tag: DT\n",
            "Token: sentence, POS: NOUN, POS Tag: NN\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: To, POS: PART, POS Tag: TO\n",
            "Token: address, POS: VERB, POS Tag: VB\n",
            "Token: these, POS: DET, POS Tag: DT\n",
            "Token: challenges, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: researchers, POS: NOUN, POS Tag: NNS\n",
            "Token: have, POS: AUX, POS Tag: VBP\n",
            "Token: developed, POS: VERB, POS Tag: VBN\n",
            "Token: a, POS: DET, POS Tag: DT\n",
            "Token: range, POS: NOUN, POS Tag: NN\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: techniques, POS: NOUN, POS Tag: NNS\n",
            "Token: including, POS: VERB, POS Tag: VBG\n",
            "Token: statistical, POS: ADJ, POS Tag: JJ\n",
            "Token: models, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: rule, POS: NOUN, POS Tag: NN\n",
            "Token: -, POS: PUNCT, POS Tag: HYPH\n",
            "Token: based, POS: VERB, POS Tag: VBN\n",
            "Token: systems, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: and, POS: CCONJ, POS Tag: CC\n",
            "Token: deep, POS: ADJ, POS Tag: JJ\n",
            "Token: learning, POS: NOUN, POS Tag: NN\n",
            "Token: algorithms, POS: NOUN, POS Tag: NNS\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: \n",
            "\n",
            ", POS: SPACE, POS Tag: _SP\n",
            "Token: One, POS: NUM, POS Tag: CD\n",
            "Token: popular, POS: ADJ, POS Tag: JJ\n",
            "Token: application, POS: NOUN, POS Tag: NN\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: is, POS: AUX, POS Tag: VBZ\n",
            "Token: sentiment, POS: NOUN, POS Tag: NN\n",
            "Token: analysis, POS: NOUN, POS Tag: NN\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: which, POS: PRON, POS Tag: WDT\n",
            "Token: involves, POS: VERB, POS Tag: VBZ\n",
            "Token: analyzing, POS: VERB, POS Tag: VBG\n",
            "Token: text, POS: NOUN, POS Tag: NN\n",
            "Token: to, POS: PART, POS Tag: TO\n",
            "Token: determine, POS: VERB, POS Tag: VB\n",
            "Token: the, POS: DET, POS Tag: DT\n",
            "Token: emotional, POS: ADJ, POS Tag: JJ\n",
            "Token: tone, POS: NOUN, POS Tag: NN\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: the, POS: DET, POS Tag: DT\n",
            "Token: writer, POS: NOUN, POS Tag: NN\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: This, POS: PRON, POS Tag: DT\n",
            "Token: can, POS: AUX, POS Tag: MD\n",
            "Token: be, POS: AUX, POS Tag: VB\n",
            "Token: useful, POS: ADJ, POS Tag: JJ\n",
            "Token: for, POS: ADP, POS Tag: IN\n",
            "Token: businesses, POS: NOUN, POS Tag: NNS\n",
            "Token: who, POS: PRON, POS Tag: WP\n",
            "Token: want, POS: VERB, POS Tag: VBP\n",
            "Token: to, POS: PART, POS Tag: TO\n",
            "Token: understand, POS: VERB, POS Tag: VB\n",
            "Token: how, POS: SCONJ, POS Tag: WRB\n",
            "Token: their, POS: PRON, POS Tag: PRP$\n",
            "Token: customers, POS: NOUN, POS Tag: NNS\n",
            "Token: feel, POS: VERB, POS Tag: VBP\n",
            "Token: about, POS: ADP, POS Tag: IN\n",
            "Token: their, POS: PRON, POS Tag: PRP$\n",
            "Token: products, POS: NOUN, POS Tag: NNS\n",
            "Token: or, POS: CCONJ, POS Tag: CC\n",
            "Token: services, POS: NOUN, POS Tag: NNS\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: Another, POS: DET, POS Tag: DT\n",
            "Token: application, POS: NOUN, POS Tag: NN\n",
            "Token: is, POS: AUX, POS Tag: VBZ\n",
            "Token: chatbots, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: which, POS: PRON, POS Tag: WDT\n",
            "Token: use, POS: VERB, POS Tag: VBP\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: to, POS: PART, POS Tag: TO\n",
            "Token: understand, POS: VERB, POS Tag: VB\n",
            "Token: and, POS: CCONJ, POS Tag: CC\n",
            "Token: respond, POS: VERB, POS Tag: VB\n",
            "Token: to, POS: ADP, POS Tag: IN\n",
            "Token: user, POS: NOUN, POS Tag: NN\n",
            "Token: queries, POS: NOUN, POS Tag: NNS\n",
            "Token: in, POS: ADP, POS Tag: IN\n",
            "Token: a, POS: DET, POS Tag: DT\n",
            "Token: natural, POS: ADJ, POS Tag: JJ\n",
            "Token: way, POS: NOUN, POS Tag: NN\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: \n",
            "\n",
            ", POS: SPACE, POS Tag: _SP\n",
            "Token: Despite, POS: SCONJ, POS Tag: IN\n",
            "Token: its, POS: PRON, POS Tag: PRP$\n",
            "Token: many, POS: ADJ, POS Tag: JJ\n",
            "Token: benefits, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: also, POS: ADV, POS Tag: RB\n",
            "Token: raises, POS: VERB, POS Tag: VBZ\n",
            "Token: ethical, POS: ADJ, POS Tag: JJ\n",
            "Token: concerns, POS: NOUN, POS Tag: NNS\n",
            "Token: around, POS: ADP, POS Tag: IN\n",
            "Token: issues, POS: NOUN, POS Tag: NNS\n",
            "Token: such, POS: ADJ, POS Tag: JJ\n",
            "Token: as, POS: ADP, POS Tag: IN\n",
            "Token: privacy, POS: NOUN, POS Tag: NN\n",
            "Token: and, POS: CCONJ, POS Tag: CC\n",
            "Token: bias, POS: NOUN, POS Tag: NN\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: For, POS: ADP, POS Tag: IN\n",
            "Token: example, POS: NOUN, POS Tag: NN\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: if, POS: SCONJ, POS Tag: IN\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: is, POS: AUX, POS Tag: VBZ\n",
            "Token: used, POS: VERB, POS Tag: VBN\n",
            "Token: to, POS: PART, POS Tag: TO\n",
            "Token: analyze, POS: VERB, POS Tag: VB\n",
            "Token: social, POS: ADJ, POS Tag: JJ\n",
            "Token: media, POS: NOUN, POS Tag: NNS\n",
            "Token: data, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: there, POS: PRON, POS Tag: EX\n",
            "Token: is, POS: VERB, POS Tag: VBZ\n",
            "Token: a, POS: DET, POS Tag: DT\n",
            "Token: risk, POS: NOUN, POS Tag: NN\n",
            "Token: that, POS: SCONJ, POS Tag: IN\n",
            "Token: personal, POS: ADJ, POS Tag: JJ\n",
            "Token: information, POS: NOUN, POS Tag: NN\n",
            "Token: could, POS: AUX, POS Tag: MD\n",
            "Token: be, POS: AUX, POS Tag: VB\n",
            "Token: exposed, POS: VERB, POS Tag: VBN\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: Additionally, POS: ADV, POS Tag: RB\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: if, POS: SCONJ, POS Tag: IN\n",
            "Token: the, POS: DET, POS Tag: DT\n",
            "Token: algorithms, POS: NOUN, POS Tag: NNS\n",
            "Token: used, POS: VERB, POS Tag: VBN\n",
            "Token: in, POS: ADP, POS Tag: IN\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: are, POS: AUX, POS Tag: VBP\n",
            "Token: biased, POS: VERB, POS Tag: VBN\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: they, POS: PRON, POS Tag: PRP\n",
            "Token: could, POS: AUX, POS Tag: MD\n",
            "Token: perpetuate, POS: VERB, POS Tag: VB\n",
            "Token: discrimination, POS: NOUN, POS Tag: NN\n",
            "Token: against, POS: ADP, POS Tag: IN\n",
            "Token: certain, POS: ADJ, POS Tag: JJ\n",
            "Token: groups, POS: NOUN, POS Tag: NNS\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: people, POS: NOUN, POS Tag: NNS\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: \n",
            "\n",
            ", POS: SPACE, POS Tag: _SP\n",
            "Token: In, POS: ADP, POS Tag: IN\n",
            "Token: conclusion, POS: NOUN, POS Tag: NN\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: is, POS: AUX, POS Tag: VBZ\n",
            "Token: a, POS: DET, POS Tag: DT\n",
            "Token: rapidly, POS: ADV, POS Tag: RB\n",
            "Token: growing, POS: VERB, POS Tag: VBG\n",
            "Token: field, POS: NOUN, POS Tag: NN\n",
            "Token: with, POS: ADP, POS Tag: IN\n",
            "Token: many, POS: ADJ, POS Tag: JJ\n",
            "Token: exciting, POS: ADJ, POS Tag: JJ\n",
            "Token: applications, POS: NOUN, POS Tag: NNS\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: As, POS: SCONJ, POS Tag: IN\n",
            "Token: we, POS: PRON, POS Tag: PRP\n",
            "Token: continue, POS: VERB, POS Tag: VBP\n",
            "Token: to, POS: PART, POS Tag: TO\n",
            "Token: generate, POS: VERB, POS Tag: VB\n",
            "Token: vast, POS: ADJ, POS Tag: JJ\n",
            "Token: amounts, POS: NOUN, POS Tag: NNS\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: unstructured, POS: ADJ, POS Tag: JJ\n",
            "Token: data, POS: NOUN, POS Tag: NNS\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: techniques, POS: NOUN, POS Tag: NNS\n",
            "Token: will, POS: AUX, POS Tag: MD\n",
            "Token: become, POS: VERB, POS Tag: VB\n",
            "Token: increasingly, POS: ADV, POS Tag: RB\n",
            "Token: important, POS: ADJ, POS Tag: JJ\n",
            "Token: for, POS: ADP, POS Tag: IN\n",
            "Token: extracting, POS: VERB, POS Tag: VBG\n",
            "Token: valuable, POS: ADJ, POS Tag: JJ\n",
            "Token: insights, POS: NOUN, POS Tag: NNS\n",
            "Token: from, POS: ADP, POS Tag: IN\n",
            "Token: this, POS: DET, POS Tag: DT\n",
            "Token: data, POS: NOUN, POS Tag: NNS\n",
            "Token: ., POS: PUNCT, POS Tag: .\n",
            "Token: However, POS: ADV, POS Tag: RB\n",
            "Token: ,, POS: PUNCT, POS Tag: ,\n",
            "Token: it, POS: PRON, POS Tag: PRP\n",
            "Token: is, POS: AUX, POS Tag: VBZ\n",
            "Token: important, POS: ADJ, POS Tag: JJ\n",
            "Token: to, POS: PART, POS Tag: TO\n",
            "Token: be, POS: AUX, POS Tag: VB\n",
            "Token: aware, POS: ADJ, POS Tag: JJ\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: the, POS: DET, POS Tag: DT\n",
            "Token: ethical, POS: ADJ, POS Tag: JJ\n",
            "Token: implications, POS: NOUN, POS Tag: NNS\n",
            "Token: of, POS: ADP, POS Tag: IN\n",
            "Token: using, POS: VERB, POS Tag: VBG\n",
            "Token: NLP, POS: PROPN, POS Tag: NNP\n",
            "Token: and, POS: CCONJ, POS Tag: CC\n",
            "Token: to, POS: PART, POS Tag: TO\n",
            "Token: ensure, POS: VERB, POS Tag: VB\n",
            "Token: that, POS: SCONJ, POS Tag: IN\n",
            "Token: these, POS: DET, POS Tag: DT\n",
            "Token: technologies, POS: NOUN, POS Tag: NNS\n",
            "Token: are, POS: AUX, POS Tag: VBP\n",
            "Token: developed, POS: VERB, POS Tag: VBN\n",
            "Token: in, POS: ADP, POS Tag: IN\n",
            "Token: a, POS: DET, POS Tag: DT\n",
            "Token: responsible, POS: ADJ, POS Tag: JJ\n",
            "Token: and, POS: CCONJ, POS Tag: CC\n",
            "Token: equitable, POS: ADJ, POS Tag: JJ\n",
            "Token: way, POS: NOUN, POS Tag: NN\n",
            "Token: ., POS: PUNCT, POS Tag: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Create a chunk grammar that chunks the tokens into noun phrases.Print out the resulting chunks."
      ],
      "metadata": {
        "id": "y_pSjRFvx3EY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#The noun chunks are contained in the doc.noun_chunks class variable\n",
        "for noun_chunk in doc.noun_chunks:\n",
        "    print(noun_chunk.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaieDJhH0mO4",
        "outputId": "3dfb5a36-6247-4f87-d78d-3e65c461feb5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An Introduction\n",
            "NLP\n",
            "NLP\n",
            "a field\n",
            "study\n",
            "that\n",
            "the interaction\n",
            "computers\n",
            "humans\n",
            "natural language\n",
            "It\n",
            "algorithms\n",
            "computational models\n",
            "that\n",
            "human language\n",
            "NLP\n",
            "recent years\n",
            "the vast amounts\n",
            "unstructured data\n",
            "that\n",
            "social media\n",
            "emails\n",
            "other forms\n",
            "communication\n",
            "NLP techniques\n",
            "it\n",
            "valuable insights\n",
            "this data\n",
            "which\n",
            "a wide range\n",
            "applications\n",
            "sentiment analysis\n",
            "chatbots\n",
            "machine translation\n",
            "the key challenges\n",
            "NLP\n",
            "the ambiguity\n",
            "human language\n",
            "Words\n",
            "multiple meanings\n",
            "the context\n",
            "which\n",
            "they\n",
            "grammar rules\n",
            "the meaning\n",
            "a sentence\n",
            "these challenges\n",
            "NLP researchers\n",
            "a range\n",
            "techniques\n",
            "statistical models\n",
            "rule-based systems\n",
            "deep learning algorithms\n",
            "One popular application\n",
            "NLP\n",
            "sentiment analysis\n",
            "which\n",
            "text\n",
            "the emotional tone\n",
            "the writer\n",
            "This\n",
            "businesses\n",
            "who\n",
            "their customers\n",
            "their products\n",
            "services\n",
            "Another application\n",
            "chatbots\n",
            "which\n",
            "NLP\n",
            "user queries\n",
            "a natural way\n",
            "its many benefits\n",
            "NLP\n",
            "ethical concerns\n",
            "issues\n",
            "privacy\n",
            "bias\n",
            "example\n",
            "NLP\n",
            "social media data\n",
            "a risk\n",
            "personal information\n",
            "the algorithms\n",
            "NLP\n",
            "they\n",
            "discrimination\n",
            "certain groups\n",
            "people\n",
            "conclusion\n",
            "NLP\n",
            "a rapidly growing field\n",
            "many exciting applications\n",
            "we\n",
            "vast amounts\n",
            "unstructured data\n",
            "NLP techniques\n",
            "valuable insights\n",
            "this data\n",
            "it\n",
            "the ethical implications\n",
            "NLP\n",
            "these technologies\n",
            "a responsible and equitable way\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ref : [Extracting noun chunks](https://subscription.packtpub.com/book/data/9781838987312/2/ch02lvl1sec14/extracting-noun-chunks)"
      ],
      "metadata": {
        "id": "ahuzM1CF2nHv"
      }
    }
  ]
}