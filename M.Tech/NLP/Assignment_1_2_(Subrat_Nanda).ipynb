{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Natural Language Processing (PDS3102)\n",
        "\n",
        "Submitted By : Subrat Ku Nanda"
      ],
      "metadata": {
        "id": "_cysfKLjf_Uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 1.2\n"
      ],
      "metadata": {
        "id": "2QzEX2Cdgwo6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the English spaCy model.\n",
        "2. Process a sample review text: \"The food was amazing, and the service was\n",
        "impeccable.\"\n",
        "3. Perform part-of-speech tagging on the processed text.\n",
        "4. Extract and print all the noun phrases (chunks) and verb phrases that represent\n",
        "aspects of the restaurant, along with their corresponding POS tags."
      ],
      "metadata": {
        "id": "TnOFQuQtvh6O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "SgiifdyqvyZr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1qVB5havcAm",
        "outputId": "6e275b90-b7dc-4b1a-d68e-61cb42e8b82f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token: The, POS: DET, POS Tag: DT, POS Depe :det\n",
            "Token: food, POS: NOUN, POS Tag: NN, POS Depe :nsubj\n",
            "Token: was, POS: AUX, POS Tag: VBD, POS Depe :ROOT\n",
            "Token: amazing, POS: ADJ, POS Tag: JJ, POS Depe :acomp\n",
            "Token: ,, POS: PUNCT, POS Tag: ,, POS Depe :punct\n",
            "Token: and, POS: CCONJ, POS Tag: CC, POS Depe :cc\n",
            "Token: the, POS: DET, POS Tag: DT, POS Depe :det\n",
            "Token: service, POS: NOUN, POS Tag: NN, POS Depe :nsubj\n",
            "Token: was, POS: AUX, POS Tag: VBD, POS Depe :conj\n",
            "Token: impeccable, POS: ADJ, POS Tag: JJ, POS Depe :acomp\n",
            "Token: ., POS: PUNCT, POS Tag: ., POS Depe :punct\n"
          ]
        }
      ],
      "source": [
        "# Sample text for POS tagging\n",
        "data = 'The food was amazing, and the service was impeccable.'\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(data)\n",
        "\n",
        "# Iterate over the tokens and print their text and POS tags\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, POS: {token.pos_}, POS Tag: {token.tag_}, POS Depe :{token.dep_}\") # token.dep_ : syntactic dependency relation between tokens in a sentence.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#The noun chunks are contained in the doc.noun_chunks class variable\n",
        "for noun_chunk in doc.noun_chunks:\n",
        "    print(noun_chunk.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4t3I2Y_0wDsZ",
        "outputId": "dc7f6447-fd1e-40e7-f670-07e67163f365"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The food\n",
            "the service\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "    if token.pos_ == \"VERB\":\n",
        "        print(token.text, token.tag_, token.dep_, token.head.text)"
      ],
      "metadata": {
        "id": "9aobzGLFywNv"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Exercise:\n",
        "Sentiment analysis for the above given customer review using TextBlob.\n",
        "Note: TextBlob is a Python library that provides simple and intuitive tools for processing textual data, performing various natural language processing (NLP) tasks, and even basic sentiment analysis. It's built on top of NLTK (Natural Language Toolkit) and Pattern, combining their capabilities into a user-friendly API."
      ],
      "metadata": {
        "id": "L2UpRB0oz6S7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install textblob"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyhbXCK5z-yE",
        "outputId": "33e12aaf-f043-466c-c16b-73868bac22f8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.10/dist-packages (from textblob) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.1->textblob) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob, Word, Blobber\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from textblob.taggers import NLTKTagger"
      ],
      "metadata": {
        "id": "fShM_z0m1Fbn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = 'The food was amazing, and the service was impeccable.'\n",
        "res = TextBlob(data)\n",
        "print(res.sentiment.polarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Thu7Dnwz0v_C",
        "outputId": "c97cc4d8-c046-4416-fd5c-89777206c29a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IHAhnKE1Nuw",
        "outputId": "16d6eb5d-5b4f-4022-9e0f-9e9d14948340"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m975.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from vaderSentiment) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->vaderSentiment) (2023.7.22)\n",
            "Installing collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "sid_obj= SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "LFHkfBhI1RUZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample Score of Statments\n",
        "print(sid_obj.polarity_scores(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVDu_bNy1VFy",
        "outputId": "92ff55da-dfa2-40de-cfa5-d81e0302abf3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'neg': 0.0, 'neu': 0.678, 'pos': 0.322, 'compound': 0.5859}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = TextBlob(data)\n",
        "a.sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jYK_d7n2LnP",
        "outputId": "cd8dc460-c04f-4e3a-a105-88bdafceca30"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sentiment(polarity=0.675, subjectivity=0.825)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob Features"
      ],
      "metadata": {
        "id": "ociFcBxZ2m9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading the stopwords from nltk\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('brown')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xET467O3FuW",
        "outputId": "24205d6f-a6cf-4d55-de2f-85b6d8d84f0c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenization\n",
        "blob = TextBlob(data)\n",
        "print(blob.words); print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TspHIhbj2sEA",
        "outputId": "d22688e0-44a3-42bf-c8d5-1fc65228c449"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'food', 'was', 'amazing', 'and', 'the', 'service', 'was', 'impeccable']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Noun Phrases\n",
        "print(blob.noun_phrases);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPJYBudq3nH4",
        "outputId": "6cf0a0b3-15d0-4628-a2bd-3d1e88536e32"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Spelling Correction\n",
        "print(blob.correct())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOaBhUVN4IiP",
        "outputId": "82edd96a-55f1-4f7b-a512-4df8b50382c4"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The food was amazing, and the service was implacable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# individual words giving different corrections and providing percentages of correctness\n",
        "from textblob import Word\n",
        "print(Word('larnin').spellcheck());\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkPGze2R4OW7",
        "outputId": "3cd8ecc8-24a1-4e1b-dd78-dd216e335335"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('learning', 0.25384615384615383), ('warning', 0.24615384615384617), ('margin', 0.2153846153846154), ('latin', 0.13076923076923078), ('martin', 0.05384615384615385), ('lain', 0.046153846153846156), ('earning', 0.038461538461538464), ('marin', 0.007692307692307693), ('darwin', 0.007692307692307693)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting the given instances\n",
        "print(blob.word_counts['i']);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsUVA9hL4S9h",
        "outputId": "c471d122-147a-408e-b8b0-a85fd0b7aedd"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment Analysis\n",
        "print(blob.sentiment);"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-plXyFuX4jXn",
        "outputId": "f18deaee-35cd-4dbb-9fb0-ff6461f670eb"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.675, subjectivity=0.825)\n"
          ]
        }
      ]
    }
  ]
}