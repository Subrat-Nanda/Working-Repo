{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Natural Language Processing (PDS3102)\n",
        "\n",
        "Submitted By : Subrat Ku Nanda"
      ],
      "metadata": {
        "id": "_cysfKLjf_Uk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment 2\n",
        "1. Create a unigram representation of a sentence. (Using NLTK and basic string\n",
        "manipulation and collections.Counter)\n",
        "2. Create a bigram representation of a sentence. (Using NLTK and basic string\n",
        "manipulation and collections.Counter)\n",
        "3. Create a skip gram representation of a sentence. (Using NLTK and basic\n",
        "string manipulation and collections.Counter)\n",
        "4. Find Out-of-Vocabulary Words for the sentence “An example sentence with\n",
        "unseen words” where \"This is a sample sentence.\", \"Another sentence for\n",
        "demonstration.\", \"Creating a Bag-of-Words matrix.\" are in a given corpus.\n",
        "5. Calculate tf, idf, df and tf-idf for a given text corpus."
      ],
      "metadata": {
        "id": "HL17x8ZKg-TJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####1 - Create a unigram representation of a sentence. (Using NLTK and basic string manipulation and collections.Counter)\n"
      ],
      "metadata": {
        "id": "dv8JtR61hC_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "from nltk.util import bigrams\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r76mxKAlhJNb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Downloading the stopwords from nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TzedD8-si0ow",
        "outputId": "e9815813-8145-4697-da64-38f688c43b8c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#open text file in read mode\n",
        "text_file = open(\"/content/sample_data/nlp_text.txt\", \"r\")\n",
        "\n",
        "#read whole file to a string\n",
        "data = text_file.read()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "14izni7XhagV",
        "outputId": "1cd4448c-9def-4f00-bd95-3c829d0a698e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'# An Introduction to Natural Language Processing (NLP)\\n\\nNatural Language Processing (NLP) is a field of study that focuses on the interaction between computers and humans using natural language. It involves developing algorithms and computational models that can analyze, understand, and generate human language.\\n\\nNLP has become increasingly important in recent years due to the vast amounts of unstructured data that are generated every day through social media, emails, and other forms of communication. By using NLP techniques, it is possible to extract valuable insights from this data, which can be used for a wide range of applications including sentiment analysis, chatbots, and machine translation.\\n\\nOne of the key challenges in NLP is the ambiguity of human language. Words can have multiple meanings depending on the context in which they are used, and grammar rules can be broken without affecting the meaning of a sentence. To address these challenges, NLP researchers have developed a range of techniques including statistical models, rule-based systems, and deep learning algorithms.\\n\\nOne popular application of NLP is sentiment analysis, which involves analyzing text to determine the emotional tone of the writer. This can be useful for businesses who want to understand how their customers feel about their products or services. Another application is chatbots, which use NLP to understand and respond to user queries in a natural way.\\n\\nDespite its many benefits, NLP also raises ethical concerns around issues such as privacy and bias. For example, if NLP is used to analyze social media data, there is a risk that personal information could be exposed. Additionally, if the algorithms used in NLP are biased, they could perpetuate discrimination against certain groups of people.\\n\\nIn conclusion, NLP is a rapidly growing field with many exciting applications. As we continue to generate vast amounts of unstructured data, NLP techniques will become increasingly important for extracting valuable insights from this data. However, it is important to be aware of the ethical implications of using NLP and to ensure that these technologies are developed in a responsible and equitable way.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenization\n",
        "tokens = word_tokenize(data)\n",
        "\n",
        "# Calculate the frequency distribution of words\n",
        "freq_dist = FreqDist(tokens)\n",
        "\n",
        "# Unigram representation (dictionary with word frequencies)\n",
        "unigram_representation = dict(freq_dist)\n",
        "\n",
        "print(unigram_representation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdFvmY74hVBH",
        "outputId": "521dc43b-f362-44d5-b0df-33b1d1baae23"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'#': 1, 'An': 1, 'Introduction': 1, 'to': 11, 'Natural': 2, 'Language': 2, 'Processing': 2, '(': 2, 'NLP': 14, ')': 2, 'is': 9, 'a': 8, 'field': 2, 'of': 14, 'study': 1, 'that': 5, 'focuses': 1, 'on': 2, 'the': 10, 'interaction': 1, 'between': 1, 'computers': 1, 'and': 11, 'humans': 1, 'using': 3, 'natural': 2, 'language': 3, '.': 16, 'It': 1, 'involves': 2, 'developing': 1, 'algorithms': 3, 'computational': 1, 'models': 2, 'can': 5, 'analyze': 2, ',': 22, 'understand': 3, 'generate': 2, 'human': 2, 'has': 1, 'become': 2, 'increasingly': 2, 'important': 3, 'in': 6, 'recent': 1, 'years': 1, 'due': 1, 'vast': 2, 'amounts': 2, 'unstructured': 2, 'data': 5, 'are': 4, 'generated': 1, 'every': 1, 'day': 1, 'through': 1, 'social': 2, 'media': 2, 'emails': 1, 'other': 1, 'forms': 1, 'communication': 1, 'By': 1, 'techniques': 3, 'it': 2, 'possible': 1, 'extract': 1, 'valuable': 2, 'insights': 2, 'from': 2, 'this': 2, 'which': 4, 'be': 5, 'used': 4, 'for': 3, 'wide': 1, 'range': 2, 'applications': 2, 'including': 2, 'sentiment': 2, 'analysis': 2, 'chatbots': 2, 'machine': 1, 'translation': 1, 'One': 2, 'key': 1, 'challenges': 2, 'ambiguity': 1, 'Words': 1, 'have': 2, 'multiple': 1, 'meanings': 1, 'depending': 1, 'context': 1, 'they': 2, 'grammar': 1, 'rules': 1, 'broken': 1, 'without': 1, 'affecting': 1, 'meaning': 1, 'sentence': 1, 'To': 1, 'address': 1, 'these': 2, 'researchers': 1, 'developed': 2, 'statistical': 1, 'rule-based': 1, 'systems': 1, 'deep': 1, 'learning': 1, 'popular': 1, 'application': 2, 'analyzing': 1, 'text': 1, 'determine': 1, 'emotional': 1, 'tone': 1, 'writer': 1, 'This': 1, 'useful': 1, 'businesses': 1, 'who': 1, 'want': 1, 'how': 1, 'their': 2, 'customers': 1, 'feel': 1, 'about': 1, 'products': 1, 'or': 1, 'services': 1, 'Another': 1, 'use': 1, 'respond': 1, 'user': 1, 'queries': 1, 'way': 2, 'Despite': 1, 'its': 1, 'many': 2, 'benefits': 1, 'also': 1, 'raises': 1, 'ethical': 2, 'concerns': 1, 'around': 1, 'issues': 1, 'such': 1, 'as': 1, 'privacy': 1, 'bias': 1, 'For': 1, 'example': 1, 'if': 2, 'there': 1, 'risk': 1, 'personal': 1, 'information': 1, 'could': 2, 'exposed': 1, 'Additionally': 1, 'biased': 1, 'perpetuate': 1, 'discrimination': 1, 'against': 1, 'certain': 1, 'groups': 1, 'people': 1, 'In': 1, 'conclusion': 1, 'rapidly': 1, 'growing': 1, 'with': 1, 'exciting': 1, 'As': 1, 'we': 1, 'continue': 1, 'will': 1, 'extracting': 1, 'However': 1, 'aware': 1, 'implications': 1, 'ensure': 1, 'technologies': 1, 'responsible': 1, 'equitable': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unigram Using basic string manipulation and collections.Counter"
      ],
      "metadata": {
        "id": "LsP_3d2Aidvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Tokenize the sentence\n",
        "words = re.findall(r'\\w+', data.lower())  # Convert to lowercase and extract words\n",
        "\n",
        "# Calculate the word frequencies\n",
        "word_counts = Counter(words)\n",
        "\n",
        "# Unigram representation (dictionary with word frequencies)\n",
        "unigram_representation = dict(word_counts)\n",
        "\n",
        "print(unigram_representation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0NLC1_Lie7s",
        "outputId": "dbd92e12-210d-4b7b-a8e4-329830a1ff6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'an': 1, 'introduction': 1, 'to': 12, 'natural': 4, 'language': 5, 'processing': 2, 'nlp': 14, 'is': 9, 'a': 8, 'field': 2, 'of': 14, 'study': 1, 'that': 5, 'focuses': 1, 'on': 2, 'the': 10, 'interaction': 1, 'between': 1, 'computers': 1, 'and': 11, 'humans': 1, 'using': 3, 'it': 3, 'involves': 2, 'developing': 1, 'algorithms': 3, 'computational': 1, 'models': 2, 'can': 5, 'analyze': 2, 'understand': 3, 'generate': 2, 'human': 2, 'has': 1, 'become': 2, 'increasingly': 2, 'important': 3, 'in': 7, 'recent': 1, 'years': 1, 'due': 1, 'vast': 2, 'amounts': 2, 'unstructured': 2, 'data': 5, 'are': 4, 'generated': 1, 'every': 1, 'day': 1, 'through': 1, 'social': 2, 'media': 2, 'emails': 1, 'other': 1, 'forms': 1, 'communication': 1, 'by': 1, 'techniques': 3, 'possible': 1, 'extract': 1, 'valuable': 2, 'insights': 2, 'from': 2, 'this': 3, 'which': 4, 'be': 5, 'used': 4, 'for': 4, 'wide': 1, 'range': 2, 'applications': 2, 'including': 2, 'sentiment': 2, 'analysis': 2, 'chatbots': 2, 'machine': 1, 'translation': 1, 'one': 2, 'key': 1, 'challenges': 2, 'ambiguity': 1, 'words': 1, 'have': 2, 'multiple': 1, 'meanings': 1, 'depending': 1, 'context': 1, 'they': 2, 'grammar': 1, 'rules': 1, 'broken': 1, 'without': 1, 'affecting': 1, 'meaning': 1, 'sentence': 1, 'address': 1, 'these': 2, 'researchers': 1, 'developed': 2, 'statistical': 1, 'rule': 1, 'based': 1, 'systems': 1, 'deep': 1, 'learning': 1, 'popular': 1, 'application': 2, 'analyzing': 1, 'text': 1, 'determine': 1, 'emotional': 1, 'tone': 1, 'writer': 1, 'useful': 1, 'businesses': 1, 'who': 1, 'want': 1, 'how': 1, 'their': 2, 'customers': 1, 'feel': 1, 'about': 1, 'products': 1, 'or': 1, 'services': 1, 'another': 1, 'use': 1, 'respond': 1, 'user': 1, 'queries': 1, 'way': 2, 'despite': 1, 'its': 1, 'many': 2, 'benefits': 1, 'also': 1, 'raises': 1, 'ethical': 2, 'concerns': 1, 'around': 1, 'issues': 1, 'such': 1, 'as': 2, 'privacy': 1, 'bias': 1, 'example': 1, 'if': 2, 'there': 1, 'risk': 1, 'personal': 1, 'information': 1, 'could': 2, 'exposed': 1, 'additionally': 1, 'biased': 1, 'perpetuate': 1, 'discrimination': 1, 'against': 1, 'certain': 1, 'groups': 1, 'people': 1, 'conclusion': 1, 'rapidly': 1, 'growing': 1, 'with': 1, 'exciting': 1, 'we': 1, 'continue': 1, 'will': 1, 'extracting': 1, 'however': 1, 'aware': 1, 'implications': 1, 'ensure': 1, 'technologies': 1, 'responsible': 1, 'equitable': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2 - Create a bigram representation of a sentence. (Using NLTK and basic string manipulation and collections.Counter)"
      ],
      "metadata": {
        "id": "LyeicOdRiTam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate bigrams\n",
        "bi_grams = list(bigrams(tokens))\n",
        "\n",
        "# Calculate the frequency distribution of bigrams\n",
        "freq_dist = FreqDist(bi_grams)\n",
        "\n",
        "# Bigram representation (dictionary with bigram frequencies)\n",
        "bigram_representation = dict(freq_dist)\n",
        "\n",
        "print(bigram_representation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U85ihC8niUno",
        "outputId": "c29c04f9-a6c5-4e18-bda0-1b5edbc1b529"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('#', 'An'): 1, ('An', 'Introduction'): 1, ('Introduction', 'to'): 1, ('to', 'Natural'): 1, ('Natural', 'Language'): 2, ('Language', 'Processing'): 2, ('Processing', '('): 2, ('(', 'NLP'): 2, ('NLP', ')'): 2, (')', 'Natural'): 1, (')', 'is'): 1, ('is', 'a'): 3, ('a', 'field'): 1, ('field', 'of'): 1, ('of', 'study'): 1, ('study', 'that'): 1, ('that', 'focuses'): 1, ('focuses', 'on'): 1, ('on', 'the'): 2, ('the', 'interaction'): 1, ('interaction', 'between'): 1, ('between', 'computers'): 1, ('computers', 'and'): 1, ('and', 'humans'): 1, ('humans', 'using'): 1, ('using', 'natural'): 1, ('natural', 'language'): 1, ('language', '.'): 3, ('.', 'It'): 1, ('It', 'involves'): 1, ('involves', 'developing'): 1, ('developing', 'algorithms'): 1, ('algorithms', 'and'): 1, ('and', 'computational'): 1, ('computational', 'models'): 1, ('models', 'that'): 1, ('that', 'can'): 1, ('can', 'analyze'): 1, ('analyze', ','): 1, (',', 'understand'): 1, ('understand', ','): 1, (',', 'and'): 5, ('and', 'generate'): 1, ('generate', 'human'): 1, ('human', 'language'): 2, ('.', 'NLP'): 1, ('NLP', 'has'): 1, ('has', 'become'): 1, ('become', 'increasingly'): 2, ('increasingly', 'important'): 2, ('important', 'in'): 1, ('in', 'recent'): 1, ('recent', 'years'): 1, ('years', 'due'): 1, ('due', 'to'): 1, ('to', 'the'): 1, ('the', 'vast'): 1, ('vast', 'amounts'): 2, ('amounts', 'of'): 2, ('of', 'unstructured'): 2, ('unstructured', 'data'): 2, ('data', 'that'): 1, ('that', 'are'): 1, ('are', 'generated'): 1, ('generated', 'every'): 1, ('every', 'day'): 1, ('day', 'through'): 1, ('through', 'social'): 1, ('social', 'media'): 2, ('media', ','): 1, (',', 'emails'): 1, ('emails', ','): 1, ('and', 'other'): 1, ('other', 'forms'): 1, ('forms', 'of'): 1, ('of', 'communication'): 1, ('communication', '.'): 1, ('.', 'By'): 1, ('By', 'using'): 1, ('using', 'NLP'): 2, ('NLP', 'techniques'): 2, ('techniques', ','): 1, (',', 'it'): 2, ('it', 'is'): 2, ('is', 'possible'): 1, ('possible', 'to'): 1, ('to', 'extract'): 1, ('extract', 'valuable'): 1, ('valuable', 'insights'): 2, ('insights', 'from'): 2, ('from', 'this'): 2, ('this', 'data'): 2, ('data', ','): 3, (',', 'which'): 3, ('which', 'can'): 1, ('can', 'be'): 3, ('be', 'used'): 1, ('used', 'for'): 1, ('for', 'a'): 1, ('a', 'wide'): 1, ('wide', 'range'): 1, ('range', 'of'): 2, ('of', 'applications'): 1, ('applications', 'including'): 1, ('including', 'sentiment'): 1, ('sentiment', 'analysis'): 2, ('analysis', ','): 2, (',', 'chatbots'): 1, ('chatbots', ','): 2, ('and', 'machine'): 1, ('machine', 'translation'): 1, ('translation', '.'): 1, ('.', 'One'): 2, ('One', 'of'): 1, ('of', 'the'): 3, ('the', 'key'): 1, ('key', 'challenges'): 1, ('challenges', 'in'): 1, ('in', 'NLP'): 2, ('NLP', 'is'): 4, ('is', 'the'): 1, ('the', 'ambiguity'): 1, ('ambiguity', 'of'): 1, ('of', 'human'): 1, ('.', 'Words'): 1, ('Words', 'can'): 1, ('can', 'have'): 1, ('have', 'multiple'): 1, ('multiple', 'meanings'): 1, ('meanings', 'depending'): 1, ('depending', 'on'): 1, ('the', 'context'): 1, ('context', 'in'): 1, ('in', 'which'): 1, ('which', 'they'): 1, ('they', 'are'): 1, ('are', 'used'): 1, ('used', ','): 1, ('and', 'grammar'): 1, ('grammar', 'rules'): 1, ('rules', 'can'): 1, ('be', 'broken'): 1, ('broken', 'without'): 1, ('without', 'affecting'): 1, ('affecting', 'the'): 1, ('the', 'meaning'): 1, ('meaning', 'of'): 1, ('of', 'a'): 1, ('a', 'sentence'): 1, ('sentence', '.'): 1, ('.', 'To'): 1, ('To', 'address'): 1, ('address', 'these'): 1, ('these', 'challenges'): 1, ('challenges', ','): 1, (',', 'NLP'): 4, ('NLP', 'researchers'): 1, ('researchers', 'have'): 1, ('have', 'developed'): 1, ('developed', 'a'): 1, ('a', 'range'): 1, ('of', 'techniques'): 1, ('techniques', 'including'): 1, ('including', 'statistical'): 1, ('statistical', 'models'): 1, ('models', ','): 1, (',', 'rule-based'): 1, ('rule-based', 'systems'): 1, ('systems', ','): 1, ('and', 'deep'): 1, ('deep', 'learning'): 1, ('learning', 'algorithms'): 1, ('algorithms', '.'): 1, ('One', 'popular'): 1, ('popular', 'application'): 1, ('application', 'of'): 1, ('of', 'NLP'): 1, ('is', 'sentiment'): 1, ('which', 'involves'): 1, ('involves', 'analyzing'): 1, ('analyzing', 'text'): 1, ('text', 'to'): 1, ('to', 'determine'): 1, ('determine', 'the'): 1, ('the', 'emotional'): 1, ('emotional', 'tone'): 1, ('tone', 'of'): 1, ('the', 'writer'): 1, ('writer', '.'): 1, ('.', 'This'): 1, ('This', 'can'): 1, ('be', 'useful'): 1, ('useful', 'for'): 1, ('for', 'businesses'): 1, ('businesses', 'who'): 1, ('who', 'want'): 1, ('want', 'to'): 1, ('to', 'understand'): 2, ('understand', 'how'): 1, ('how', 'their'): 1, ('their', 'customers'): 1, ('customers', 'feel'): 1, ('feel', 'about'): 1, ('about', 'their'): 1, ('their', 'products'): 1, ('products', 'or'): 1, ('or', 'services'): 1, ('services', '.'): 1, ('.', 'Another'): 1, ('Another', 'application'): 1, ('application', 'is'): 1, ('is', 'chatbots'): 1, ('which', 'use'): 1, ('use', 'NLP'): 1, ('NLP', 'to'): 1, ('understand', 'and'): 1, ('and', 'respond'): 1, ('respond', 'to'): 1, ('to', 'user'): 1, ('user', 'queries'): 1, ('queries', 'in'): 1, ('in', 'a'): 2, ('a', 'natural'): 1, ('natural', 'way'): 1, ('way', '.'): 2, ('.', 'Despite'): 1, ('Despite', 'its'): 1, ('its', 'many'): 1, ('many', 'benefits'): 1, ('benefits', ','): 1, ('NLP', 'also'): 1, ('also', 'raises'): 1, ('raises', 'ethical'): 1, ('ethical', 'concerns'): 1, ('concerns', 'around'): 1, ('around', 'issues'): 1, ('issues', 'such'): 1, ('such', 'as'): 1, ('as', 'privacy'): 1, ('privacy', 'and'): 1, ('and', 'bias'): 1, ('bias', '.'): 1, ('.', 'For'): 1, ('For', 'example'): 1, ('example', ','): 1, (',', 'if'): 2, ('if', 'NLP'): 1, ('is', 'used'): 1, ('used', 'to'): 1, ('to', 'analyze'): 1, ('analyze', 'social'): 1, ('media', 'data'): 1, (',', 'there'): 1, ('there', 'is'): 1, ('a', 'risk'): 1, ('risk', 'that'): 1, ('that', 'personal'): 1, ('personal', 'information'): 1, ('information', 'could'): 1, ('could', 'be'): 1, ('be', 'exposed'): 1, ('exposed', '.'): 1, ('.', 'Additionally'): 1, ('Additionally', ','): 1, ('if', 'the'): 1, ('the', 'algorithms'): 1, ('algorithms', 'used'): 1, ('used', 'in'): 1, ('NLP', 'are'): 1, ('are', 'biased'): 1, ('biased', ','): 1, (',', 'they'): 1, ('they', 'could'): 1, ('could', 'perpetuate'): 1, ('perpetuate', 'discrimination'): 1, ('discrimination', 'against'): 1, ('against', 'certain'): 1, ('certain', 'groups'): 1, ('groups', 'of'): 1, ('of', 'people'): 1, ('people', '.'): 1, ('.', 'In'): 1, ('In', 'conclusion'): 1, ('conclusion', ','): 1, ('a', 'rapidly'): 1, ('rapidly', 'growing'): 1, ('growing', 'field'): 1, ('field', 'with'): 1, ('with', 'many'): 1, ('many', 'exciting'): 1, ('exciting', 'applications'): 1, ('applications', '.'): 1, ('.', 'As'): 1, ('As', 'we'): 1, ('we', 'continue'): 1, ('continue', 'to'): 1, ('to', 'generate'): 1, ('generate', 'vast'): 1, ('techniques', 'will'): 1, ('will', 'become'): 1, ('important', 'for'): 1, ('for', 'extracting'): 1, ('extracting', 'valuable'): 1, ('data', '.'): 1, ('.', 'However'): 1, ('However', ','): 1, ('is', 'important'): 1, ('important', 'to'): 1, ('to', 'be'): 1, ('be', 'aware'): 1, ('aware', 'of'): 1, ('the', 'ethical'): 1, ('ethical', 'implications'): 1, ('implications', 'of'): 1, ('of', 'using'): 1, ('NLP', 'and'): 1, ('and', 'to'): 1, ('to', 'ensure'): 1, ('ensure', 'that'): 1, ('that', 'these'): 1, ('these', 'technologies'): 1, ('technologies', 'are'): 1, ('are', 'developed'): 1, ('developed', 'in'): 1, ('a', 'responsible'): 1, ('responsible', 'and'): 1, ('and', 'equitable'): 1, ('equitable', 'way'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bigram Using basic string manipulation and collections.Counter"
      ],
      "metadata": {
        "id": "hNz4ZPm1jjOB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate bigrams\n",
        "bi_grams = [(words[i], words[i+1]) for i in range(len(words) - 1)]\n",
        "\n",
        "# Calculate the bigram frequencies\n",
        "bigram_counts = Counter(bi_grams)\n",
        "\n",
        "# Bigram representation (dictionary with bigram frequencies)\n",
        "bigram_representation = dict(bigram_counts)\n",
        "\n",
        "print(bigram_representation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBuSdHqBjk1f",
        "outputId": "f5766b96-f08a-421a-f8b5-80b5b354cfd5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('an', 'introduction'): 1, ('introduction', 'to'): 1, ('to', 'natural'): 1, ('natural', 'language'): 3, ('language', 'processing'): 2, ('processing', 'nlp'): 2, ('nlp', 'natural'): 1, ('nlp', 'is'): 5, ('is', 'a'): 3, ('a', 'field'): 1, ('field', 'of'): 1, ('of', 'study'): 1, ('study', 'that'): 1, ('that', 'focuses'): 1, ('focuses', 'on'): 1, ('on', 'the'): 2, ('the', 'interaction'): 1, ('interaction', 'between'): 1, ('between', 'computers'): 1, ('computers', 'and'): 1, ('and', 'humans'): 1, ('humans', 'using'): 1, ('using', 'natural'): 1, ('language', 'it'): 1, ('it', 'involves'): 1, ('involves', 'developing'): 1, ('developing', 'algorithms'): 1, ('algorithms', 'and'): 1, ('and', 'computational'): 1, ('computational', 'models'): 1, ('models', 'that'): 1, ('that', 'can'): 1, ('can', 'analyze'): 1, ('analyze', 'understand'): 1, ('understand', 'and'): 2, ('and', 'generate'): 1, ('generate', 'human'): 1, ('human', 'language'): 2, ('language', 'nlp'): 1, ('nlp', 'has'): 1, ('has', 'become'): 1, ('become', 'increasingly'): 2, ('increasingly', 'important'): 2, ('important', 'in'): 1, ('in', 'recent'): 1, ('recent', 'years'): 1, ('years', 'due'): 1, ('due', 'to'): 1, ('to', 'the'): 1, ('the', 'vast'): 1, ('vast', 'amounts'): 2, ('amounts', 'of'): 2, ('of', 'unstructured'): 2, ('unstructured', 'data'): 2, ('data', 'that'): 1, ('that', 'are'): 1, ('are', 'generated'): 1, ('generated', 'every'): 1, ('every', 'day'): 1, ('day', 'through'): 1, ('through', 'social'): 1, ('social', 'media'): 2, ('media', 'emails'): 1, ('emails', 'and'): 1, ('and', 'other'): 1, ('other', 'forms'): 1, ('forms', 'of'): 1, ('of', 'communication'): 1, ('communication', 'by'): 1, ('by', 'using'): 1, ('using', 'nlp'): 2, ('nlp', 'techniques'): 2, ('techniques', 'it'): 1, ('it', 'is'): 2, ('is', 'possible'): 1, ('possible', 'to'): 1, ('to', 'extract'): 1, ('extract', 'valuable'): 1, ('valuable', 'insights'): 2, ('insights', 'from'): 2, ('from', 'this'): 2, ('this', 'data'): 2, ('data', 'which'): 1, ('which', 'can'): 1, ('can', 'be'): 3, ('be', 'used'): 1, ('used', 'for'): 1, ('for', 'a'): 1, ('a', 'wide'): 1, ('wide', 'range'): 1, ('range', 'of'): 2, ('of', 'applications'): 1, ('applications', 'including'): 1, ('including', 'sentiment'): 1, ('sentiment', 'analysis'): 2, ('analysis', 'chatbots'): 1, ('chatbots', 'and'): 1, ('and', 'machine'): 1, ('machine', 'translation'): 1, ('translation', 'one'): 1, ('one', 'of'): 1, ('of', 'the'): 3, ('the', 'key'): 1, ('key', 'challenges'): 1, ('challenges', 'in'): 1, ('in', 'nlp'): 2, ('is', 'the'): 1, ('the', 'ambiguity'): 1, ('ambiguity', 'of'): 1, ('of', 'human'): 1, ('language', 'words'): 1, ('words', 'can'): 1, ('can', 'have'): 1, ('have', 'multiple'): 1, ('multiple', 'meanings'): 1, ('meanings', 'depending'): 1, ('depending', 'on'): 1, ('the', 'context'): 1, ('context', 'in'): 1, ('in', 'which'): 1, ('which', 'they'): 1, ('they', 'are'): 1, ('are', 'used'): 1, ('used', 'and'): 1, ('and', 'grammar'): 1, ('grammar', 'rules'): 1, ('rules', 'can'): 1, ('be', 'broken'): 1, ('broken', 'without'): 1, ('without', 'affecting'): 1, ('affecting', 'the'): 1, ('the', 'meaning'): 1, ('meaning', 'of'): 1, ('of', 'a'): 1, ('a', 'sentence'): 1, ('sentence', 'to'): 1, ('to', 'address'): 1, ('address', 'these'): 1, ('these', 'challenges'): 1, ('challenges', 'nlp'): 1, ('nlp', 'researchers'): 1, ('researchers', 'have'): 1, ('have', 'developed'): 1, ('developed', 'a'): 1, ('a', 'range'): 1, ('of', 'techniques'): 1, ('techniques', 'including'): 1, ('including', 'statistical'): 1, ('statistical', 'models'): 1, ('models', 'rule'): 1, ('rule', 'based'): 1, ('based', 'systems'): 1, ('systems', 'and'): 1, ('and', 'deep'): 1, ('deep', 'learning'): 1, ('learning', 'algorithms'): 1, ('algorithms', 'one'): 1, ('one', 'popular'): 1, ('popular', 'application'): 1, ('application', 'of'): 1, ('of', 'nlp'): 1, ('is', 'sentiment'): 1, ('analysis', 'which'): 1, ('which', 'involves'): 1, ('involves', 'analyzing'): 1, ('analyzing', 'text'): 1, ('text', 'to'): 1, ('to', 'determine'): 1, ('determine', 'the'): 1, ('the', 'emotional'): 1, ('emotional', 'tone'): 1, ('tone', 'of'): 1, ('the', 'writer'): 1, ('writer', 'this'): 1, ('this', 'can'): 1, ('be', 'useful'): 1, ('useful', 'for'): 1, ('for', 'businesses'): 1, ('businesses', 'who'): 1, ('who', 'want'): 1, ('want', 'to'): 1, ('to', 'understand'): 2, ('understand', 'how'): 1, ('how', 'their'): 1, ('their', 'customers'): 1, ('customers', 'feel'): 1, ('feel', 'about'): 1, ('about', 'their'): 1, ('their', 'products'): 1, ('products', 'or'): 1, ('or', 'services'): 1, ('services', 'another'): 1, ('another', 'application'): 1, ('application', 'is'): 1, ('is', 'chatbots'): 1, ('chatbots', 'which'): 1, ('which', 'use'): 1, ('use', 'nlp'): 1, ('nlp', 'to'): 1, ('and', 'respond'): 1, ('respond', 'to'): 1, ('to', 'user'): 1, ('user', 'queries'): 1, ('queries', 'in'): 1, ('in', 'a'): 2, ('a', 'natural'): 1, ('natural', 'way'): 1, ('way', 'despite'): 1, ('despite', 'its'): 1, ('its', 'many'): 1, ('many', 'benefits'): 1, ('benefits', 'nlp'): 1, ('nlp', 'also'): 1, ('also', 'raises'): 1, ('raises', 'ethical'): 1, ('ethical', 'concerns'): 1, ('concerns', 'around'): 1, ('around', 'issues'): 1, ('issues', 'such'): 1, ('such', 'as'): 1, ('as', 'privacy'): 1, ('privacy', 'and'): 1, ('and', 'bias'): 1, ('bias', 'for'): 1, ('for', 'example'): 1, ('example', 'if'): 1, ('if', 'nlp'): 1, ('is', 'used'): 1, ('used', 'to'): 1, ('to', 'analyze'): 1, ('analyze', 'social'): 1, ('media', 'data'): 1, ('data', 'there'): 1, ('there', 'is'): 1, ('a', 'risk'): 1, ('risk', 'that'): 1, ('that', 'personal'): 1, ('personal', 'information'): 1, ('information', 'could'): 1, ('could', 'be'): 1, ('be', 'exposed'): 1, ('exposed', 'additionally'): 1, ('additionally', 'if'): 1, ('if', 'the'): 1, ('the', 'algorithms'): 1, ('algorithms', 'used'): 1, ('used', 'in'): 1, ('nlp', 'are'): 1, ('are', 'biased'): 1, ('biased', 'they'): 1, ('they', 'could'): 1, ('could', 'perpetuate'): 1, ('perpetuate', 'discrimination'): 1, ('discrimination', 'against'): 1, ('against', 'certain'): 1, ('certain', 'groups'): 1, ('groups', 'of'): 1, ('of', 'people'): 1, ('people', 'in'): 1, ('in', 'conclusion'): 1, ('conclusion', 'nlp'): 1, ('a', 'rapidly'): 1, ('rapidly', 'growing'): 1, ('growing', 'field'): 1, ('field', 'with'): 1, ('with', 'many'): 1, ('many', 'exciting'): 1, ('exciting', 'applications'): 1, ('applications', 'as'): 1, ('as', 'we'): 1, ('we', 'continue'): 1, ('continue', 'to'): 1, ('to', 'generate'): 1, ('generate', 'vast'): 1, ('data', 'nlp'): 1, ('techniques', 'will'): 1, ('will', 'become'): 1, ('important', 'for'): 1, ('for', 'extracting'): 1, ('extracting', 'valuable'): 1, ('data', 'however'): 1, ('however', 'it'): 1, ('is', 'important'): 1, ('important', 'to'): 1, ('to', 'be'): 1, ('be', 'aware'): 1, ('aware', 'of'): 1, ('the', 'ethical'): 1, ('ethical', 'implications'): 1, ('implications', 'of'): 1, ('of', 'using'): 1, ('nlp', 'and'): 1, ('and', 'to'): 1, ('to', 'ensure'): 1, ('ensure', 'that'): 1, ('that', 'these'): 1, ('these', 'technologies'): 1, ('technologies', 'are'): 1, ('are', 'developed'): 1, ('developed', 'in'): 1, ('a', 'responsible'): 1, ('responsible', 'and'): 1, ('and', 'equitable'): 1, ('equitable', 'way'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3 - Create a skip gram representation of a sentence. (Using NLTK and basic string manipulation and collections.Counter)"
      ],
      "metadata": {
        "id": "9be9hnfjjssu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing libraries\n",
        "import nltk\n",
        "from nltk.util import skipgrams\n",
        "\n",
        "# Define window size and number of skips for skipgrams\n",
        "window_size = 2\n",
        "num_skips = 1\n",
        "\n",
        "# Generate skipgrams\n",
        "skips = list(skipgrams(tokens, window_size, num_skips))\n",
        "\n",
        "# Calculate the frequency distribution of skipgrams\n",
        "freq_dist = FreqDist(skips)\n",
        "\n",
        "# Skipgram representation (dictionary with skipgram frequencies)\n",
        "skipgram_representation = dict(freq_dist)\n",
        "\n",
        "print(skipgram_representation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Dpq7f8sjv4J",
        "outputId": "7d0383fa-14ef-4eb0-cd02-6c0ffd0df2f4"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('#', 'An'): 1, ('#', 'Introduction'): 1, ('An', 'Introduction'): 1, ('An', 'to'): 1, ('Introduction', 'to'): 1, ('Introduction', 'Natural'): 1, ('to', 'Natural'): 1, ('to', 'Language'): 1, ('Natural', 'Language'): 2, ('Natural', 'Processing'): 2, ('Language', 'Processing'): 2, ('Language', '('): 2, ('Processing', '('): 2, ('Processing', 'NLP'): 2, ('(', 'NLP'): 2, ('(', ')'): 2, ('NLP', ')'): 2, ('NLP', 'Natural'): 1, (')', 'Natural'): 1, (')', 'Language'): 1, ('NLP', 'is'): 5, (')', 'is'): 1, (')', 'a'): 1, ('is', 'a'): 3, ('is', 'field'): 1, ('a', 'field'): 1, ('a', 'of'): 2, ('field', 'of'): 1, ('field', 'study'): 1, ('of', 'study'): 1, ('of', 'that'): 1, ('study', 'that'): 1, ('study', 'focuses'): 1, ('that', 'focuses'): 1, ('that', 'on'): 1, ('focuses', 'on'): 1, ('focuses', 'the'): 1, ('on', 'the'): 2, ('on', 'interaction'): 1, ('the', 'interaction'): 1, ('the', 'between'): 1, ('interaction', 'between'): 1, ('interaction', 'computers'): 1, ('between', 'computers'): 1, ('between', 'and'): 1, ('computers', 'and'): 1, ('computers', 'humans'): 1, ('and', 'humans'): 1, ('and', 'using'): 1, ('humans', 'using'): 1, ('humans', 'natural'): 1, ('using', 'natural'): 1, ('using', 'language'): 1, ('natural', 'language'): 1, ('natural', '.'): 2, ('language', '.'): 3, ('language', 'It'): 1, ('.', 'It'): 1, ('.', 'involves'): 1, ('It', 'involves'): 1, ('It', 'developing'): 1, ('involves', 'developing'): 1, ('involves', 'algorithms'): 1, ('developing', 'algorithms'): 1, ('developing', 'and'): 1, ('algorithms', 'and'): 1, ('algorithms', 'computational'): 1, ('and', 'computational'): 1, ('and', 'models'): 1, ('computational', 'models'): 1, ('computational', 'that'): 1, ('models', 'that'): 1, ('models', 'can'): 1, ('that', 'can'): 1, ('that', 'analyze'): 1, ('can', 'analyze'): 1, ('can', ','): 1, ('analyze', ','): 1, ('analyze', 'understand'): 1, (',', 'understand'): 1, (',', ','): 3, ('understand', ','): 1, ('understand', 'and'): 2, (',', 'and'): 5, (',', 'generate'): 1, ('and', 'generate'): 1, ('and', 'human'): 1, ('generate', 'human'): 1, ('generate', 'language'): 1, ('human', 'language'): 2, ('human', '.'): 2, ('language', 'NLP'): 1, ('.', 'NLP'): 1, ('.', 'has'): 1, ('NLP', 'has'): 1, ('NLP', 'become'): 1, ('has', 'become'): 1, ('has', 'increasingly'): 1, ('become', 'increasingly'): 2, ('become', 'important'): 2, ('increasingly', 'important'): 2, ('increasingly', 'in'): 1, ('important', 'in'): 1, ('important', 'recent'): 1, ('in', 'recent'): 1, ('in', 'years'): 1, ('recent', 'years'): 1, ('recent', 'due'): 1, ('years', 'due'): 1, ('years', 'to'): 1, ('due', 'to'): 1, ('due', 'the'): 1, ('to', 'the'): 2, ('to', 'vast'): 2, ('the', 'vast'): 1, ('the', 'amounts'): 1, ('vast', 'amounts'): 2, ('vast', 'of'): 2, ('amounts', 'of'): 2, ('amounts', 'unstructured'): 2, ('of', 'unstructured'): 2, ('of', 'data'): 2, ('unstructured', 'data'): 2, ('unstructured', 'that'): 1, ('data', 'that'): 1, ('data', 'are'): 1, ('that', 'are'): 1, ('that', 'generated'): 1, ('are', 'generated'): 1, ('are', 'every'): 1, ('generated', 'every'): 1, ('generated', 'day'): 1, ('every', 'day'): 1, ('every', 'through'): 1, ('day', 'through'): 1, ('day', 'social'): 1, ('through', 'social'): 1, ('through', 'media'): 1, ('social', 'media'): 2, ('social', ','): 1, ('media', ','): 2, ('media', 'emails'): 1, (',', 'emails'): 1, ('emails', ','): 1, ('emails', 'and'): 1, (',', 'other'): 1, ('and', 'other'): 1, ('and', 'forms'): 1, ('other', 'forms'): 1, ('other', 'of'): 1, ('forms', 'of'): 1, ('forms', 'communication'): 1, ('of', 'communication'): 1, ('of', '.'): 2, ('communication', '.'): 1, ('communication', 'By'): 1, ('.', 'By'): 1, ('.', 'using'): 1, ('By', 'using'): 1, ('By', 'NLP'): 1, ('using', 'NLP'): 2, ('using', 'techniques'): 1, ('NLP', 'techniques'): 2, ('NLP', ','): 1, ('techniques', ','): 1, ('techniques', 'it'): 1, (',', 'it'): 2, (',', 'is'): 4, ('it', 'is'): 2, ('it', 'possible'): 1, ('is', 'possible'): 1, ('is', 'to'): 3, ('possible', 'to'): 1, ('possible', 'extract'): 1, ('to', 'extract'): 1, ('to', 'valuable'): 1, ('extract', 'valuable'): 1, ('extract', 'insights'): 1, ('valuable', 'insights'): 2, ('valuable', 'from'): 2, ('insights', 'from'): 2, ('insights', 'this'): 2, ('from', 'this'): 2, ('from', 'data'): 2, ('this', 'data'): 2, ('this', ','): 1, ('data', ','): 3, ('data', 'which'): 1, (',', 'which'): 3, (',', 'can'): 1, ('which', 'can'): 1, ('which', 'be'): 1, ('can', 'be'): 3, ('can', 'used'): 1, ('be', 'used'): 1, ('be', 'for'): 2, ('used', 'for'): 1, ('used', 'a'): 1, ('for', 'a'): 1, ('for', 'wide'): 1, ('a', 'wide'): 1, ('a', 'range'): 2, ('wide', 'range'): 1, ('wide', 'of'): 1, ('range', 'of'): 2, ('range', 'applications'): 1, ('of', 'applications'): 1, ('of', 'including'): 2, ('applications', 'including'): 1, ('applications', 'sentiment'): 1, ('including', 'sentiment'): 1, ('including', 'analysis'): 1, ('sentiment', 'analysis'): 2, ('sentiment', ','): 2, ('analysis', ','): 2, ('analysis', 'chatbots'): 1, (',', 'chatbots'): 1, ('chatbots', ','): 2, ('chatbots', 'and'): 1, (',', 'machine'): 1, ('and', 'machine'): 1, ('and', 'translation'): 1, ('machine', 'translation'): 1, ('machine', '.'): 1, ('translation', '.'): 1, ('translation', 'One'): 1, ('.', 'One'): 2, ('.', 'of'): 1, ('One', 'of'): 1, ('One', 'the'): 1, ('of', 'the'): 3, ('of', 'key'): 1, ('the', 'key'): 1, ('the', 'challenges'): 1, ('key', 'challenges'): 1, ('key', 'in'): 1, ('challenges', 'in'): 1, ('challenges', 'NLP'): 2, ('in', 'NLP'): 2, ('in', 'is'): 1, ('NLP', 'the'): 1, ('is', 'the'): 1, ('is', 'ambiguity'): 1, ('the', 'ambiguity'): 1, ('the', 'of'): 2, ('ambiguity', 'of'): 1, ('ambiguity', 'human'): 1, ('of', 'human'): 1, ('of', 'language'): 1, ('language', 'Words'): 1, ('.', 'Words'): 1, ('.', 'can'): 2, ('Words', 'can'): 1, ('Words', 'have'): 1, ('can', 'have'): 1, ('can', 'multiple'): 1, ('have', 'multiple'): 1, ('have', 'meanings'): 1, ('multiple', 'meanings'): 1, ('multiple', 'depending'): 1, ('meanings', 'depending'): 1, ('meanings', 'on'): 1, ('depending', 'on'): 1, ('depending', 'the'): 1, ('on', 'context'): 1, ('the', 'context'): 1, ('the', 'in'): 1, ('context', 'in'): 1, ('context', 'which'): 1, ('in', 'which'): 1, ('in', 'they'): 1, ('which', 'they'): 1, ('which', 'are'): 1, ('they', 'are'): 1, ('they', 'used'): 1, ('are', 'used'): 1, ('are', ','): 2, ('used', ','): 1, ('used', 'and'): 1, (',', 'grammar'): 1, ('and', 'grammar'): 1, ('and', 'rules'): 1, ('grammar', 'rules'): 1, ('grammar', 'can'): 1, ('rules', 'can'): 1, ('rules', 'be'): 1, ('can', 'broken'): 1, ('be', 'broken'): 1, ('be', 'without'): 1, ('broken', 'without'): 1, ('broken', 'affecting'): 1, ('without', 'affecting'): 1, ('without', 'the'): 1, ('affecting', 'the'): 1, ('affecting', 'meaning'): 1, ('the', 'meaning'): 1, ('meaning', 'of'): 1, ('meaning', 'a'): 1, ('of', 'a'): 1, ('of', 'sentence'): 1, ('a', 'sentence'): 1, ('a', '.'): 1, ('sentence', '.'): 1, ('sentence', 'To'): 1, ('.', 'To'): 1, ('.', 'address'): 1, ('To', 'address'): 1, ('To', 'these'): 1, ('address', 'these'): 1, ('address', 'challenges'): 1, ('these', 'challenges'): 1, ('these', ','): 1, ('challenges', ','): 1, (',', 'NLP'): 5, (',', 'researchers'): 1, ('NLP', 'researchers'): 1, ('NLP', 'have'): 1, ('researchers', 'have'): 1, ('researchers', 'developed'): 1, ('have', 'developed'): 1, ('have', 'a'): 1, ('developed', 'a'): 2, ('developed', 'range'): 1, ('range', 'techniques'): 1, ('of', 'techniques'): 1, ('techniques', 'including'): 1, ('techniques', 'statistical'): 1, ('including', 'statistical'): 1, ('including', 'models'): 1, ('statistical', 'models'): 1, ('statistical', ','): 1, ('models', ','): 1, ('models', 'rule-based'): 1, (',', 'rule-based'): 1, (',', 'systems'): 1, ('rule-based', 'systems'): 1, ('rule-based', ','): 1, ('systems', ','): 1, ('systems', 'and'): 1, (',', 'deep'): 1, ('and', 'deep'): 1, ('and', 'learning'): 1, ('deep', 'learning'): 1, ('deep', 'algorithms'): 1, ('learning', 'algorithms'): 1, ('learning', '.'): 1, ('algorithms', '.'): 1, ('algorithms', 'One'): 1, ('.', 'popular'): 1, ('One', 'popular'): 1, ('One', 'application'): 1, ('popular', 'application'): 1, ('popular', 'of'): 1, ('application', 'of'): 1, ('application', 'NLP'): 1, ('of', 'NLP'): 2, ('of', 'is'): 1, ('NLP', 'sentiment'): 1, ('is', 'sentiment'): 1, ('is', 'analysis'): 1, ('analysis', 'which'): 1, (',', 'involves'): 1, ('which', 'involves'): 1, ('which', 'analyzing'): 1, ('involves', 'analyzing'): 1, ('involves', 'text'): 1, ('analyzing', 'text'): 1, ('analyzing', 'to'): 1, ('text', 'to'): 1, ('text', 'determine'): 1, ('to', 'determine'): 1, ('determine', 'the'): 1, ('determine', 'emotional'): 1, ('the', 'emotional'): 1, ('the', 'tone'): 1, ('emotional', 'tone'): 1, ('emotional', 'of'): 1, ('tone', 'of'): 1, ('tone', 'the'): 1, ('of', 'writer'): 1, ('the', 'writer'): 1, ('the', '.'): 1, ('writer', '.'): 1, ('writer', 'This'): 1, ('.', 'This'): 1, ('This', 'can'): 1, ('This', 'be'): 1, ('can', 'useful'): 1, ('be', 'useful'): 1, ('useful', 'for'): 1, ('useful', 'businesses'): 1, ('for', 'businesses'): 1, ('for', 'who'): 1, ('businesses', 'who'): 1, ('businesses', 'want'): 1, ('who', 'want'): 1, ('who', 'to'): 1, ('want', 'to'): 1, ('want', 'understand'): 1, ('to', 'understand'): 2, ('to', 'how'): 1, ('understand', 'how'): 1, ('understand', 'their'): 1, ('how', 'their'): 1, ('how', 'customers'): 1, ('their', 'customers'): 1, ('their', 'feel'): 1, ('customers', 'feel'): 1, ('customers', 'about'): 1, ('feel', 'about'): 1, ('feel', 'their'): 1, ('about', 'their'): 1, ('about', 'products'): 1, ('their', 'products'): 1, ('their', 'or'): 1, ('products', 'or'): 1, ('products', 'services'): 1, ('or', 'services'): 1, ('or', '.'): 1, ('services', '.'): 1, ('services', 'Another'): 1, ('.', 'Another'): 1, ('.', 'application'): 1, ('Another', 'application'): 1, ('Another', 'is'): 1, ('application', 'is'): 1, ('application', 'chatbots'): 1, ('is', 'chatbots'): 1, ('is', ','): 1, ('chatbots', 'which'): 1, (',', 'use'): 1, ('which', 'use'): 1, ('which', 'NLP'): 1, ('use', 'NLP'): 1, ('use', 'to'): 1, ('NLP', 'to'): 2, ('NLP', 'understand'): 1, ('to', 'and'): 1, ('understand', 'respond'): 1, ('and', 'respond'): 1, ('and', 'to'): 2, ('respond', 'to'): 1, ('respond', 'user'): 1, ('to', 'user'): 1, ('to', 'queries'): 1, ('user', 'queries'): 1, ('user', 'in'): 1, ('queries', 'in'): 1, ('queries', 'a'): 1, ('in', 'a'): 2, ('in', 'natural'): 1, ('a', 'natural'): 1, ('a', 'way'): 1, ('natural', 'way'): 1, ('way', '.'): 2, ('way', 'Despite'): 1, ('.', 'Despite'): 1, ('.', 'its'): 1, ('Despite', 'its'): 1, ('Despite', 'many'): 1, ('its', 'many'): 1, ('its', 'benefits'): 1, ('many', 'benefits'): 1, ('many', ','): 1, ('benefits', ','): 1, ('benefits', 'NLP'): 1, (',', 'also'): 1, ('NLP', 'also'): 1, ('NLP', 'raises'): 1, ('also', 'raises'): 1, ('also', 'ethical'): 1, ('raises', 'ethical'): 1, ('raises', 'concerns'): 1, ('ethical', 'concerns'): 1, ('ethical', 'around'): 1, ('concerns', 'around'): 1, ('concerns', 'issues'): 1, ('around', 'issues'): 1, ('around', 'such'): 1, ('issues', 'such'): 1, ('issues', 'as'): 1, ('such', 'as'): 1, ('such', 'privacy'): 1, ('as', 'privacy'): 1, ('as', 'and'): 1, ('privacy', 'and'): 1, ('privacy', 'bias'): 1, ('and', 'bias'): 1, ('and', '.'): 1, ('bias', '.'): 1, ('bias', 'For'): 1, ('.', 'For'): 1, ('.', 'example'): 1, ('For', 'example'): 1, ('For', ','): 1, ('example', ','): 1, ('example', 'if'): 1, (',', 'if'): 2, ('if', 'NLP'): 1, ('if', 'is'): 1, ('NLP', 'used'): 1, ('is', 'used'): 1, ('used', 'to'): 1, ('used', 'analyze'): 1, ('to', 'analyze'): 1, ('to', 'social'): 1, ('analyze', 'social'): 1, ('analyze', 'media'): 1, ('social', 'data'): 1, ('media', 'data'): 1, ('data', 'there'): 1, (',', 'there'): 1, ('there', 'is'): 1, ('there', 'a'): 1, ('is', 'risk'): 1, ('a', 'risk'): 1, ('a', 'that'): 1, ('risk', 'that'): 1, ('risk', 'personal'): 1, ('that', 'personal'): 1, ('that', 'information'): 1, ('personal', 'information'): 1, ('personal', 'could'): 1, ('information', 'could'): 1, ('information', 'be'): 1, ('could', 'be'): 1, ('could', 'exposed'): 1, ('be', 'exposed'): 1, ('be', '.'): 1, ('exposed', '.'): 1, ('exposed', 'Additionally'): 1, ('.', 'Additionally'): 1, ('.', ','): 2, ('Additionally', ','): 1, ('Additionally', 'if'): 1, (',', 'the'): 1, ('if', 'the'): 1, ('if', 'algorithms'): 1, ('the', 'algorithms'): 1, ('the', 'used'): 1, ('algorithms', 'used'): 1, ('algorithms', 'in'): 1, ('used', 'in'): 1, ('used', 'NLP'): 1, ('in', 'are'): 1, ('NLP', 'are'): 1, ('NLP', 'biased'): 1, ('are', 'biased'): 1, ('biased', ','): 1, ('biased', 'they'): 1, (',', 'they'): 1, (',', 'could'): 1, ('they', 'could'): 1, ('they', 'perpetuate'): 1, ('could', 'perpetuate'): 1, ('could', 'discrimination'): 1, ('perpetuate', 'discrimination'): 1, ('perpetuate', 'against'): 1, ('discrimination', 'against'): 1, ('discrimination', 'certain'): 1, ('against', 'certain'): 1, ('against', 'groups'): 1, ('certain', 'groups'): 1, ('certain', 'of'): 1, ('groups', 'of'): 1, ('groups', 'people'): 1, ('of', 'people'): 1, ('people', '.'): 1, ('people', 'In'): 1, ('.', 'In'): 1, ('.', 'conclusion'): 1, ('In', 'conclusion'): 1, ('In', ','): 1, ('conclusion', ','): 1, ('conclusion', 'NLP'): 1, ('NLP', 'a'): 1, ('is', 'rapidly'): 1, ('a', 'rapidly'): 1, ('a', 'growing'): 1, ('rapidly', 'growing'): 1, ('rapidly', 'field'): 1, ('growing', 'field'): 1, ('growing', 'with'): 1, ('field', 'with'): 1, ('field', 'many'): 1, ('with', 'many'): 1, ('with', 'exciting'): 1, ('many', 'exciting'): 1, ('many', 'applications'): 1, ('exciting', 'applications'): 1, ('exciting', '.'): 1, ('applications', '.'): 1, ('applications', 'As'): 1, ('.', 'As'): 1, ('.', 'we'): 1, ('As', 'we'): 1, ('As', 'continue'): 1, ('we', 'continue'): 1, ('we', 'to'): 1, ('continue', 'to'): 1, ('continue', 'generate'): 1, ('to', 'generate'): 1, ('generate', 'vast'): 1, ('generate', 'amounts'): 1, ('unstructured', ','): 1, ('data', 'NLP'): 1, (',', 'techniques'): 1, ('NLP', 'will'): 1, ('techniques', 'will'): 1, ('techniques', 'become'): 1, ('will', 'become'): 1, ('will', 'increasingly'): 1, ('increasingly', 'for'): 1, ('important', 'for'): 1, ('important', 'extracting'): 1, ('for', 'extracting'): 1, ('for', 'valuable'): 1, ('extracting', 'valuable'): 1, ('extracting', 'insights'): 1, ('this', '.'): 1, ('data', '.'): 1, ('data', 'However'): 1, ('.', 'However'): 1, ('However', ','): 1, ('However', 'it'): 1, ('it', 'important'): 1, ('is', 'important'): 1, ('important', 'to'): 1, ('important', 'be'): 1, ('to', 'be'): 1, ('to', 'aware'): 1, ('be', 'aware'): 1, ('be', 'of'): 1, ('aware', 'of'): 1, ('aware', 'the'): 1, ('of', 'ethical'): 1, ('the', 'ethical'): 1, ('the', 'implications'): 1, ('ethical', 'implications'): 1, ('ethical', 'of'): 1, ('implications', 'of'): 1, ('implications', 'using'): 1, ('of', 'using'): 1, ('using', 'and'): 1, ('NLP', 'and'): 1, ('and', 'ensure'): 1, ('to', 'ensure'): 1, ('to', 'that'): 1, ('ensure', 'that'): 1, ('ensure', 'these'): 1, ('that', 'these'): 1, ('that', 'technologies'): 1, ('these', 'technologies'): 1, ('these', 'are'): 1, ('technologies', 'are'): 1, ('technologies', 'developed'): 1, ('are', 'developed'): 1, ('are', 'in'): 1, ('developed', 'in'): 1, ('in', 'responsible'): 1, ('a', 'responsible'): 1, ('a', 'and'): 1, ('responsible', 'and'): 1, ('responsible', 'equitable'): 1, ('and', 'equitable'): 1, ('and', 'way'): 1, ('equitable', 'way'): 1, ('equitable', '.'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skip-Gram using basic string manipulation and collections.Counter"
      ],
      "metadata": {
        "id": "Asdx-qgLkNTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define window size for skipgrams\n",
        "window_size = 2\n",
        "\n",
        "# Generate skipgrams\n",
        "skipgrams = []\n",
        "for i, center_word in enumerate(words):\n",
        "    context_words = words[max(0, i - window_size):i] + words[i+1:i+window_size+1]\n",
        "    skipgrams.extend([(center_word, context_word) for context_word in context_words])\n",
        "\n",
        "# Calculate the skipgram frequencies\n",
        "skipgram_counts = Counter(skipgrams)\n",
        "\n",
        "# Skipgram representation (dictionary with skipgram frequencies)\n",
        "skipgram_representation = dict(skipgram_counts)\n",
        "\n",
        "print(skipgram_representation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLJ4rco4kQQL",
        "outputId": "f997edaf-9926-4b40-e22a-0cb30fbc3fed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{('an', 'introduction'): 1, ('an', 'to'): 1, ('introduction', 'an'): 1, ('introduction', 'to'): 1, ('introduction', 'natural'): 1, ('to', 'an'): 1, ('to', 'introduction'): 1, ('to', 'natural'): 1, ('to', 'language'): 1, ('natural', 'introduction'): 1, ('natural', 'to'): 1, ('natural', 'language'): 3, ('natural', 'processing'): 3, ('language', 'to'): 1, ('language', 'natural'): 3, ('language', 'processing'): 2, ('language', 'nlp'): 4, ('processing', 'natural'): 3, ('processing', 'language'): 2, ('processing', 'nlp'): 2, ('nlp', 'language'): 4, ('nlp', 'processing'): 2, ('nlp', 'natural'): 1, ('natural', 'nlp'): 1, ('processing', 'is'): 1, ('nlp', 'is'): 5, ('nlp', 'a'): 2, ('is', 'processing'): 1, ('is', 'nlp'): 5, ('is', 'a'): 3, ('is', 'field'): 1, ('a', 'nlp'): 2, ('a', 'is'): 3, ('a', 'field'): 1, ('a', 'of'): 3, ('field', 'is'): 1, ('field', 'a'): 1, ('field', 'of'): 1, ('field', 'study'): 1, ('of', 'a'): 3, ('of', 'field'): 1, ('of', 'study'): 1, ('of', 'that'): 1, ('study', 'field'): 1, ('study', 'of'): 1, ('study', 'that'): 1, ('study', 'focuses'): 1, ('that', 'of'): 1, ('that', 'study'): 1, ('that', 'focuses'): 1, ('that', 'on'): 1, ('focuses', 'study'): 1, ('focuses', 'that'): 1, ('focuses', 'on'): 1, ('focuses', 'the'): 1, ('on', 'that'): 1, ('on', 'focuses'): 1, ('on', 'the'): 2, ('on', 'interaction'): 1, ('the', 'focuses'): 1, ('the', 'on'): 2, ('the', 'interaction'): 1, ('the', 'between'): 1, ('interaction', 'on'): 1, ('interaction', 'the'): 1, ('interaction', 'between'): 1, ('interaction', 'computers'): 1, ('between', 'the'): 1, ('between', 'interaction'): 1, ('between', 'computers'): 1, ('between', 'and'): 1, ('computers', 'interaction'): 1, ('computers', 'between'): 1, ('computers', 'and'): 1, ('computers', 'humans'): 1, ('and', 'between'): 1, ('and', 'computers'): 1, ('and', 'humans'): 1, ('and', 'using'): 2, ('humans', 'computers'): 1, ('humans', 'and'): 1, ('humans', 'using'): 1, ('humans', 'natural'): 1, ('using', 'and'): 2, ('using', 'humans'): 1, ('using', 'natural'): 1, ('using', 'language'): 1, ('natural', 'humans'): 1, ('natural', 'using'): 1, ('natural', 'it'): 1, ('language', 'using'): 1, ('language', 'it'): 1, ('language', 'involves'): 1, ('it', 'natural'): 1, ('it', 'language'): 1, ('it', 'involves'): 1, ('it', 'developing'): 1, ('involves', 'language'): 1, ('involves', 'it'): 1, ('involves', 'developing'): 1, ('involves', 'algorithms'): 1, ('developing', 'it'): 1, ('developing', 'involves'): 1, ('developing', 'algorithms'): 1, ('developing', 'and'): 1, ('algorithms', 'involves'): 1, ('algorithms', 'developing'): 1, ('algorithms', 'and'): 1, ('algorithms', 'computational'): 1, ('and', 'developing'): 1, ('and', 'algorithms'): 1, ('and', 'computational'): 1, ('and', 'models'): 1, ('computational', 'algorithms'): 1, ('computational', 'and'): 1, ('computational', 'models'): 1, ('computational', 'that'): 1, ('models', 'and'): 1, ('models', 'computational'): 1, ('models', 'that'): 1, ('models', 'can'): 1, ('that', 'computational'): 1, ('that', 'models'): 1, ('that', 'can'): 1, ('that', 'analyze'): 1, ('can', 'models'): 1, ('can', 'that'): 1, ('can', 'analyze'): 1, ('can', 'understand'): 1, ('analyze', 'that'): 1, ('analyze', 'can'): 1, ('analyze', 'understand'): 1, ('analyze', 'and'): 1, ('understand', 'can'): 1, ('understand', 'analyze'): 1, ('understand', 'and'): 2, ('understand', 'generate'): 1, ('and', 'analyze'): 1, ('and', 'understand'): 2, ('and', 'generate'): 1, ('and', 'human'): 1, ('generate', 'understand'): 1, ('generate', 'and'): 1, ('generate', 'human'): 1, ('generate', 'language'): 1, ('human', 'and'): 1, ('human', 'generate'): 1, ('human', 'language'): 2, ('human', 'nlp'): 1, ('language', 'generate'): 1, ('language', 'human'): 2, ('language', 'has'): 1, ('nlp', 'human'): 1, ('nlp', 'has'): 1, ('nlp', 'become'): 1, ('has', 'language'): 1, ('has', 'nlp'): 1, ('has', 'become'): 1, ('has', 'increasingly'): 1, ('become', 'nlp'): 1, ('become', 'has'): 1, ('become', 'increasingly'): 2, ('become', 'important'): 2, ('increasingly', 'has'): 1, ('increasingly', 'become'): 2, ('increasingly', 'important'): 2, ('increasingly', 'in'): 1, ('important', 'become'): 2, ('important', 'increasingly'): 2, ('important', 'in'): 1, ('important', 'recent'): 1, ('in', 'increasingly'): 1, ('in', 'important'): 1, ('in', 'recent'): 1, ('in', 'years'): 1, ('recent', 'important'): 1, ('recent', 'in'): 1, ('recent', 'years'): 1, ('recent', 'due'): 1, ('years', 'in'): 1, ('years', 'recent'): 1, ('years', 'due'): 1, ('years', 'to'): 1, ('due', 'recent'): 1, ('due', 'years'): 1, ('due', 'to'): 1, ('due', 'the'): 1, ('to', 'years'): 1, ('to', 'due'): 1, ('to', 'the'): 2, ('to', 'vast'): 2, ('the', 'due'): 1, ('the', 'to'): 2, ('the', 'vast'): 1, ('the', 'amounts'): 1, ('vast', 'to'): 2, ('vast', 'the'): 1, ('vast', 'amounts'): 2, ('vast', 'of'): 2, ('amounts', 'the'): 1, ('amounts', 'vast'): 2, ('amounts', 'of'): 2, ('amounts', 'unstructured'): 2, ('of', 'vast'): 2, ('of', 'amounts'): 2, ('of', 'unstructured'): 2, ('of', 'data'): 2, ('unstructured', 'amounts'): 2, ('unstructured', 'of'): 2, ('unstructured', 'data'): 2, ('unstructured', 'that'): 1, ('data', 'of'): 2, ('data', 'unstructured'): 2, ('data', 'that'): 1, ('data', 'are'): 1, ('that', 'unstructured'): 1, ('that', 'data'): 1, ('that', 'are'): 1, ('that', 'generated'): 1, ('are', 'data'): 1, ('are', 'that'): 1, ('are', 'generated'): 1, ('are', 'every'): 1, ('generated', 'that'): 1, ('generated', 'are'): 1, ('generated', 'every'): 1, ('generated', 'day'): 1, ('every', 'are'): 1, ('every', 'generated'): 1, ('every', 'day'): 1, ('every', 'through'): 1, ('day', 'generated'): 1, ('day', 'every'): 1, ('day', 'through'): 1, ('day', 'social'): 1, ('through', 'every'): 1, ('through', 'day'): 1, ('through', 'social'): 1, ('through', 'media'): 1, ('social', 'day'): 1, ('social', 'through'): 1, ('social', 'media'): 2, ('social', 'emails'): 1, ('media', 'through'): 1, ('media', 'social'): 2, ('media', 'emails'): 1, ('media', 'and'): 1, ('emails', 'social'): 1, ('emails', 'media'): 1, ('emails', 'and'): 1, ('emails', 'other'): 1, ('and', 'media'): 1, ('and', 'emails'): 1, ('and', 'other'): 1, ('and', 'forms'): 1, ('other', 'emails'): 1, ('other', 'and'): 1, ('other', 'forms'): 1, ('other', 'of'): 1, ('forms', 'and'): 1, ('forms', 'other'): 1, ('forms', 'of'): 1, ('forms', 'communication'): 1, ('of', 'other'): 1, ('of', 'forms'): 1, ('of', 'communication'): 1, ('of', 'by'): 1, ('communication', 'forms'): 1, ('communication', 'of'): 1, ('communication', 'by'): 1, ('communication', 'using'): 1, ('by', 'of'): 1, ('by', 'communication'): 1, ('by', 'using'): 1, ('by', 'nlp'): 1, ('using', 'communication'): 1, ('using', 'by'): 1, ('using', 'nlp'): 2, ('using', 'techniques'): 1, ('nlp', 'by'): 1, ('nlp', 'using'): 2, ('nlp', 'techniques'): 2, ('nlp', 'it'): 1, ('techniques', 'using'): 1, ('techniques', 'nlp'): 2, ('techniques', 'it'): 1, ('techniques', 'is'): 1, ('it', 'nlp'): 1, ('it', 'techniques'): 1, ('it', 'is'): 2, ('it', 'possible'): 1, ('is', 'techniques'): 1, ('is', 'it'): 2, ('is', 'possible'): 1, ('is', 'to'): 3, ('possible', 'it'): 1, ('possible', 'is'): 1, ('possible', 'to'): 1, ('possible', 'extract'): 1, ('to', 'is'): 3, ('to', 'possible'): 1, ('to', 'extract'): 1, ('to', 'valuable'): 1, ('extract', 'possible'): 1, ('extract', 'to'): 1, ('extract', 'valuable'): 1, ('extract', 'insights'): 1, ('valuable', 'to'): 1, ('valuable', 'extract'): 1, ('valuable', 'insights'): 2, ('valuable', 'from'): 2, ('insights', 'extract'): 1, ('insights', 'valuable'): 2, ('insights', 'from'): 2, ('insights', 'this'): 2, ('from', 'valuable'): 2, ('from', 'insights'): 2, ('from', 'this'): 2, ('from', 'data'): 2, ('this', 'insights'): 2, ('this', 'from'): 2, ('this', 'data'): 2, ('this', 'which'): 1, ('data', 'from'): 2, ('data', 'this'): 2, ('data', 'which'): 1, ('data', 'can'): 1, ('which', 'this'): 1, ('which', 'data'): 1, ('which', 'can'): 1, ('which', 'be'): 1, ('can', 'data'): 1, ('can', 'which'): 1, ('can', 'be'): 3, ('can', 'used'): 1, ('be', 'which'): 1, ('be', 'can'): 3, ('be', 'used'): 1, ('be', 'for'): 2, ('used', 'can'): 1, ('used', 'be'): 1, ('used', 'for'): 1, ('used', 'a'): 1, ('for', 'be'): 2, ('for', 'used'): 1, ('for', 'a'): 1, ('for', 'wide'): 1, ('a', 'used'): 1, ('a', 'for'): 1, ('a', 'wide'): 1, ('a', 'range'): 2, ('wide', 'for'): 1, ('wide', 'a'): 1, ('wide', 'range'): 1, ('wide', 'of'): 1, ('range', 'a'): 2, ('range', 'wide'): 1, ('range', 'of'): 2, ('range', 'applications'): 1, ('of', 'wide'): 1, ('of', 'range'): 2, ('of', 'applications'): 1, ('of', 'including'): 2, ('applications', 'range'): 1, ('applications', 'of'): 1, ('applications', 'including'): 1, ('applications', 'sentiment'): 1, ('including', 'of'): 2, ('including', 'applications'): 1, ('including', 'sentiment'): 1, ('including', 'analysis'): 1, ('sentiment', 'applications'): 1, ('sentiment', 'including'): 1, ('sentiment', 'analysis'): 2, ('sentiment', 'chatbots'): 1, ('analysis', 'including'): 1, ('analysis', 'sentiment'): 2, ('analysis', 'chatbots'): 1, ('analysis', 'and'): 1, ('chatbots', 'sentiment'): 1, ('chatbots', 'analysis'): 1, ('chatbots', 'and'): 1, ('chatbots', 'machine'): 1, ('and', 'analysis'): 1, ('and', 'chatbots'): 1, ('and', 'machine'): 1, ('and', 'translation'): 1, ('machine', 'chatbots'): 1, ('machine', 'and'): 1, ('machine', 'translation'): 1, ('machine', 'one'): 1, ('translation', 'and'): 1, ('translation', 'machine'): 1, ('translation', 'one'): 1, ('translation', 'of'): 1, ('one', 'machine'): 1, ('one', 'translation'): 1, ('one', 'of'): 1, ('one', 'the'): 1, ('of', 'translation'): 1, ('of', 'one'): 1, ('of', 'the'): 5, ('of', 'key'): 1, ('the', 'one'): 1, ('the', 'of'): 5, ('the', 'key'): 1, ('the', 'challenges'): 1, ('key', 'of'): 1, ('key', 'the'): 1, ('key', 'challenges'): 1, ('key', 'in'): 1, ('challenges', 'the'): 1, ('challenges', 'key'): 1, ('challenges', 'in'): 1, ('challenges', 'nlp'): 2, ('in', 'key'): 1, ('in', 'challenges'): 1, ('in', 'nlp'): 3, ('in', 'is'): 1, ('nlp', 'challenges'): 2, ('nlp', 'in'): 3, ('nlp', 'the'): 1, ('is', 'in'): 1, ('is', 'the'): 1, ('is', 'ambiguity'): 1, ('the', 'nlp'): 1, ('the', 'is'): 1, ('the', 'ambiguity'): 1, ('ambiguity', 'is'): 1, ('ambiguity', 'the'): 1, ('ambiguity', 'of'): 1, ('ambiguity', 'human'): 1, ('of', 'ambiguity'): 1, ('of', 'human'): 1, ('of', 'language'): 1, ('human', 'ambiguity'): 1, ('human', 'of'): 1, ('human', 'words'): 1, ('language', 'of'): 1, ('language', 'words'): 1, ('language', 'can'): 1, ('words', 'human'): 1, ('words', 'language'): 1, ('words', 'can'): 1, ('words', 'have'): 1, ('can', 'language'): 1, ('can', 'words'): 1, ('can', 'have'): 1, ('can', 'multiple'): 1, ('have', 'words'): 1, ('have', 'can'): 1, ('have', 'multiple'): 1, ('have', 'meanings'): 1, ('multiple', 'can'): 1, ('multiple', 'have'): 1, ('multiple', 'meanings'): 1, ('multiple', 'depending'): 1, ('meanings', 'have'): 1, ('meanings', 'multiple'): 1, ('meanings', 'depending'): 1, ('meanings', 'on'): 1, ('depending', 'multiple'): 1, ('depending', 'meanings'): 1, ('depending', 'on'): 1, ('depending', 'the'): 1, ('on', 'meanings'): 1, ('on', 'depending'): 1, ('on', 'context'): 1, ('the', 'depending'): 1, ('the', 'context'): 1, ('the', 'in'): 1, ('context', 'on'): 1, ('context', 'the'): 1, ('context', 'in'): 1, ('context', 'which'): 1, ('in', 'the'): 1, ('in', 'context'): 1, ('in', 'which'): 1, ('in', 'they'): 1, ('which', 'context'): 1, ('which', 'in'): 1, ('which', 'they'): 1, ('which', 'are'): 1, ('they', 'in'): 1, ('they', 'which'): 1, ('they', 'are'): 2, ('they', 'used'): 1, ('are', 'which'): 1, ('are', 'they'): 2, ('are', 'used'): 1, ('are', 'and'): 1, ('used', 'they'): 1, ('used', 'are'): 1, ('used', 'and'): 1, ('used', 'grammar'): 1, ('and', 'are'): 1, ('and', 'used'): 1, ('and', 'grammar'): 1, ('and', 'rules'): 1, ('grammar', 'used'): 1, ('grammar', 'and'): 1, ('grammar', 'rules'): 1, ('grammar', 'can'): 1, ('rules', 'and'): 1, ('rules', 'grammar'): 1, ('rules', 'can'): 1, ('rules', 'be'): 1, ('can', 'grammar'): 1, ('can', 'rules'): 1, ('can', 'broken'): 1, ('be', 'rules'): 1, ('be', 'broken'): 1, ('be', 'without'): 1, ('broken', 'can'): 1, ('broken', 'be'): 1, ('broken', 'without'): 1, ('broken', 'affecting'): 1, ('without', 'be'): 1, ('without', 'broken'): 1, ('without', 'affecting'): 1, ('without', 'the'): 1, ('affecting', 'broken'): 1, ('affecting', 'without'): 1, ('affecting', 'the'): 1, ('affecting', 'meaning'): 1, ('the', 'without'): 1, ('the', 'affecting'): 1, ('the', 'meaning'): 1, ('meaning', 'affecting'): 1, ('meaning', 'the'): 1, ('meaning', 'of'): 1, ('meaning', 'a'): 1, ('of', 'meaning'): 1, ('of', 'sentence'): 1, ('a', 'meaning'): 1, ('a', 'sentence'): 1, ('a', 'to'): 1, ('sentence', 'of'): 1, ('sentence', 'a'): 1, ('sentence', 'to'): 1, ('sentence', 'address'): 1, ('to', 'a'): 1, ('to', 'sentence'): 1, ('to', 'address'): 1, ('to', 'these'): 1, ('address', 'sentence'): 1, ('address', 'to'): 1, ('address', 'these'): 1, ('address', 'challenges'): 1, ('these', 'to'): 1, ('these', 'address'): 1, ('these', 'challenges'): 1, ('these', 'nlp'): 1, ('challenges', 'address'): 1, ('challenges', 'these'): 1, ('challenges', 'researchers'): 1, ('nlp', 'these'): 1, ('nlp', 'researchers'): 1, ('nlp', 'have'): 1, ('researchers', 'challenges'): 1, ('researchers', 'nlp'): 1, ('researchers', 'have'): 1, ('researchers', 'developed'): 1, ('have', 'nlp'): 1, ('have', 'researchers'): 1, ('have', 'developed'): 1, ('have', 'a'): 1, ('developed', 'researchers'): 1, ('developed', 'have'): 1, ('developed', 'a'): 2, ('developed', 'range'): 1, ('a', 'have'): 1, ('a', 'developed'): 2, ('range', 'developed'): 1, ('range', 'techniques'): 1, ('of', 'techniques'): 1, ('techniques', 'range'): 1, ('techniques', 'of'): 1, ('techniques', 'including'): 1, ('techniques', 'statistical'): 1, ('including', 'techniques'): 1, ('including', 'statistical'): 1, ('including', 'models'): 1, ('statistical', 'techniques'): 1, ('statistical', 'including'): 1, ('statistical', 'models'): 1, ('statistical', 'rule'): 1, ('models', 'including'): 1, ('models', 'statistical'): 1, ('models', 'rule'): 1, ('models', 'based'): 1, ('rule', 'statistical'): 1, ('rule', 'models'): 1, ('rule', 'based'): 1, ('rule', 'systems'): 1, ('based', 'models'): 1, ('based', 'rule'): 1, ('based', 'systems'): 1, ('based', 'and'): 1, ('systems', 'rule'): 1, ('systems', 'based'): 1, ('systems', 'and'): 1, ('systems', 'deep'): 1, ('and', 'based'): 1, ('and', 'systems'): 1, ('and', 'deep'): 1, ('and', 'learning'): 1, ('deep', 'systems'): 1, ('deep', 'and'): 1, ('deep', 'learning'): 1, ('deep', 'algorithms'): 1, ('learning', 'and'): 1, ('learning', 'deep'): 1, ('learning', 'algorithms'): 1, ('learning', 'one'): 1, ('algorithms', 'deep'): 1, ('algorithms', 'learning'): 1, ('algorithms', 'one'): 1, ('algorithms', 'popular'): 1, ('one', 'learning'): 1, ('one', 'algorithms'): 1, ('one', 'popular'): 1, ('one', 'application'): 1, ('popular', 'algorithms'): 1, ('popular', 'one'): 1, ('popular', 'application'): 1, ('popular', 'of'): 1, ('application', 'one'): 1, ('application', 'popular'): 1, ('application', 'of'): 1, ('application', 'nlp'): 1, ('of', 'popular'): 1, ('of', 'application'): 1, ('of', 'nlp'): 2, ('of', 'is'): 1, ('nlp', 'application'): 1, ('nlp', 'of'): 2, ('nlp', 'sentiment'): 1, ('is', 'of'): 1, ('is', 'sentiment'): 1, ('is', 'analysis'): 1, ('sentiment', 'nlp'): 1, ('sentiment', 'is'): 1, ('sentiment', 'which'): 1, ('analysis', 'is'): 1, ('analysis', 'which'): 1, ('analysis', 'involves'): 1, ('which', 'sentiment'): 1, ('which', 'analysis'): 1, ('which', 'involves'): 1, ('which', 'analyzing'): 1, ('involves', 'analysis'): 1, ('involves', 'which'): 1, ('involves', 'analyzing'): 1, ('involves', 'text'): 1, ('analyzing', 'which'): 1, ('analyzing', 'involves'): 1, ('analyzing', 'text'): 1, ('analyzing', 'to'): 1, ('text', 'involves'): 1, ('text', 'analyzing'): 1, ('text', 'to'): 1, ('text', 'determine'): 1, ('to', 'analyzing'): 1, ('to', 'text'): 1, ('to', 'determine'): 1, ('determine', 'text'): 1, ('determine', 'to'): 1, ('determine', 'the'): 1, ('determine', 'emotional'): 1, ('the', 'determine'): 1, ('the', 'emotional'): 1, ('the', 'tone'): 2, ('emotional', 'determine'): 1, ('emotional', 'the'): 1, ('emotional', 'tone'): 1, ('emotional', 'of'): 1, ('tone', 'the'): 2, ('tone', 'emotional'): 1, ('tone', 'of'): 1, ('of', 'emotional'): 1, ('of', 'tone'): 1, ('of', 'writer'): 1, ('the', 'writer'): 1, ('the', 'this'): 1, ('writer', 'of'): 1, ('writer', 'the'): 1, ('writer', 'this'): 1, ('writer', 'can'): 1, ('this', 'the'): 1, ('this', 'writer'): 1, ('this', 'can'): 1, ('this', 'be'): 1, ('can', 'writer'): 1, ('can', 'this'): 1, ('can', 'useful'): 1, ('be', 'this'): 1, ('be', 'useful'): 1, ('useful', 'can'): 1, ('useful', 'be'): 1, ('useful', 'for'): 1, ('useful', 'businesses'): 1, ('for', 'useful'): 1, ('for', 'businesses'): 1, ('for', 'who'): 1, ('businesses', 'useful'): 1, ('businesses', 'for'): 1, ('businesses', 'who'): 1, ('businesses', 'want'): 1, ('who', 'for'): 1, ('who', 'businesses'): 1, ('who', 'want'): 1, ('who', 'to'): 1, ('want', 'businesses'): 1, ('want', 'who'): 1, ('want', 'to'): 1, ('want', 'understand'): 1, ('to', 'who'): 1, ('to', 'want'): 1, ('to', 'understand'): 2, ('to', 'how'): 1, ('understand', 'want'): 1, ('understand', 'to'): 2, ('understand', 'how'): 1, ('understand', 'their'): 1, ('how', 'to'): 1, ('how', 'understand'): 1, ('how', 'their'): 1, ('how', 'customers'): 1, ('their', 'understand'): 1, ('their', 'how'): 1, ('their', 'customers'): 1, ('their', 'feel'): 2, ('customers', 'how'): 1, ('customers', 'their'): 1, ('customers', 'feel'): 1, ('customers', 'about'): 1, ('feel', 'their'): 2, ('feel', 'customers'): 1, ('feel', 'about'): 1, ('about', 'customers'): 1, ('about', 'feel'): 1, ('about', 'their'): 1, ('about', 'products'): 1, ('their', 'about'): 1, ('their', 'products'): 1, ('their', 'or'): 1, ('products', 'about'): 1, ('products', 'their'): 1, ('products', 'or'): 1, ('products', 'services'): 1, ('or', 'their'): 1, ('or', 'products'): 1, ('or', 'services'): 1, ('or', 'another'): 1, ('services', 'products'): 1, ('services', 'or'): 1, ('services', 'another'): 1, ('services', 'application'): 1, ('another', 'or'): 1, ('another', 'services'): 1, ('another', 'application'): 1, ('another', 'is'): 1, ('application', 'services'): 1, ('application', 'another'): 1, ('application', 'is'): 1, ('application', 'chatbots'): 1, ('is', 'another'): 1, ('is', 'application'): 1, ('is', 'chatbots'): 1, ('is', 'which'): 1, ('chatbots', 'application'): 1, ('chatbots', 'is'): 1, ('chatbots', 'which'): 1, ('chatbots', 'use'): 1, ('which', 'is'): 1, ('which', 'chatbots'): 1, ('which', 'use'): 1, ('which', 'nlp'): 1, ('use', 'chatbots'): 1, ('use', 'which'): 1, ('use', 'nlp'): 1, ('use', 'to'): 1, ('nlp', 'which'): 1, ('nlp', 'use'): 1, ('nlp', 'to'): 2, ('nlp', 'understand'): 1, ('to', 'use'): 1, ('to', 'nlp'): 2, ('to', 'and'): 3, ('understand', 'nlp'): 1, ('understand', 'respond'): 1, ('and', 'to'): 3, ('and', 'respond'): 1, ('respond', 'understand'): 1, ('respond', 'and'): 1, ('respond', 'to'): 1, ('respond', 'user'): 1, ('to', 'respond'): 1, ('to', 'user'): 1, ('to', 'queries'): 1, ('user', 'respond'): 1, ('user', 'to'): 1, ('user', 'queries'): 1, ('user', 'in'): 1, ('queries', 'to'): 1, ('queries', 'user'): 1, ('queries', 'in'): 1, ('queries', 'a'): 1, ('in', 'user'): 1, ('in', 'queries'): 1, ('in', 'a'): 2, ('in', 'natural'): 1, ('a', 'queries'): 1, ('a', 'in'): 2, ('a', 'natural'): 1, ('a', 'way'): 1, ('natural', 'in'): 1, ('natural', 'a'): 1, ('natural', 'way'): 1, ('natural', 'despite'): 1, ('way', 'a'): 1, ('way', 'natural'): 1, ('way', 'despite'): 1, ('way', 'its'): 1, ('despite', 'natural'): 1, ('despite', 'way'): 1, ('despite', 'its'): 1, ('despite', 'many'): 1, ('its', 'way'): 1, ('its', 'despite'): 1, ('its', 'many'): 1, ('its', 'benefits'): 1, ('many', 'despite'): 1, ('many', 'its'): 1, ('many', 'benefits'): 1, ('many', 'nlp'): 1, ('benefits', 'its'): 1, ('benefits', 'many'): 1, ('benefits', 'nlp'): 1, ('benefits', 'also'): 1, ('nlp', 'many'): 1, ('nlp', 'benefits'): 1, ('nlp', 'also'): 1, ('nlp', 'raises'): 1, ('also', 'benefits'): 1, ('also', 'nlp'): 1, ('also', 'raises'): 1, ('also', 'ethical'): 1, ('raises', 'nlp'): 1, ('raises', 'also'): 1, ('raises', 'ethical'): 1, ('raises', 'concerns'): 1, ('ethical', 'also'): 1, ('ethical', 'raises'): 1, ('ethical', 'concerns'): 1, ('ethical', 'around'): 1, ('concerns', 'raises'): 1, ('concerns', 'ethical'): 1, ('concerns', 'around'): 1, ('concerns', 'issues'): 1, ('around', 'ethical'): 1, ('around', 'concerns'): 1, ('around', 'issues'): 1, ('around', 'such'): 1, ('issues', 'concerns'): 1, ('issues', 'around'): 1, ('issues', 'such'): 1, ('issues', 'as'): 1, ('such', 'around'): 1, ('such', 'issues'): 1, ('such', 'as'): 1, ('such', 'privacy'): 1, ('as', 'issues'): 1, ('as', 'such'): 1, ('as', 'privacy'): 1, ('as', 'and'): 1, ('privacy', 'such'): 1, ('privacy', 'as'): 1, ('privacy', 'and'): 1, ('privacy', 'bias'): 1, ('and', 'as'): 1, ('and', 'privacy'): 1, ('and', 'bias'): 1, ('and', 'for'): 1, ('bias', 'privacy'): 1, ('bias', 'and'): 1, ('bias', 'for'): 1, ('bias', 'example'): 1, ('for', 'and'): 1, ('for', 'bias'): 1, ('for', 'example'): 1, ('for', 'if'): 1, ('example', 'bias'): 1, ('example', 'for'): 1, ('example', 'if'): 1, ('example', 'nlp'): 1, ('if', 'for'): 1, ('if', 'example'): 1, ('if', 'nlp'): 1, ('if', 'is'): 1, ('nlp', 'example'): 1, ('nlp', 'if'): 1, ('nlp', 'used'): 2, ('is', 'if'): 1, ('is', 'used'): 1, ('used', 'nlp'): 2, ('used', 'is'): 1, ('used', 'to'): 1, ('used', 'analyze'): 1, ('to', 'used'): 1, ('to', 'analyze'): 1, ('to', 'social'): 1, ('analyze', 'used'): 1, ('analyze', 'to'): 1, ('analyze', 'social'): 1, ('analyze', 'media'): 1, ('social', 'to'): 1, ('social', 'analyze'): 1, ('social', 'data'): 1, ('media', 'analyze'): 1, ('media', 'data'): 1, ('media', 'there'): 1, ('data', 'social'): 1, ('data', 'media'): 1, ('data', 'there'): 1, ('data', 'is'): 1, ('there', 'media'): 1, ('there', 'data'): 1, ('there', 'is'): 1, ('there', 'a'): 1, ('is', 'data'): 1, ('is', 'there'): 1, ('is', 'risk'): 1, ('a', 'there'): 1, ('a', 'risk'): 1, ('a', 'that'): 1, ('risk', 'is'): 1, ('risk', 'a'): 1, ('risk', 'that'): 1, ('risk', 'personal'): 1, ('that', 'a'): 1, ('that', 'risk'): 1, ('that', 'personal'): 1, ('that', 'information'): 1, ('personal', 'risk'): 1, ('personal', 'that'): 1, ('personal', 'information'): 1, ('personal', 'could'): 1, ('information', 'that'): 1, ('information', 'personal'): 1, ('information', 'could'): 1, ('information', 'be'): 1, ('could', 'personal'): 1, ('could', 'information'): 1, ('could', 'be'): 1, ('could', 'exposed'): 1, ('be', 'information'): 1, ('be', 'could'): 1, ('be', 'exposed'): 1, ('be', 'additionally'): 1, ('exposed', 'could'): 1, ('exposed', 'be'): 1, ('exposed', 'additionally'): 1, ('exposed', 'if'): 1, ('additionally', 'be'): 1, ('additionally', 'exposed'): 1, ('additionally', 'if'): 1, ('additionally', 'the'): 1, ('if', 'exposed'): 1, ('if', 'additionally'): 1, ('if', 'the'): 1, ('if', 'algorithms'): 1, ('the', 'additionally'): 1, ('the', 'if'): 1, ('the', 'algorithms'): 1, ('the', 'used'): 1, ('algorithms', 'if'): 1, ('algorithms', 'the'): 1, ('algorithms', 'used'): 1, ('algorithms', 'in'): 1, ('used', 'the'): 1, ('used', 'algorithms'): 1, ('used', 'in'): 1, ('in', 'algorithms'): 1, ('in', 'used'): 1, ('in', 'are'): 2, ('nlp', 'are'): 1, ('nlp', 'biased'): 1, ('are', 'in'): 2, ('are', 'nlp'): 1, ('are', 'biased'): 1, ('biased', 'nlp'): 1, ('biased', 'are'): 1, ('biased', 'they'): 1, ('biased', 'could'): 1, ('they', 'biased'): 1, ('they', 'could'): 1, ('they', 'perpetuate'): 1, ('could', 'biased'): 1, ('could', 'they'): 1, ('could', 'perpetuate'): 1, ('could', 'discrimination'): 1, ('perpetuate', 'they'): 1, ('perpetuate', 'could'): 1, ('perpetuate', 'discrimination'): 1, ('perpetuate', 'against'): 1, ('discrimination', 'could'): 1, ('discrimination', 'perpetuate'): 1, ('discrimination', 'against'): 1, ('discrimination', 'certain'): 1, ('against', 'perpetuate'): 1, ('against', 'discrimination'): 1, ('against', 'certain'): 1, ('against', 'groups'): 1, ('certain', 'discrimination'): 1, ('certain', 'against'): 1, ('certain', 'groups'): 1, ('certain', 'of'): 1, ('groups', 'against'): 1, ('groups', 'certain'): 1, ('groups', 'of'): 1, ('groups', 'people'): 1, ('of', 'certain'): 1, ('of', 'groups'): 1, ('of', 'people'): 1, ('of', 'in'): 1, ('people', 'groups'): 1, ('people', 'of'): 1, ('people', 'in'): 1, ('people', 'conclusion'): 1, ('in', 'of'): 1, ('in', 'people'): 1, ('in', 'conclusion'): 1, ('conclusion', 'people'): 1, ('conclusion', 'in'): 1, ('conclusion', 'nlp'): 1, ('conclusion', 'is'): 1, ('nlp', 'conclusion'): 1, ('is', 'conclusion'): 1, ('is', 'rapidly'): 1, ('a', 'rapidly'): 1, ('a', 'growing'): 1, ('rapidly', 'is'): 1, ('rapidly', 'a'): 1, ('rapidly', 'growing'): 1, ('rapidly', 'field'): 1, ('growing', 'a'): 1, ('growing', 'rapidly'): 1, ('growing', 'field'): 1, ('growing', 'with'): 1, ('field', 'rapidly'): 1, ('field', 'growing'): 1, ('field', 'with'): 1, ('field', 'many'): 1, ('with', 'growing'): 1, ('with', 'field'): 1, ('with', 'many'): 1, ('with', 'exciting'): 1, ('many', 'field'): 1, ('many', 'with'): 1, ('many', 'exciting'): 1, ('many', 'applications'): 1, ('exciting', 'with'): 1, ('exciting', 'many'): 1, ('exciting', 'applications'): 1, ('exciting', 'as'): 1, ('applications', 'many'): 1, ('applications', 'exciting'): 1, ('applications', 'as'): 1, ('applications', 'we'): 1, ('as', 'exciting'): 1, ('as', 'applications'): 1, ('as', 'we'): 1, ('as', 'continue'): 1, ('we', 'applications'): 1, ('we', 'as'): 1, ('we', 'continue'): 1, ('we', 'to'): 1, ('continue', 'as'): 1, ('continue', 'we'): 1, ('continue', 'to'): 1, ('continue', 'generate'): 1, ('to', 'we'): 1, ('to', 'continue'): 1, ('to', 'generate'): 1, ('generate', 'continue'): 1, ('generate', 'to'): 1, ('generate', 'vast'): 1, ('generate', 'amounts'): 1, ('vast', 'generate'): 1, ('amounts', 'generate'): 1, ('unstructured', 'nlp'): 1, ('data', 'nlp'): 1, ('data', 'techniques'): 1, ('nlp', 'unstructured'): 1, ('nlp', 'data'): 1, ('nlp', 'will'): 1, ('techniques', 'data'): 1, ('techniques', 'will'): 1, ('techniques', 'become'): 1, ('will', 'nlp'): 1, ('will', 'techniques'): 1, ('will', 'become'): 1, ('will', 'increasingly'): 1, ('become', 'techniques'): 1, ('become', 'will'): 1, ('increasingly', 'will'): 1, ('increasingly', 'for'): 1, ('important', 'for'): 1, ('important', 'extracting'): 1, ('for', 'increasingly'): 1, ('for', 'important'): 1, ('for', 'extracting'): 1, ('for', 'valuable'): 1, ('extracting', 'important'): 1, ('extracting', 'for'): 1, ('extracting', 'valuable'): 1, ('extracting', 'insights'): 1, ('valuable', 'for'): 1, ('valuable', 'extracting'): 1, ('insights', 'extracting'): 1, ('this', 'however'): 1, ('data', 'however'): 1, ('data', 'it'): 1, ('however', 'this'): 1, ('however', 'data'): 1, ('however', 'it'): 1, ('however', 'is'): 1, ('it', 'data'): 1, ('it', 'however'): 1, ('it', 'important'): 1, ('is', 'however'): 1, ('is', 'important'): 1, ('important', 'it'): 1, ('important', 'is'): 1, ('important', 'to'): 1, ('important', 'be'): 1, ('to', 'important'): 1, ('to', 'be'): 1, ('to', 'aware'): 1, ('be', 'important'): 1, ('be', 'to'): 1, ('be', 'aware'): 1, ('be', 'of'): 1, ('aware', 'to'): 1, ('aware', 'be'): 1, ('aware', 'of'): 1, ('aware', 'the'): 1, ('of', 'be'): 1, ('of', 'aware'): 1, ('of', 'ethical'): 2, ('the', 'aware'): 1, ('the', 'ethical'): 1, ('the', 'implications'): 1, ('ethical', 'of'): 2, ('ethical', 'the'): 1, ('ethical', 'implications'): 1, ('implications', 'the'): 1, ('implications', 'ethical'): 1, ('implications', 'of'): 1, ('implications', 'using'): 1, ('of', 'implications'): 1, ('of', 'using'): 1, ('using', 'implications'): 1, ('using', 'of'): 1, ('nlp', 'and'): 1, ('and', 'nlp'): 1, ('and', 'ensure'): 1, ('to', 'ensure'): 1, ('to', 'that'): 1, ('ensure', 'and'): 1, ('ensure', 'to'): 1, ('ensure', 'that'): 1, ('ensure', 'these'): 1, ('that', 'to'): 1, ('that', 'ensure'): 1, ('that', 'these'): 1, ('that', 'technologies'): 1, ('these', 'ensure'): 1, ('these', 'that'): 1, ('these', 'technologies'): 1, ('these', 'are'): 1, ('technologies', 'that'): 1, ('technologies', 'these'): 1, ('technologies', 'are'): 1, ('technologies', 'developed'): 1, ('are', 'these'): 1, ('are', 'technologies'): 1, ('are', 'developed'): 1, ('developed', 'technologies'): 1, ('developed', 'are'): 1, ('developed', 'in'): 1, ('in', 'developed'): 1, ('in', 'responsible'): 1, ('a', 'responsible'): 1, ('a', 'and'): 1, ('responsible', 'in'): 1, ('responsible', 'a'): 1, ('responsible', 'and'): 1, ('responsible', 'equitable'): 1, ('and', 'a'): 1, ('and', 'responsible'): 1, ('and', 'equitable'): 1, ('and', 'way'): 1, ('equitable', 'responsible'): 1, ('equitable', 'and'): 1, ('equitable', 'way'): 1, ('way', 'and'): 1, ('way', 'equitable'): 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4 - Find Out-of-Vocabulary Words for the sentence “An example sentence with unseen words” where \"This is a sample sentence.\", \"Another sentence for demonstration.\", \"Creating a Bag-of-Words matrix.\" are in a given corpus."
      ],
      "metadata": {
        "id": "dCasOHNJvBGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List of sentences\n",
        "sentences = [\n",
        "    \"This is a sample sentence.\",\n",
        "    \"Another sentence for demonstration.\",\n",
        "    \"Creating a Bag-of-Words matrix.\"\n",
        "]\n",
        "\n",
        "# Preprocess and tokenize the sentences\n",
        "preprocessed_sentences = [re.findall(r'\\w+', sentence.lower()) for sentence in sentences]\n",
        "\n",
        "# Join the preprocessed sentences back into strings\n",
        "preprocessed_strings = [' '.join(words) for words in preprocessed_sentences]\n",
        "\n",
        "# Create a CountVectorizer instance\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to build the vocabulary\n",
        "vectorizer.fit(preprocessed_strings)\n",
        "\n",
        "# Transform the sentences into a BoW matrix\n",
        "bow_matrix = vectorizer.transform(preprocessed_strings)\n",
        "\n",
        "# Get the feature (word) names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the BoW matrix to an array for printing\n",
        "bow_array = bow_matrix.toarray()\n",
        "\n",
        "# Print the BoW matrix\n",
        "print(\"Feature Names:\", feature_names)\n",
        "print(\"BoW Matrix:\")\n",
        "print(bow_array)\n",
        "\n",
        "# New sentence with an OOV word\n",
        "new_sentence = \"An example sentence with unseen words.\"\n",
        "\n",
        "# Tokenize the new sentence\n",
        "new_sentence_tokens = re.findall(r'\\w+', new_sentence.lower())\n",
        "\n",
        "# Identify OOV words\n",
        "oov_words = [word for word in new_sentence_tokens if word not in feature_names]\n",
        "\n",
        "print(\"Out-of-Vocabulary Words:\", oov_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGO3sCYYvFzY",
        "outputId": "4f2c3637-83b0-4b3b-aa67-4c7f489f46e3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['another' 'bag' 'creating' 'demonstration' 'for' 'is' 'matrix' 'of'\n",
            " 'sample' 'sentence' 'this' 'words']\n",
            "BoW Matrix:\n",
            "[[0 0 0 0 0 1 0 0 1 1 1 0]\n",
            " [1 0 0 1 1 0 0 0 0 1 0 0]\n",
            " [0 1 1 0 0 0 1 1 0 0 0 1]]\n",
            "Out-of-Vocabulary Words: ['an', 'example', 'with', 'unseen']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Based on the given input text data\n",
        "\n",
        "\n",
        "# List of documents (corpus)\n",
        "sentences = sent_tokenize(data)\n",
        "\n",
        "# Preprocess and tokenize the sentences\n",
        "preprocessed_sentences = [re.findall(r'\\w+', sentence.lower()) for sentence in sentences]\n",
        "\n",
        "# Join the preprocessed sentences back into strings\n",
        "preprocessed_strings = [' '.join(words) for words in preprocessed_sentences]\n",
        "\n",
        "# Create a CountVectorizer instance\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit the vectorizer to build the vocabulary\n",
        "vectorizer.fit(preprocessed_strings)\n",
        "\n",
        "# Transform the sentences into a BoW matrix\n",
        "bow_matrix = vectorizer.transform(preprocessed_strings)\n",
        "\n",
        "# Get the feature (word) names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the BoW matrix to an array for printing\n",
        "bow_array = bow_matrix.toarray()\n",
        "\n",
        "# Print the BoW matrix\n",
        "print(\"Feature Names:\", feature_names)\n",
        "print(\"BoW Matrix:\")\n",
        "print(bow_array)\n",
        "\n",
        "# New sentence with an OOV word\n",
        "new_sentence = \"An example sentence with unseen words.\"\n",
        "\n",
        "# Tokenize the new sentence\n",
        "new_sentence_tokens = re.findall(r'\\w+', new_sentence.lower())\n",
        "\n",
        "# Identify OOV words\n",
        "oov_words = [word for word in new_sentence_tokens if word not in feature_names]\n",
        "\n",
        "print(\"Out-of-Vocabulary Words:\", oov_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83EFKjQFv7CC",
        "outputId": "298ed22d-cebf-44b1-f8d3-88104bf827d7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['about' 'additionally' 'address' 'affecting' 'against' 'algorithms'\n",
            " 'also' 'ambiguity' 'amounts' 'an' 'analysis' 'analyze' 'analyzing' 'and'\n",
            " 'another' 'application' 'applications' 'are' 'around' 'as' 'aware'\n",
            " 'based' 'be' 'become' 'benefits' 'between' 'bias' 'biased' 'broken'\n",
            " 'businesses' 'by' 'can' 'certain' 'challenges' 'chatbots' 'communication'\n",
            " 'computational' 'computers' 'concerns' 'conclusion' 'context' 'continue'\n",
            " 'could' 'customers' 'data' 'day' 'deep' 'depending' 'despite' 'determine'\n",
            " 'developed' 'developing' 'discrimination' 'due' 'emails' 'emotional'\n",
            " 'ensure' 'equitable' 'ethical' 'every' 'example' 'exciting' 'exposed'\n",
            " 'extract' 'extracting' 'feel' 'field' 'focuses' 'for' 'forms' 'from'\n",
            " 'generate' 'generated' 'grammar' 'groups' 'growing' 'has' 'have' 'how'\n",
            " 'however' 'human' 'humans' 'if' 'implications' 'important' 'in'\n",
            " 'including' 'increasingly' 'information' 'insights' 'interaction'\n",
            " 'introduction' 'involves' 'is' 'issues' 'it' 'its' 'key' 'language'\n",
            " 'learning' 'machine' 'many' 'meaning' 'meanings' 'media' 'models'\n",
            " 'multiple' 'natural' 'nlp' 'of' 'on' 'one' 'or' 'other' 'people'\n",
            " 'perpetuate' 'personal' 'popular' 'possible' 'privacy' 'processing'\n",
            " 'products' 'queries' 'raises' 'range' 'rapidly' 'recent' 'researchers'\n",
            " 'respond' 'responsible' 'risk' 'rule' 'rules' 'sentence' 'sentiment'\n",
            " 'services' 'social' 'statistical' 'study' 'such' 'systems' 'techniques'\n",
            " 'technologies' 'text' 'that' 'the' 'their' 'there' 'these' 'they' 'this'\n",
            " 'through' 'to' 'tone' 'translation' 'understand' 'unstructured' 'use'\n",
            " 'used' 'useful' 'user' 'using' 'valuable' 'vast' 'want' 'way' 'we'\n",
            " 'which' 'who' 'wide' 'will' 'with' 'without' 'words' 'writer' 'years']\n",
            "BoW Matrix:\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 1]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n",
            "Out-of-Vocabulary Words: ['unseen']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5 - Calculate tf, idf, df and tf-idf for a given text corpus."
      ],
      "metadata": {
        "id": "m6EX2AZPkkDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libraries\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import math"
      ],
      "metadata": {
        "id": "GbogNKF7tnT6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TF [Term Frequency]\n",
        "\n",
        "# List of documents (corpus)\n",
        "sentences = sent_tokenize(data)\n",
        "\n",
        "# Preprocess and tokenize the corpus\n",
        "preprocessed_corpus = [re.findall(r'\\w+', document.lower()) for document in sentences]\n",
        "\n",
        "# Calculate term frequency (TF)\n",
        "tf = [defaultdict(int) for _ in range(len(preprocessed_corpus))]\n",
        "\n",
        "for doc_idx, document in enumerate(preprocessed_corpus):\n",
        "    for word in document:\n",
        "        tf[doc_idx][word] += 1\n",
        "\n",
        "# Print the term frequency for each document\n",
        "for doc_idx, tf_dict in enumerate(tf):\n",
        "    print(f\"Document {doc_idx + 1} - TF:\")\n",
        "    print(\"-\"*50)\n",
        "    for word, freq in tf_dict.items():\n",
        "        print(f\"{word}: {freq}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUuvleymkmBu",
        "outputId": "c8abb567-727c-4c3d-98a3-bede239bd8de"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1 - TF:\n",
            "--------------------------------------------------\n",
            "an: 1\n",
            "introduction: 1\n",
            "to: 1\n",
            "natural: 3\n",
            "language: 3\n",
            "processing: 2\n",
            "nlp: 2\n",
            "is: 1\n",
            "a: 1\n",
            "field: 1\n",
            "of: 1\n",
            "study: 1\n",
            "that: 1\n",
            "focuses: 1\n",
            "on: 1\n",
            "the: 1\n",
            "interaction: 1\n",
            "between: 1\n",
            "computers: 1\n",
            "and: 1\n",
            "humans: 1\n",
            "using: 1\n",
            "\n",
            "Document 2 - TF:\n",
            "--------------------------------------------------\n",
            "it: 1\n",
            "involves: 1\n",
            "developing: 1\n",
            "algorithms: 1\n",
            "and: 2\n",
            "computational: 1\n",
            "models: 1\n",
            "that: 1\n",
            "can: 1\n",
            "analyze: 1\n",
            "understand: 1\n",
            "generate: 1\n",
            "human: 1\n",
            "language: 1\n",
            "\n",
            "Document 3 - TF:\n",
            "--------------------------------------------------\n",
            "nlp: 1\n",
            "has: 1\n",
            "become: 1\n",
            "increasingly: 1\n",
            "important: 1\n",
            "in: 1\n",
            "recent: 1\n",
            "years: 1\n",
            "due: 1\n",
            "to: 1\n",
            "the: 1\n",
            "vast: 1\n",
            "amounts: 1\n",
            "of: 2\n",
            "unstructured: 1\n",
            "data: 1\n",
            "that: 1\n",
            "are: 1\n",
            "generated: 1\n",
            "every: 1\n",
            "day: 1\n",
            "through: 1\n",
            "social: 1\n",
            "media: 1\n",
            "emails: 1\n",
            "and: 1\n",
            "other: 1\n",
            "forms: 1\n",
            "communication: 1\n",
            "\n",
            "Document 4 - TF:\n",
            "--------------------------------------------------\n",
            "by: 1\n",
            "using: 1\n",
            "nlp: 1\n",
            "techniques: 1\n",
            "it: 1\n",
            "is: 1\n",
            "possible: 1\n",
            "to: 1\n",
            "extract: 1\n",
            "valuable: 1\n",
            "insights: 1\n",
            "from: 1\n",
            "this: 1\n",
            "data: 1\n",
            "which: 1\n",
            "can: 1\n",
            "be: 1\n",
            "used: 1\n",
            "for: 1\n",
            "a: 1\n",
            "wide: 1\n",
            "range: 1\n",
            "of: 1\n",
            "applications: 1\n",
            "including: 1\n",
            "sentiment: 1\n",
            "analysis: 1\n",
            "chatbots: 1\n",
            "and: 1\n",
            "machine: 1\n",
            "translation: 1\n",
            "\n",
            "Document 5 - TF:\n",
            "--------------------------------------------------\n",
            "one: 1\n",
            "of: 2\n",
            "the: 2\n",
            "key: 1\n",
            "challenges: 1\n",
            "in: 1\n",
            "nlp: 1\n",
            "is: 1\n",
            "ambiguity: 1\n",
            "human: 1\n",
            "language: 1\n",
            "\n",
            "Document 6 - TF:\n",
            "--------------------------------------------------\n",
            "words: 1\n",
            "can: 2\n",
            "have: 1\n",
            "multiple: 1\n",
            "meanings: 1\n",
            "depending: 1\n",
            "on: 1\n",
            "the: 2\n",
            "context: 1\n",
            "in: 1\n",
            "which: 1\n",
            "they: 1\n",
            "are: 1\n",
            "used: 1\n",
            "and: 1\n",
            "grammar: 1\n",
            "rules: 1\n",
            "be: 1\n",
            "broken: 1\n",
            "without: 1\n",
            "affecting: 1\n",
            "meaning: 1\n",
            "of: 1\n",
            "a: 1\n",
            "sentence: 1\n",
            "\n",
            "Document 7 - TF:\n",
            "--------------------------------------------------\n",
            "to: 1\n",
            "address: 1\n",
            "these: 1\n",
            "challenges: 1\n",
            "nlp: 1\n",
            "researchers: 1\n",
            "have: 1\n",
            "developed: 1\n",
            "a: 1\n",
            "range: 1\n",
            "of: 1\n",
            "techniques: 1\n",
            "including: 1\n",
            "statistical: 1\n",
            "models: 1\n",
            "rule: 1\n",
            "based: 1\n",
            "systems: 1\n",
            "and: 1\n",
            "deep: 1\n",
            "learning: 1\n",
            "algorithms: 1\n",
            "\n",
            "Document 8 - TF:\n",
            "--------------------------------------------------\n",
            "one: 1\n",
            "popular: 1\n",
            "application: 1\n",
            "of: 2\n",
            "nlp: 1\n",
            "is: 1\n",
            "sentiment: 1\n",
            "analysis: 1\n",
            "which: 1\n",
            "involves: 1\n",
            "analyzing: 1\n",
            "text: 1\n",
            "to: 1\n",
            "determine: 1\n",
            "the: 2\n",
            "emotional: 1\n",
            "tone: 1\n",
            "writer: 1\n",
            "\n",
            "Document 9 - TF:\n",
            "--------------------------------------------------\n",
            "this: 1\n",
            "can: 1\n",
            "be: 1\n",
            "useful: 1\n",
            "for: 1\n",
            "businesses: 1\n",
            "who: 1\n",
            "want: 1\n",
            "to: 1\n",
            "understand: 1\n",
            "how: 1\n",
            "their: 2\n",
            "customers: 1\n",
            "feel: 1\n",
            "about: 1\n",
            "products: 1\n",
            "or: 1\n",
            "services: 1\n",
            "\n",
            "Document 10 - TF:\n",
            "--------------------------------------------------\n",
            "another: 1\n",
            "application: 1\n",
            "is: 1\n",
            "chatbots: 1\n",
            "which: 1\n",
            "use: 1\n",
            "nlp: 1\n",
            "to: 2\n",
            "understand: 1\n",
            "and: 1\n",
            "respond: 1\n",
            "user: 1\n",
            "queries: 1\n",
            "in: 1\n",
            "a: 1\n",
            "natural: 1\n",
            "way: 1\n",
            "\n",
            "Document 11 - TF:\n",
            "--------------------------------------------------\n",
            "despite: 1\n",
            "its: 1\n",
            "many: 1\n",
            "benefits: 1\n",
            "nlp: 1\n",
            "also: 1\n",
            "raises: 1\n",
            "ethical: 1\n",
            "concerns: 1\n",
            "around: 1\n",
            "issues: 1\n",
            "such: 1\n",
            "as: 1\n",
            "privacy: 1\n",
            "and: 1\n",
            "bias: 1\n",
            "\n",
            "Document 12 - TF:\n",
            "--------------------------------------------------\n",
            "for: 1\n",
            "example: 1\n",
            "if: 1\n",
            "nlp: 1\n",
            "is: 2\n",
            "used: 1\n",
            "to: 1\n",
            "analyze: 1\n",
            "social: 1\n",
            "media: 1\n",
            "data: 1\n",
            "there: 1\n",
            "a: 1\n",
            "risk: 1\n",
            "that: 1\n",
            "personal: 1\n",
            "information: 1\n",
            "could: 1\n",
            "be: 1\n",
            "exposed: 1\n",
            "\n",
            "Document 13 - TF:\n",
            "--------------------------------------------------\n",
            "additionally: 1\n",
            "if: 1\n",
            "the: 1\n",
            "algorithms: 1\n",
            "used: 1\n",
            "in: 1\n",
            "nlp: 1\n",
            "are: 1\n",
            "biased: 1\n",
            "they: 1\n",
            "could: 1\n",
            "perpetuate: 1\n",
            "discrimination: 1\n",
            "against: 1\n",
            "certain: 1\n",
            "groups: 1\n",
            "of: 1\n",
            "people: 1\n",
            "\n",
            "Document 14 - TF:\n",
            "--------------------------------------------------\n",
            "in: 1\n",
            "conclusion: 1\n",
            "nlp: 1\n",
            "is: 1\n",
            "a: 1\n",
            "rapidly: 1\n",
            "growing: 1\n",
            "field: 1\n",
            "with: 1\n",
            "many: 1\n",
            "exciting: 1\n",
            "applications: 1\n",
            "\n",
            "Document 15 - TF:\n",
            "--------------------------------------------------\n",
            "as: 1\n",
            "we: 1\n",
            "continue: 1\n",
            "to: 1\n",
            "generate: 1\n",
            "vast: 1\n",
            "amounts: 1\n",
            "of: 1\n",
            "unstructured: 1\n",
            "data: 2\n",
            "nlp: 1\n",
            "techniques: 1\n",
            "will: 1\n",
            "become: 1\n",
            "increasingly: 1\n",
            "important: 1\n",
            "for: 1\n",
            "extracting: 1\n",
            "valuable: 1\n",
            "insights: 1\n",
            "from: 1\n",
            "this: 1\n",
            "\n",
            "Document 16 - TF:\n",
            "--------------------------------------------------\n",
            "however: 1\n",
            "it: 1\n",
            "is: 1\n",
            "important: 1\n",
            "to: 2\n",
            "be: 1\n",
            "aware: 1\n",
            "of: 2\n",
            "the: 1\n",
            "ethical: 1\n",
            "implications: 1\n",
            "using: 1\n",
            "nlp: 1\n",
            "and: 2\n",
            "ensure: 1\n",
            "that: 1\n",
            "these: 1\n",
            "technologies: 1\n",
            "are: 1\n",
            "developed: 1\n",
            "in: 1\n",
            "a: 1\n",
            "responsible: 1\n",
            "equitable: 1\n",
            "way: 1\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#idf [Inverse Document Frequency ]\n",
        "\n",
        "# Create a dictionary to store document frequencies\n",
        "doc_frequency = defaultdict(int)\n",
        "\n",
        "# Calculate document frequency\n",
        "for document in preprocessed_corpus:\n",
        "    unique_words = set(document)\n",
        "    for word in unique_words:\n",
        "        doc_frequency[word] += 1\n",
        "\n",
        "# Total number of documents\n",
        "total_documents = len(preprocessed_corpus)\n",
        "\n",
        "# Calculate inverse document frequency (IDF)\n",
        "inverse_doc_frequency = {word: math.log(total_documents / df) for word, df in doc_frequency.items()}"
      ],
      "metadata": {
        "id": "HyVoXP6AtTRX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print inverse document frequencies\n",
        "print(\"\\nInverse Document Frequencies:\")\n",
        "for word, idf in inverse_doc_frequency.items():\n",
        "    print(f\"{word}: {idf}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IratM3zVuAOh",
        "outputId": "c49af4b8-82e3-4ccb-964c-e2ae2831c6b9"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inverse Document Frequencies:\n",
            "humans: 2.772588722239781\n",
            "is: 0.6931471805599453\n",
            "introduction: 2.772588722239781\n",
            "interaction: 2.772588722239781\n",
            "language: 1.6739764335716716\n",
            "nlp: 0.20763936477824455\n",
            "and: 0.5753641449035618\n",
            "to: 0.47000362924573563\n",
            "study: 2.772588722239781\n",
            "focuses: 2.772588722239781\n",
            "using: 1.6739764335716716\n",
            "processing: 2.772588722239781\n",
            "field: 2.0794415416798357\n",
            "the: 0.8266785731844679\n",
            "natural: 2.0794415416798357\n",
            "an: 2.772588722239781\n",
            "of: 0.47000362924573563\n",
            "that: 1.1631508098056809\n",
            "computers: 2.772588722239781\n",
            "a: 0.6931471805599453\n",
            "between: 2.772588722239781\n",
            "on: 2.0794415416798357\n",
            "developing: 2.772588722239781\n",
            "can: 1.3862943611198906\n",
            "human: 2.0794415416798357\n",
            "algorithms: 1.6739764335716716\n",
            "analyze: 2.0794415416798357\n",
            "models: 2.0794415416798357\n",
            "understand: 1.6739764335716716\n",
            "involves: 2.0794415416798357\n",
            "computational: 2.772588722239781\n",
            "generate: 2.0794415416798357\n",
            "it: 1.6739764335716716\n",
            "other: 2.772588722239781\n",
            "increasingly: 2.0794415416798357\n",
            "social: 2.0794415416798357\n",
            "day: 2.772588722239781\n",
            "generated: 2.772588722239781\n",
            "due: 2.772588722239781\n",
            "every: 2.772588722239781\n",
            "communication: 2.772588722239781\n",
            "has: 2.772588722239781\n",
            "vast: 2.0794415416798357\n",
            "amounts: 2.0794415416798357\n",
            "forms: 2.772588722239781\n",
            "years: 2.772588722239781\n",
            "become: 2.0794415416798357\n",
            "important: 1.6739764335716716\n",
            "unstructured: 2.0794415416798357\n",
            "media: 2.0794415416798357\n",
            "data: 1.3862943611198906\n",
            "in: 0.8266785731844679\n",
            "through: 2.772588722239781\n",
            "are: 1.3862943611198906\n",
            "emails: 2.772588722239781\n",
            "recent: 2.772588722239781\n",
            "this: 1.6739764335716716\n",
            "extract: 2.772588722239781\n",
            "wide: 2.772588722239781\n",
            "analysis: 2.0794415416798357\n",
            "sentiment: 2.0794415416798357\n",
            "for: 1.3862943611198906\n",
            "possible: 2.772588722239781\n",
            "which: 1.3862943611198906\n",
            "range: 2.0794415416798357\n",
            "chatbots: 2.0794415416798357\n",
            "machine: 2.772588722239781\n",
            "insights: 2.0794415416798357\n",
            "techniques: 1.6739764335716716\n",
            "by: 2.772588722239781\n",
            "valuable: 2.0794415416798357\n",
            "be: 1.1631508098056809\n",
            "applications: 2.0794415416798357\n",
            "from: 2.0794415416798357\n",
            "including: 2.0794415416798357\n",
            "translation: 2.772588722239781\n",
            "used: 1.3862943611198906\n",
            "one: 2.0794415416798357\n",
            "key: 2.772588722239781\n",
            "challenges: 2.0794415416798357\n",
            "ambiguity: 2.772588722239781\n",
            "meanings: 2.772588722239781\n",
            "broken: 2.772588722239781\n",
            "have: 2.0794415416798357\n",
            "meaning: 2.772588722239781\n",
            "they: 2.0794415416798357\n",
            "rules: 2.772588722239781\n",
            "multiple: 2.772588722239781\n",
            "affecting: 2.772588722239781\n",
            "words: 2.772588722239781\n",
            "grammar: 2.772588722239781\n",
            "without: 2.772588722239781\n",
            "sentence: 2.772588722239781\n",
            "depending: 2.772588722239781\n",
            "context: 2.772588722239781\n",
            "deep: 2.772588722239781\n",
            "developed: 2.0794415416798357\n",
            "these: 2.0794415416798357\n",
            "systems: 2.772588722239781\n",
            "statistical: 2.772588722239781\n",
            "based: 2.772588722239781\n",
            "address: 2.772588722239781\n",
            "researchers: 2.772588722239781\n",
            "learning: 2.772588722239781\n",
            "rule: 2.772588722239781\n",
            "text: 2.772588722239781\n",
            "determine: 2.772588722239781\n",
            "emotional: 2.772588722239781\n",
            "tone: 2.772588722239781\n",
            "popular: 2.772588722239781\n",
            "application: 2.0794415416798357\n",
            "writer: 2.772588722239781\n",
            "analyzing: 2.772588722239781\n",
            "feel: 2.772588722239781\n",
            "products: 2.772588722239781\n",
            "who: 2.772588722239781\n",
            "want: 2.772588722239781\n",
            "their: 2.772588722239781\n",
            "how: 2.772588722239781\n",
            "businesses: 2.772588722239781\n",
            "useful: 2.772588722239781\n",
            "customers: 2.772588722239781\n",
            "about: 2.772588722239781\n",
            "or: 2.772588722239781\n",
            "services: 2.772588722239781\n",
            "use: 2.772588722239781\n",
            "user: 2.772588722239781\n",
            "way: 2.0794415416798357\n",
            "another: 2.772588722239781\n",
            "respond: 2.772588722239781\n",
            "queries: 2.772588722239781\n",
            "raises: 2.772588722239781\n",
            "its: 2.772588722239781\n",
            "despite: 2.772588722239781\n",
            "many: 2.0794415416798357\n",
            "privacy: 2.772588722239781\n",
            "also: 2.772588722239781\n",
            "ethical: 2.0794415416798357\n",
            "concerns: 2.772588722239781\n",
            "issues: 2.772588722239781\n",
            "such: 2.772588722239781\n",
            "around: 2.772588722239781\n",
            "bias: 2.772588722239781\n",
            "as: 2.0794415416798357\n",
            "benefits: 2.772588722239781\n",
            "personal: 2.772588722239781\n",
            "information: 2.772588722239781\n",
            "example: 2.772588722239781\n",
            "there: 2.772588722239781\n",
            "exposed: 2.772588722239781\n",
            "risk: 2.772588722239781\n",
            "could: 2.0794415416798357\n",
            "if: 2.0794415416798357\n",
            "groups: 2.772588722239781\n",
            "biased: 2.772588722239781\n",
            "perpetuate: 2.772588722239781\n",
            "additionally: 2.772588722239781\n",
            "against: 2.772588722239781\n",
            "certain: 2.772588722239781\n",
            "people: 2.772588722239781\n",
            "discrimination: 2.772588722239781\n",
            "rapidly: 2.772588722239781\n",
            "exciting: 2.772588722239781\n",
            "with: 2.772588722239781\n",
            "growing: 2.772588722239781\n",
            "conclusion: 2.772588722239781\n",
            "extracting: 2.772588722239781\n",
            "will: 2.772588722239781\n",
            "continue: 2.772588722239781\n",
            "we: 2.772588722239781\n",
            "responsible: 2.772588722239781\n",
            "ensure: 2.772588722239781\n",
            "implications: 2.772588722239781\n",
            "aware: 2.772588722239781\n",
            "however: 2.772588722239781\n",
            "equitable: 2.772588722239781\n",
            "technologies: 2.772588722239781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print df [Document Frequencies]\n",
        "print(\"Document Frequencies:\")\n",
        "for word, df in doc_frequency.items():\n",
        "    print(f\"{word}: {df}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hAamYeruImz",
        "outputId": "79f432fe-5e8f-4e9b-d621-7e2c875e1b1f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document Frequencies:\n",
            "humans: 1\n",
            "is: 8\n",
            "introduction: 1\n",
            "interaction: 1\n",
            "language: 3\n",
            "nlp: 13\n",
            "and: 9\n",
            "to: 10\n",
            "study: 1\n",
            "focuses: 1\n",
            "using: 3\n",
            "processing: 1\n",
            "field: 2\n",
            "the: 7\n",
            "natural: 2\n",
            "an: 1\n",
            "of: 10\n",
            "that: 5\n",
            "computers: 1\n",
            "a: 8\n",
            "between: 1\n",
            "on: 2\n",
            "developing: 1\n",
            "can: 4\n",
            "human: 2\n",
            "algorithms: 3\n",
            "analyze: 2\n",
            "models: 2\n",
            "understand: 3\n",
            "involves: 2\n",
            "computational: 1\n",
            "generate: 2\n",
            "it: 3\n",
            "other: 1\n",
            "increasingly: 2\n",
            "social: 2\n",
            "day: 1\n",
            "generated: 1\n",
            "due: 1\n",
            "every: 1\n",
            "communication: 1\n",
            "has: 1\n",
            "vast: 2\n",
            "amounts: 2\n",
            "forms: 1\n",
            "years: 1\n",
            "become: 2\n",
            "important: 3\n",
            "unstructured: 2\n",
            "media: 2\n",
            "data: 4\n",
            "in: 7\n",
            "through: 1\n",
            "are: 4\n",
            "emails: 1\n",
            "recent: 1\n",
            "this: 3\n",
            "extract: 1\n",
            "wide: 1\n",
            "analysis: 2\n",
            "sentiment: 2\n",
            "for: 4\n",
            "possible: 1\n",
            "which: 4\n",
            "range: 2\n",
            "chatbots: 2\n",
            "machine: 1\n",
            "insights: 2\n",
            "techniques: 3\n",
            "by: 1\n",
            "valuable: 2\n",
            "be: 5\n",
            "applications: 2\n",
            "from: 2\n",
            "including: 2\n",
            "translation: 1\n",
            "used: 4\n",
            "one: 2\n",
            "key: 1\n",
            "challenges: 2\n",
            "ambiguity: 1\n",
            "meanings: 1\n",
            "broken: 1\n",
            "have: 2\n",
            "meaning: 1\n",
            "they: 2\n",
            "rules: 1\n",
            "multiple: 1\n",
            "affecting: 1\n",
            "words: 1\n",
            "grammar: 1\n",
            "without: 1\n",
            "sentence: 1\n",
            "depending: 1\n",
            "context: 1\n",
            "deep: 1\n",
            "developed: 2\n",
            "these: 2\n",
            "systems: 1\n",
            "statistical: 1\n",
            "based: 1\n",
            "address: 1\n",
            "researchers: 1\n",
            "learning: 1\n",
            "rule: 1\n",
            "text: 1\n",
            "determine: 1\n",
            "emotional: 1\n",
            "tone: 1\n",
            "popular: 1\n",
            "application: 2\n",
            "writer: 1\n",
            "analyzing: 1\n",
            "feel: 1\n",
            "products: 1\n",
            "who: 1\n",
            "want: 1\n",
            "their: 1\n",
            "how: 1\n",
            "businesses: 1\n",
            "useful: 1\n",
            "customers: 1\n",
            "about: 1\n",
            "or: 1\n",
            "services: 1\n",
            "use: 1\n",
            "user: 1\n",
            "way: 2\n",
            "another: 1\n",
            "respond: 1\n",
            "queries: 1\n",
            "raises: 1\n",
            "its: 1\n",
            "despite: 1\n",
            "many: 2\n",
            "privacy: 1\n",
            "also: 1\n",
            "ethical: 2\n",
            "concerns: 1\n",
            "issues: 1\n",
            "such: 1\n",
            "around: 1\n",
            "bias: 1\n",
            "as: 2\n",
            "benefits: 1\n",
            "personal: 1\n",
            "information: 1\n",
            "example: 1\n",
            "there: 1\n",
            "exposed: 1\n",
            "risk: 1\n",
            "could: 2\n",
            "if: 2\n",
            "groups: 1\n",
            "biased: 1\n",
            "perpetuate: 1\n",
            "additionally: 1\n",
            "against: 1\n",
            "certain: 1\n",
            "people: 1\n",
            "discrimination: 1\n",
            "rapidly: 1\n",
            "exciting: 1\n",
            "with: 1\n",
            "growing: 1\n",
            "conclusion: 1\n",
            "extracting: 1\n",
            "will: 1\n",
            "continue: 1\n",
            "we: 1\n",
            "responsible: 1\n",
            "ensure: 1\n",
            "implications: 1\n",
            "aware: 1\n",
            "however: 1\n",
            "equitable: 1\n",
            "technologies: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TF-IDF (Term Frequency - Inverse Document Frequency)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TfidfVectorizer instance\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Transform the sentences into a TF-IDF matrix\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Get the feature (word) names\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Convert the TF-IDF matrix to an array for printing\n",
        "tfidf_array = tfidf_matrix.toarray()\n",
        "\n",
        "# Print the feature names and TF-IDF matrix\n",
        "print(\"Feature Names:\", feature_names)\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_array)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf3SvII4ubXP",
        "outputId": "2df0c80d-8173-46dd-ea3a-e19c46c613ad"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['about' 'additionally' 'address' 'affecting' 'against' 'algorithms'\n",
            " 'also' 'ambiguity' 'amounts' 'an' 'analysis' 'analyze' 'analyzing' 'and'\n",
            " 'another' 'application' 'applications' 'are' 'around' 'as' 'aware'\n",
            " 'based' 'be' 'become' 'benefits' 'between' 'bias' 'biased' 'broken'\n",
            " 'businesses' 'by' 'can' 'certain' 'challenges' 'chatbots' 'communication'\n",
            " 'computational' 'computers' 'concerns' 'conclusion' 'context' 'continue'\n",
            " 'could' 'customers' 'data' 'day' 'deep' 'depending' 'despite' 'determine'\n",
            " 'developed' 'developing' 'discrimination' 'due' 'emails' 'emotional'\n",
            " 'ensure' 'equitable' 'ethical' 'every' 'example' 'exciting' 'exposed'\n",
            " 'extract' 'extracting' 'feel' 'field' 'focuses' 'for' 'forms' 'from'\n",
            " 'generate' 'generated' 'grammar' 'groups' 'growing' 'has' 'have' 'how'\n",
            " 'however' 'human' 'humans' 'if' 'implications' 'important' 'in'\n",
            " 'including' 'increasingly' 'information' 'insights' 'interaction'\n",
            " 'introduction' 'involves' 'is' 'issues' 'it' 'its' 'key' 'language'\n",
            " 'learning' 'machine' 'many' 'meaning' 'meanings' 'media' 'models'\n",
            " 'multiple' 'natural' 'nlp' 'of' 'on' 'one' 'or' 'other' 'people'\n",
            " 'perpetuate' 'personal' 'popular' 'possible' 'privacy' 'processing'\n",
            " 'products' 'queries' 'raises' 'range' 'rapidly' 'recent' 'researchers'\n",
            " 'respond' 'responsible' 'risk' 'rule' 'rules' 'sentence' 'sentiment'\n",
            " 'services' 'social' 'statistical' 'study' 'such' 'systems' 'techniques'\n",
            " 'technologies' 'text' 'that' 'the' 'their' 'there' 'these' 'they' 'this'\n",
            " 'through' 'to' 'tone' 'translation' 'understand' 'unstructured' 'use'\n",
            " 'used' 'useful' 'user' 'using' 'valuable' 'vast' 'want' 'way' 'we'\n",
            " 'which' 'who' 'wide' 'will' 'with' 'without' 'words' 'writer' 'years']\n",
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.21620553]\n",
            " ...\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]\n",
            " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
          ]
        }
      ]
    }
  ]
}