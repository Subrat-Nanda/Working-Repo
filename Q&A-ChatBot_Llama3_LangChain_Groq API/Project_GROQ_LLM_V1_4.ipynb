{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-groq\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-community\n",
        "!pip install faiss-gpu\n",
        "!pip install pypdf\n",
        "!pip install rouge\n",
        "!pip install python-dotenv\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJLxeWoHUOS8",
        "outputId": "fefb28fb-c8a4-431c-eb2b-26ff71a1a858"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-groq\n",
            "  Downloading langchain_groq-0.1.9-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
            "  Downloading groq-0.9.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting langchain-core<0.3.0,>=0.2.26 (from langchain-groq)\n",
            "  Downloading langchain_core-0.2.28-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from groq<1,>=0.4.1->langchain-groq)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.26->langchain-groq)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain-core<0.3.0,>=0.2.26->langchain-groq)\n",
            "  Downloading langsmith-0.1.96-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-groq) (24.1)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain-core<0.3.0,>=0.2.26->langchain-groq)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain-groq)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-groq) (2.0.7)\n",
            "Downloading langchain_groq-0.1.9-py3-none-any.whl (14 kB)\n",
            "Downloading groq-0.9.0-py3-none-any.whl (103 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.5/103.5 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.2.28-py3-none-any.whl (379 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langsmith-0.1.96-py3-none-any.whl (140 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, h11, jsonpatch, httpcore, langsmith, httpx, langchain-core, groq, langchain-groq\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed groq-0.9.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-core-0.2.28 langchain-groq-0.1.9 langsmith-0.1.96 orjson-3.10.6 tenacity-8.5.0\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.20-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.26 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.2.28)\n",
            "Collecting openai<2.0.0,>=1.32.0 (from langchain-openai)\n",
            "  Downloading openai-1.38.0-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-openai) (0.1.96)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-openai) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.26->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.66.4)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.26->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.26->langchain-openai) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.26->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.26->langchain-openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.7)\n",
            "Downloading langchain_openai-0.1.20-py3-none-any.whl (48 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.38.0-py3-none-any.whl (335 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m335.9/335.9 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, openai, langchain-openai\n",
            "Successfully installed langchain-openai-0.1.20 openai-1.38.0 tiktoken-0.7.0\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.11-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.12 (from langchain-community)\n",
            "  Downloading langchain-0.2.12-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.27 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.28)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.96)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.12->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.12->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.27->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.27->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.12->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading langchain_community-0.2.11-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain-0.2.12-py3-none-any.whl (990 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, dataclasses-json, langchain-text-splitters, langchain, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 langchain-0.2.12 langchain-community-0.2.11 langchain-text-splitters-0.2.2 marshmallow-3.21.3 mypy-extensions-1.0.0 typing-inspect-0.9.0\n",
            "Collecting faiss-gpu\n",
            "  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.7.2\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.3.1\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv()\n",
        "import time"
      ],
      "metadata": {
        "id": "jnA6rAInsWnU"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from rouge import Rouge\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaTPdF4GwKHk",
        "outputId": "dbc4779f-6a01-46a4-c33b-a12bc08a1864"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_groq_and_openai_keys(path):\n",
        "    \"\"\"Loads GROQ and OpenAI API keys from environment variables.\"\"\"\n",
        "    # os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
        "    # groq_api_key = os.getenv('GROQ_API_KEY')\n",
        "\n",
        "    f = open('/content/sample_data/environment_variable/env_var.json')\n",
        "\n",
        "    data = json.load(f)\n",
        "\n",
        "    groq_api_key = data['groq_api_key']\n",
        "    OPENAI_API_KEY = data['OPENAI_API_KEY']\n",
        "\n",
        "    os.environ['GROQ_API_KEY'] = groq_api_key\n",
        "\n",
        "    os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "    return groq_api_key,OPENAI_API_KEY\n"
      ],
      "metadata": {
        "id": "CNzMDio2h0pJ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vectors(data_dir=\"/content/sample_data/us_census\", max_documents=20):\n",
        "    \"\"\"Loads and processes documents for retrieval.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Directory containing documents (default: \"./us_census\").\n",
        "        max_documents: Maximum number of documents to load (default: 20).\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "            embeddings: An OpenAIEmbeddings instance.\n",
        "            loader: A PyPDFDirectoryLoader instance.\n",
        "            final_documents: A list of processed documents.\n",
        "            vectors: A FAISS vector store containing document embeddings.\n",
        "    \"\"\"\n",
        "    embeddings = OpenAIEmbeddings()\n",
        "    loader = PyPDFDirectoryLoader(data_dir)\n",
        "    docs = loader.load()[:max_documents]  # Load only max_documents\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    final_documents = text_splitter.split_documents(docs)\n",
        "    vectors = FAISS.from_documents(final_documents, embeddings)\n",
        "    return embeddings, loader, final_documents, vectors\n"
      ],
      "metadata": {
        "id": "cmvotPYBsdk9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_question(question, groq_api_key, vectors ):\n",
        "    \"\"\"Answers a question using the provided GROQ API key and vector store.\n",
        "\n",
        "    Args:\n",
        "        question: The question to answer (str).\n",
        "        groq_api_key: The GROQ API key (str).\n",
        "        vectors: A FAISS vector store containing document embeddings.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the answer and context.\n",
        "    \"\"\"\n",
        "    llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"Llama3-8b-8192\")\n",
        "    prompt = ChatPromptTemplate.from_template(\n",
        "        \"\"\"\n",
        "        Answer the questions based on the provided context only.\n",
        "        Please provide the most accurate response based on the question\n",
        "        <context>\n",
        "        {context}\n",
        "        <context>\n",
        "        Questions:{input}\n",
        "        \"\"\"\n",
        "    )\n",
        "    document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "    retriever = vectors.as_retriever()\n",
        "    retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
        "\n",
        "\n",
        "    start = time.process_time()\n",
        "    response = retrieval_chain.invoke({'input': question})\n",
        "    print(f\"Response time: {time.process_time() - start}\")\n",
        "\n",
        "\n",
        "    return response"
      ],
      "metadata": {
        "id": "5E2LvhBish4X"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "def evaluate_response(question,generated_text,reference_text):\n",
        "  \"\"\"Evaluates the generated text against the reference text using BLEU, ROUGE, METEOR, Precision, Recall, and F1-score.\"\"\"\n",
        "\n",
        "  # Tokenize the texts\n",
        "  generated_tokens = nltk.word_tokenize(generated_text)\n",
        "  reference_tokens = nltk.word_tokenize(reference_text)\n",
        "\n",
        "  # Calculate BLEU score\n",
        "  bleu_score = sentence_bleu([reference_tokens], generated_tokens)\n",
        "\n",
        "  # Calculate ROUGE scores\n",
        "  rouge = Rouge()\n",
        "  scores = rouge.get_scores(generated_text, reference_text)\n",
        "  rouge_1 = scores[0]['rouge-1']['f']\n",
        "  rouge_2 = scores[0]['rouge-2']['f']\n",
        "  rouge_l = scores[0]['rouge-l']['f']\n",
        "\n",
        "  #bleu, rouge_1, rouge_2, rouge_l = evaluate_response(response['answer'], reference_text)\n",
        "  #print(f\"BLEU: {bleu}, ROUGE-1: {rouge_1}, ROUGE-2: {rouge_2}, ROUGE-L: {rouge_l}\")\n",
        "\n",
        "  llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"Llama3-8b-8192\")\n",
        "  evaluator = load_evaluator(\"labeled_score_string\", llm=llm)\n",
        "  eval_result = evaluator.evaluate_strings(\n",
        "      prediction = generated_text,\n",
        "      reference = reference_text,\n",
        "      input=question,\n",
        "  )\n",
        "\n",
        "  print(f\"Question: {question}\")\n",
        "  print(f\"Model Prediction: {generated_text}\")\n",
        "  print(f\"Reference Text: {reference_text}\")\n",
        "  #print(f\"BLEU: {bleu}, ROUGE-1: {rouge_1}, ROUGE-2: {rouge_2}, ROUGE-L: {rouge_l}, Scoring_Evaluator: {eval_result['score']}\")\n",
        "  print(f\"BLEU: {bleu}\")\n",
        "  print(f\"ROUGE-1: {rouge_1}\")\n",
        "  print(f\"ROUGE-2: {rouge_2}\")\n",
        "  print(f\"ROUGE-L: {rouge_l}\")\n",
        "  print(f\"Scoring_Evaluator: {eval_result['score']}\")\n",
        "\n",
        "  #return  bleu_score, rouge_1, rouge_2, rouge_l, eval_result"
      ],
      "metadata": {
        "id": "5LEc8B6FvQTk"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.evaluation import load_evaluator\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "\n",
        "def find_score(question, reference_texts, response) :\n",
        "  llm = ChatGroq(groq_api_key=groq_api_key, model_name=\"Llama3-8b-8192\")\n",
        "  evaluator = load_evaluator(\"labeled_score_string\", llm=llm)\n",
        "  eval_result = evaluator.evaluate_strings(\n",
        "      prediction=response,\n",
        "      reference=reference_texts,\n",
        "      input=question,\n",
        "  )\n",
        "  #print(eval_result)\n",
        "  return eval_result"
      ],
      "metadata": {
        "id": "cHppZ_cmky-D"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test - 01\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    variable_path = \"/content/sample_data/environment_variable/env_var.json\"\n",
        "\n",
        "    groq_api_key,OPENAI_API_KEY = load_groq_and_openai_keys(variable_path)\n",
        "    embeddings, _, _, vectors = load_vectors()\n",
        "\n",
        "    ground_truths = \"2018\"\n",
        "    reference_text = \"2018 Survey of Income and Program Participation (SIPP)\"\n",
        "\n",
        "    question = input(\"Enter Your Question: \")\n",
        "    response = answer_question(question, groq_api_key, vectors)\n",
        "    print(f\"Answer: {response['answer']}\")\n",
        "\n",
        "\n",
        "\n",
        "    '''\n",
        "    # Print context snippets if desired\n",
        "    print(\"\\nAnalyze and we get in Document:\")\n",
        "    for i, doc in enumerate(response[\"context\"]):\n",
        "        print(doc.page_content)\n",
        "        print(\"--------------------------------\")\n",
        "    '''\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzTXmScesO6G",
        "outputId": "950480b6-9872-43af-abb7-8a63b9652f8b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Your Question: What is the purpose of the 2018 Survey of Income and Program Participation (SIPP) survey?\n",
            "Response time: 0.16179655599999876\n",
            "Answer: The purpose of the 2018 Survey of Income and Program Participation (SIPP) is to provide comprehensive information on the dynamics of income, employment, household composition, and government program participation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding Model Performance:\n",
        "evaluate_response(question,response['answer'],reference_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCU1VfwSvUel",
        "outputId": "f81808fa-e182-49bc-a02d-56f307dc757d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the purpose of the 2018 Survey of Income and Program Participation (SIPP) survey?\n",
            "Model Prediction: The purpose of the 2018 Survey of Income and Program Participation (SIPP) is to provide comprehensive information on the dynamics of income, employment, household composition, and government program participation.\n",
            "Reference Text: 2018 Survey of Income and Program Participation (SIPP)\n",
            "BLEU: 0.2516546237169354\n",
            "ROUGE-1: 0.4848484811753903\n",
            "ROUGE-2: 0.39999999680000003\n",
            "ROUGE-L: 0.4848484811753903\n",
            "Scoring_Evaluator: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test - 02\n",
        "\n",
        "question = \"What is the joint relationship between occupation and objective characteristics?\"\n",
        "reference_text = \"The relationship between occupation and objective characteristics likely depends on their overall relationship, which this report does not analyze\"\n",
        "\n",
        "response = answer_question(question, groq_api_key, vectors)\n",
        "print(f\"Answer: {response['answer']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEdLTjgsaTSw",
        "outputId": "fe8a272b-70cb-40db-97c7-c166f9a9e113"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response time: 0.14072748899999965\n",
            "Answer: The joint relationship between occupation and objective characteristics is not analyzed in this report. According to the context, the report \"does not attempt such an analysis\" and instead focuses on evaluating the relationship between occupation and each objective characteristic separately.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding Model Performance:\n",
        "evaluate_response(question,response['answer'],reference_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWh-h-KvbZzv",
        "outputId": "5df737ca-9ef6-4cf3-f11a-2771c05d906d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What is the joint relationship between occupation and objective characteristics?\n",
            "Model Prediction: The joint relationship between occupation and objective characteristics is not analyzed in this report. According to the context, the report \"does not attempt such an analysis\" and instead focuses on evaluating the relationship between occupation and each objective characteristic separately.\n",
            "Reference Text: The relationship between occupation and objective characteristics likely depends on their overall relationship, which this report does not analyze\n",
            "BLEU: 0.2516546237169354\n",
            "ROUGE-1: 0.44897958708871305\n",
            "ROUGE-2: 0.22222221777777784\n",
            "ROUGE-L: 0.3673469340274886\n",
            "Scoring_Evaluator: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test-03\n",
        "question = \"What are the main features of employment that are highlighted in this report?\"\n",
        "reference_text = \"The report highlights several key features of employment, including occupations, work schedules, earnings, and other job characteristics.\"\n",
        "\n",
        "response = answer_question(question, groq_api_key, vectors)\n",
        "print(f\"Answer: {response['answer']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zfaHgBdoBHr",
        "outputId": "55bae447-04c0-4ff5-979a-32f2fc1f23be"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Response time: 0.12637742200000446\n",
            "Answer: According to the context, the main features of employment that are highlighted in this report are:\n",
            "\n",
            "1. Occupations\n",
            "2. Work schedules\n",
            "3. Earnings\n",
            "4. Other job characteristics\n",
            "\n",
            "These features are highlighted using data from two surveys administered by the U.S. Census Bureau: the 2018 Survey of Income and Program Participation (SIPP) and the 2018 American Community Survey (ACS) 1-year estimates.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Finding Model Performance:\n",
        "evaluate_response(question,response['answer'],reference_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6T36Mbyznbl",
        "outputId": "5a5b7e68-b041-422b-99c9-4538dce9ac1a"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What are the main features of employment that are highlighted in this report?\n",
            "Model Prediction: According to the context, the main features of employment that are highlighted in this report are:\n",
            "\n",
            "1. Occupations\n",
            "2. Work schedules\n",
            "3. Earnings\n",
            "4. Other job characteristics\n",
            "\n",
            "These features are highlighted using data from two surveys administered by the U.S. Census Bureau: the 2018 Survey of Income and Program Participation (SIPP) and the 2018 American Community Survey (ACS) 1-year estimates.\n",
            "Reference Text: The report highlights several key features of employment, including occupations, work schedules, earnings, and other job characteristics.\n",
            "BLEU: 0.2516546237169354\n",
            "ROUGE-1: 0.17910447382490538\n",
            "ROUGE-2: 0.05405405066471898\n",
            "ROUGE-L: 0.14925372755624872\n",
            "Scoring_Evaluator: 9\n"
          ]
        }
      ]
    }
  ]
}